{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìì The GenAI Revolution Cookbook\n\n**Title:** Structured Data Extraction: How to Build a LangChain Pipeline\n\n**Description:** Build an end-to-end LangChain + OpenAI pipeline for structured data extraction from long documents‚Äîschema validation, batching, retries, and production-ready code.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thought: I now can give a great answer\n\n---\n\nStructured data extraction from invoices is a common task for AI builders, but it's easy to hit roadblocks: JSON formatting failures, token limits on long documents, unreliable field extraction, and expensive batch runs. This tutorial walks you through building a production-ready invoice extraction pipeline that handles these challenges head-on using LangChain, OpenAI's structured output, and Pydantic validation. You'll learn how to enforce schema compliance, chunk long documents with aggregation, add retries and async batching, implement caching to cut costs, and monitor token usage‚Äîall in a runnable notebook.\n\nBy the end, you'll have a complete system that extracts invoice fields reliably at scale, with clear provenance tracking and cost estimation.\n\n---\n\n## Why This Approach Works\n\nInvoice extraction requires more than a single LLM call. Real-world invoices vary in format, length, and quality. A robust pipeline must:\n\n- **Enforce schema compliance** ‚Äì Pydantic models validate output structure and types, catching malformed JSON early.\n- **Handle long documents** ‚Äì Chunking with overlap ensures no field is lost at chunk boundaries; aggregation merges partial results.\n- **Retry transient failures** ‚Äì Rate limits and timeouts are common; exponential backoff with tenacity keeps the pipeline resilient.\n- **Scale with async batching** ‚Äì Concurrent requests with semaphore-based rate limiting maximize throughput without overwhelming the API.\n- **Cache results** ‚Äì Content-based caching skips redundant extractions, saving cost and time.\n- **Track cost and usage** ‚Äì Token counting and cost estimation help you optimize model choice and prompt design.\n\nThis design balances accuracy, speed, and cost‚Äîcritical for production systems processing hundreds or thousands of invoices.\n\n---\n\n## Why LangChain with Structured Output?\n\nLangChain's `with_structured_output` wraps OpenAI's JSON schema enforcement with Pydantic validation, giving you:\n\n- **Type safety** ‚Äì Pydantic models catch type mismatches and missing fields at runtime.\n- **Schema evolution** ‚Äì Easily extend or refactor your schema without rewriting prompt logic.\n- **Ergonomics** ‚Äì Cleaner code than manually parsing JSON and validating fields.\n\n**Trade-off:** LangChain adds a dependency layer. If you need minimal dependencies or maximum control, OpenAI's SDK with `response_format={\"type\": \"json_schema\"}` is a lighter alternative. For this tutorial, LangChain's validation and retry integrations justify the overhead.\n\n**Note on scope:** This pipeline assumes text input. Most invoices are PDFs or scans, requiring OCR or PDF-to-text preprocessing (e.g., `pdfminer`, `pypdf`, `unstructured`, AWS Textract). That step is out of scope here but required in production.\n\n---\n\n## How It Works (High-Level Overview)\n\nThe pipeline follows this dataflow:\n\n1. **Input text** ‚Üí Split into overlapping chunks (if long)\n2. **Per-chunk extraction** ‚Üí Extract fields with strict schema enforcement and retries\n3. **Aggregation** ‚Üí Merge partial results, deduplicate line items, track provenance\n4. **Post-processing** ‚Üí Normalize dates/currency, validate totals\n5. **Batch processing** ‚Üí Async extraction with concurrency control\n6. **Caching** ‚Üí Store results keyed by content hash to skip redundant calls\n7. **Metrics** ‚Üí Count tokens and estimate cost per run\n\nEach component is modular, so you can use the core extraction logic standalone or scale to batches with caching and metrics.\n\n---\n\n## Setup & Installation\n\nRun this cell first to install dependencies (Colab-ready):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain-openai langchain-text-splitters pydantic==2.* tenacity tiktoken python-dotenv openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your OpenAI API key. For local development, use a `.env` file. For Colab, set it directly or use Colab secrets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n\n# Option 1: Set directly (not recommended for shared notebooks)\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n# Option 2: Use Colab userdata (recommended for Colab)\ntry:\n    from google.colab import userdata\n    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\nexcept ImportError:\n    # Option 3: Load from .env for local development\n    from dotenv import load_dotenv\n    load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Define the Schema\n\nWe use Pydantic to define the invoice structure. Each field has a description to guide the LLM, and optional fields allow partial extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Optional\nfrom pydantic import BaseModel, Field\n\nclass LineItem(BaseModel):\n    \"\"\"\n    Represents a single line item in an invoice.\n    \"\"\"\n    description: str = Field(..., description=\"Item description as written\")\n    quantity: Optional[float] = Field(None, description=\"Numeric quantity if available\")\n    unit_price: Optional[float] = Field(None, description=\"Unit price, numeric\")\n    amount: Optional[float] = Field(None, description=\"Line total amount, numeric\")\n\nclass InvoiceEntities(BaseModel):\n    \"\"\"\n    Represents the extracted entities from an invoice.\n    \"\"\"\n    invoice_number: Optional[str] = Field(None, description=\"Invoice identifier\")\n    vendor_name: Optional[str] = Field(None, description=\"Company or person issuing the invoice\")\n    vendor_address: Optional[str] = Field(None, description=\"Address block if present\")\n    bill_to_name: Optional[str] = Field(None, description=\"Recipient name\")\n    bill_to_address: Optional[str] = Field(None, description=\"Recipient address\")\n    invoice_date: Optional[str] = Field(None, description=\"Date string as found\")\n    due_date: Optional[str] = Field(None, description=\"Due date string as found\")\n    currency: Optional[str] = Field(None, description=\"Currency code/symbol if inferable\")\n    subtotal: Optional[float] = Field(None, description=\"Subtotal before tax/fees\")\n    tax: Optional[float] = Field(None, description=\"Total tax amount\")\n    total: Optional[float] = Field(None, description=\"Grand total\")\n    line_items: List[LineItem] = Field(default_factory=list, description=\"List of individual line items\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Core Extraction with Structured Output\n\nWe configure the LLM to return structured output matching our schema. Temperature is set to 0 for deterministic results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4o-mini\",  # Use \"gpt-4o\" for higher accuracy if needed\n    temperature=0,        # Deterministic output for reproducibility\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\nstructured_llm = llm.with_structured_output(InvoiceEntities)\n\nSYSTEM_PROMPT = (\n    \"You are an information extraction engine. \"\n    \"Extract invoice fields precisely as a JSON object matching the schema. \"\n    \"Do not include any text that is not part of the JSON.\"\n)\n\ndef extract_invoice(text: str) -> InvoiceEntities:\n    \"\"\"\n    Extracts invoice fields from text using the structured LLM.\n    \"\"\"\n    return structured_llm.invoke([\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": text}\n    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test with a sample invoice:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_doc = \"\"\"\nINVOICE #INV-1028\nVendor: ACME Supplies Co.\nAddress: 123 Market Street, Springfield\nBill To: Northwind Traders\nDate: 2024-08-12\nDue: 2024-09-11\nItems:\n- Paper Reams (QTY 10) @ $4.50 each = $45.00\n- Pens (QTY 20) @ $1.25 each = $25.00\nSubtotal: $70.00\nTax: $6.30\nTotal Due: $76.30\n\"\"\"\n\nresult = extract_invoice(sample_doc)\nprint(result.model_dump())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Handling Long Documents with Chunking\n\nExceeding token limits? Chunk with overlap to preserve context between chunks. Use LangChain's `RecursiveCharacterTextSplitter`. For a deeper dive into how position bias can cause models to miss details in lengthy prompts, see our analysis on [placing critical info in long prompts](/article/lost-in-the-middle-placing-critical-info-in-long-prompts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n\ndef split_document(text: str, chunk_size: int = 2000, chunk_overlap: int = 200):\n    \"\"\"\n    Splits a document into overlapping chunks for LLM processing.\n    Tracks source indices for provenance.\n    \"\"\"\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n    )\n    chunks = splitter.split_text(text)\n    \n    # Track source indices for debugging and provenance\n    offsets = []\n    start = 0\n    for chunk in chunks:\n        idx = text.find(chunk, start)\n        if idx == -1:\n            # Guard against find failure; fall back to chunk index only\n            offsets.append((start, start + len(chunk)))\n        else:\n            offsets.append((idx, idx + len(chunk)))\n            start = idx + len(chunk)\n    \n    return [{\"id\": i, \"text\": c, \"start\": s, \"end\": e} for i, (c, (s, e)) in enumerate(zip(chunks, offsets))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Aggregating Partial Results\n\nExtract from each chunk and merge results. For scalar fields, take the first non-null value. For line items, deduplicate by description and amount."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\nfrom pydantic import ValidationError\n\ndef extract_invoice_chunked(text: str) -> dict:\n    \"\"\"\n    Extracts invoice data from each chunk and aggregates results.\n    Deduplicates line items and tracks provenance.\n    \"\"\"\n    chunks = split_document(text)\n    partials = []\n    \n    for ch in chunks:\n        try:\n            r = extract_invoice(ch[\"text\"]).model_dump()\n            r[\"_chunk\"] = {\"id\": ch[\"id\"], \"start\": ch[\"start\"], \"end\": ch[\"end\"]}\n            partials.append(r)\n        except ValidationError as e:\n            # Log error for this chunk; will add retries/logging later\n            partials.append({\"_chunk\": {\"id\": ch[\"id\"], \"start\": ch[\"start\"], \"end\": ch[\"end\"]}, \"_error\": str(e)})\n\n    merged = {\n        \"invoice_number\": None,\n        \"vendor_name\": None,\n        \"vendor_address\": None,\n        \"bill_to_name\": None,\n        \"bill_to_address\": None,\n        \"invoice_date\": None,\n        \"due_date\": None,\n        \"currency\": None,\n        \"subtotal\": None,\n        \"tax\": None,\n        \"total\": None,\n        \"line_items\": [],\n        \"_provenance\": [],  # Track which chunk contributed\n    }\n\n    # For scalar fields, prefer the first non-null value found\n    scalar_fields = [k for k in merged.keys() if k not in (\"line_items\", \"_provenance\")]\n    for p in partials:\n        if \"_error\" in p:\n            continue\n        merged[\"_provenance\"].append(p[\"_chunk\"])\n        for f in scalar_fields:\n            if merged[f] is None and p.get(f) not in (None, \"\", []):\n                merged[f] = p.get(f)\n\n    # Merge and deduplicate line_items by (description, amount)\n    seen = set()\n    for p in partials:\n        if \"_error\" in p:\n            continue\n        for li in p.get(\"line_items\", []):\n            key = (li.get(\"description\", \"\").strip().lower(), li.get(\"amount\"))\n            if key not in seen and li.get(\"description\"):\n                merged[\"line_items\"].append(li)\n                seen.add(key)\n\n    return merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Strict Schema Enforcement with Fallback\n\nAdd a fallback prompt if the first extraction fails validation. This catches edge cases where the model returns malformed JSON or incorrect types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "STRICT_SYSTEM_PROMPT = (\n    \"You are an extraction engine. Output MUST be valid JSON only, strictly matching the schema. \"\n    \"Do not include comments or extra keys. Numeric fields must be numbers, not strings.\"\n)\n\ndef extract_invoice_strict(text: str) -> InvoiceEntities:\n    \"\"\"\n    Extracts invoice fields with strict schema enforcement and fallback prompt.\n    \"\"\"\n    try:\n        # First attempt with standard prompt\n        return structured_llm.invoke([\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": text}\n        ])\n    except Exception:\n        # Second attempt with stricter prompt for schema compliance\n        return llm.with_structured_output(InvoiceEntities).invoke([\n            {\"role\": \"system\", \"content\": STRICT_SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": text}\n        ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Adding Retries with Exponential Backoff\n\nTransient errors (rate limits, timeouts) are common. Use `tenacity` to retry with exponential backoff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nfrom langchain_core.exceptions import OutputParserException\n\n# Retry on LangChain-surfaced exceptions or OpenAI SDK exceptions if propagated\n@retry(\n    reraise=True,\n    stop=stop_after_attempt(4),  # Maximum 4 attempts\n    wait=wait_exponential(multiplier=1, min=1, max=8),  # Exponential backoff\n    retry=retry_if_exception_type((OutputParserException, Exception))  # Adjust based on error surface\n)\ndef extract_invoice_with_retry(text: str) -> InvoiceEntities:\n    \"\"\"\n    Extracts invoice fields with retry logic for transient errors.\n    \"\"\"\n    return extract_invoice_strict(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** If using OpenAI SDK exceptions directly (e.g., `RateLimitError`, `APIError`), ensure `openai` is installed and exceptions propagate through LangChain. Adjust the retry decorator accordingly.\n\n---\n\n## Logging Failures for Debugging\n\nLog extraction failures with document ID, input hash, and error details. Save a sample of the failed input for manual review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\nimport hashlib\nfrom pathlib import Path\n\nlogging.basicConfig(level=logging.INFO)\nFAIL_DIR = Path(\"failures\")\nFAIL_DIR.mkdir(exist_ok=True)\n\ndef log_failure(doc_id: str, text: str, error: Exception):\n    \"\"\"\n    Logs extraction failure details and saves a sample of the failed input.\n    \"\"\"\n    h = hashlib.sha256(text.encode(\"utf-8\")).hexdigest()[:12]\n    logging.error(f\"[extract-fail] doc={doc_id} hash={h} error={type(error).__name__}: {error}\")\n    (FAIL_DIR / f\"{doc_id}_{h}.txt\").write_text(text[:4000], encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Async Batch Processing with Concurrency Control\n\nProcess multiple documents concurrently using asyncio and semaphores to limit concurrent requests. This integrates strict extraction, retries, and error logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\nimport asyncio\nfrom typing import Tuple\n\nclass Extractor:\n    \"\"\"\n    Async batch extractor for invoice documents.\n    Integrates strict schema enforcement, retries, and error logging.\n    \"\"\"\n    def __init__(self, model: str = \"gpt-4o-mini\", concurrency: int = 8):\n        self.llm = ChatOpenAI(model=model, temperature=0, api_key=os.getenv(\"OPENAI_API_KEY\"))\n        self.structured_llm = self.llm.with_structured_output(InvoiceEntities)\n        self.sem = asyncio.Semaphore(concurrency)  # Limit concurrent requests\n\n    async def extract_one(self, doc_id: str, text: str) -> Tuple[str, dict, dict]:\n        \"\"\"\n        Extracts a single document asynchronously with retries and error handling.\n        \"\"\"\n        async with self.sem:\n            t0 = time.perf_counter()\n            try:\n                # Use strict extraction with retries (adapt for async if needed)\n                res = await self.structured_llm.ainvoke([\n                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                    {\"role\": \"user\", \"content\": text}\n                ])\n                elapsed = time.perf_counter() - t0\n                meta = getattr(res, \"response_metadata\", {}) or {}\n                return doc_id, res.model_dump(), {\"ok\": True, \"elapsed\": elapsed, \"meta\": meta}\n            except Exception as e:\n                log_failure(doc_id, text, e)\n                elapsed = time.perf_counter() - t0\n                return doc_id, {}, {\"ok\": False, \"elapsed\": elapsed, \"error\": str(e)}\n\nasync def process_batch(docs: list, model=\"gpt-4o-mini\", concurrency=8):\n    \"\"\"\n    Processes a batch of documents asynchronously.\n    \"\"\"\n    ex = Extractor(model=model, concurrency=concurrency)\n    tasks = [ex.extract_one(doc_id, text) for doc_id, text in docs]\n    return await asyncio.gather(*tasks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Caching Results to Save Cost\n\nCache results on disk keyed by a content hash. This saves cost when re-running. If you're interested in advanced caching strategies‚Äîincluding semantic caching with embeddings to further reduce LLM spend‚Äîcheck out our walkthrough on [implementing semantic cache with Redis Vector](/article/semantic-cache-llm-how-to-implement-with-redis-vector-to-cut-costs-6)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\nimport hashlib\nfrom pathlib import Path\n\nCACHE_DIR = Path(\"cache\")\nCACHE_DIR.mkdir(exist_ok=True)\n\ndef cache_key(text: str, model: str) -> str:\n    \"\"\"\n    Generates a cache key based on model and text content.\n    \"\"\"\n    h = hashlib.sha256((model + \"||\" + text).encode(\"utf-8\")).hexdigest()\n    return h\n\ndef get_cached(key: str):\n    \"\"\"\n    Retrieves cached extraction result if available.\n    \"\"\"\n    p = CACHE_DIR / f\"{key}.json\"\n    return json.loads(p.read_text()) if p.exists() else None\n\ndef set_cached(key: str, data: dict):\n    \"\"\"\n    Stores extraction result in cache, including provenance and metadata.\n    \"\"\"\n    p = CACHE_DIR / f\"{key}.json\"\n    p.write_text(json.dumps(data), encoding=\"utf-8\")\n\nasync def process_batch_with_cache(docs: list, model=\"gpt-4o-mini\", concurrency=8):\n    \"\"\"\n    Processes a batch of documents with disk cache.\n    \"\"\"\n    ex = Extractor(model=model, concurrency=concurrency)\n    results = []\n    \n    for doc_id, text in docs:\n        key = cache_key(text, model)\n        cached = get_cached(key)\n        if cached:\n            results.append((doc_id, cached, {\"ok\": True, \"elapsed\": 0.0, \"cached\": True, \"meta\": cached.get(\"_meta\", {})}))\n            continue\n        results.append((doc_id, None, None))\n\n    async_tasks = []\n    for (doc_id, text), (rid, cached, meta) in zip(docs, results):\n        if cached is None:\n            async_tasks.append(ex.extract_one(doc_id, text))\n    \n    fresh = await asyncio.gather(*async_tasks)\n\n    # Stitch back, set cache\n    out = []\n    fresh_iter = iter(fresh)\n    for (doc_id, text), (rid, cached, meta) in zip(docs, results):\n        if cached is None:\n            did, data, info = next(fresh_iter)\n            if info[\"ok\"]:\n                data[\"_meta\"] = info.get(\"meta\", {})\n                set_cached(cache_key(text, model), data)\n            out.append((did, data, info))\n        else:\n            out.append((doc_id, cached, meta))\n    \n    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Monitoring Token Usage and Cost\n\nCount tokens using `tiktoken` and estimate cost based on model pricing. Always verify current pricing at [OpenAI's pricing page](https://openai.com/pricing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n\ndef count_tokens(text: str, model_encoding: str = \"cl100k_base\"):\n    \"\"\"\n    Counts the number of tokens in a text string.\n    \"\"\"\n    enc = tiktoken.get_encoding(model_encoding)\n    return len(enc.encode(text))\n\n# Example price map; always verify with OpenAI pricing page\nPRICE_PER_1K = {\n    \"gpt-4o-mini\": {\"input\": 0.00015, \"output\": 0.0006},\n    \"gpt-4o\": {\"input\": 0.005, \"output\": 0.015},\n}\n\ndef estimate_cost(input_tokens: int, output_tokens: int, model: str) -> float:\n    \"\"\"\n    Estimates the cost of a request based on token usage and model.\n    \"\"\"\n    p = PRICE_PER_1K.get(model)\n    if not p:\n        return 0.0\n    return (input_tokens / 1000.0) * p[\"input\"] + (output_tokens / 1000.0) * p[\"output\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Post-Processing: Normalize and Validate\n\nNormalize currency symbols, dates, and validate invoice totals. This catches common extraction errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\nfrom datetime import datetime\n\ndef normalize_currency(s: str | None) -> str | None:\n    \"\"\"\n    Normalizes currency symbols to standard codes.\n    \"\"\"\n    if not s:\n        return None\n    s2 = s.strip().upper()\n    if s2 in {\"$\", \"USD\"}:\n        return \"USD\"\n    if s2 in {\"‚Ç¨\", \"EUR\"}:\n        return \"EUR\"\n    if s2 in {\"¬£\", \"GBP\"}:\n        return \"GBP\"\n    return s2\n\ndef normalize_date(s: str | None) -> str | None:\n    \"\"\"\n    Normalizes date strings to YYYY-MM-DD format.\n    \"\"\"\n    if not s:\n        return None\n    for fmt in (\"%Y-%m-%d\", \"%Y/%m/%d\", \"%d-%m-%Y\", \"%m/%d/%Y\", \"%Y-%m-%dT%H:%M:%S\"):\n        try:\n            return datetime.strptime(s.strip()[:19], fmt).strftime(\"%Y-%m-%d\")\n        except Exception:\n            continue\n    return s  # Return as-is if unknown\n\ndef postprocess_invoice(inv: dict) -> dict:\n    \"\"\"\n    Postprocesses invoice dict: normalizes currency, dates, and checks totals.\n    \"\"\"\n    inv[\"currency\"] = normalize_currency(inv.get(\"currency\"))\n    inv[\"invoice_date\"] = normalize_date(inv.get(\"invoice_date\"))\n    inv[\"due_date\"] = normalize_date(inv.get(\"due_date\"))\n    \n    # Sanity check totals\n    try:\n        subtotal = float(inv[\"subtotal\"]) if inv.get(\"subtotal\") is not None else None\n        tax = float(inv[\"tax\"]) if inv.get(\"tax\") is not None else None\n        total = float(inv[\"total\"]) if inv.get(\"total\") is not None else None\n        if subtotal is not None and tax is not None and total is not None:\n            if abs((subtotal + tax) - total) > 0.05:\n                inv[\"_warning\"] = \"Totals may not add up\"\n    except Exception:\n        inv[\"_warning\"] = \"Totals not numeric\"\n    \n    return inv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Run and Validate\n\nGenerate synthetic invoices for testing and run the full pipeline with caching and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n\ndef synthetic_invoice(i: int) -> str:\n    \"\"\"\n    Generates a synthetic invoice for testing.\n    \"\"\"\n    base = f\"\"\"\nINVOICE #INV-{1000+i}\nVendor: Vendor {i} LLC\nAddress: {i} Main Street, Metropolis\nBill To: Customer {i%7}\nDate: 2024-09-{(i%28)+1:02d}\nDue: 2024-10-{(i%28)+1:02d}\nItems:\n- Widget A (QTY {1+i%5}) @ ${3.5 + (i%3)*0.75:.2f} each = ${((1+i%5)*(3.5 + (i%3)*0.75)):.2f}\n- Widget B (QTY {2+i%3}) @ ${1.25 + (i%4)*0.5:.2f} each = ${((2+i%3)*(1.25 + (i%4)*0.5)):.2f}\nSubtotal: $100.00\nTax: $8.25\nTotal Due: $108.25\n\"\"\"\n    # Inject noise for some invoices\n    if i % 9 == 0:\n        base += \"\\nNote: Prices include a 5% eco-fee.\"\n    if i % 11 == 0:\n        base = base.replace(\"Total Due\", \"Grand Total\")\n    return base\n\nasync def run_demo(n_docs=60, model=\"gpt-4o-mini\", concurrency=8):\n    \"\"\"\n    Runs a demo batch extraction and prints metrics.\n    \"\"\"\n    docs = [(f\"doc-{i}\", synthetic_invoice(i)) for i in range(n_docs)]\n    t0 = time.perf_counter()\n    results = await process_batch_with_cache(docs, model=model, concurrency=concurrency)\n    elapsed = time.perf_counter() - t0\n\n    ok = sum(1 for _, _, info in results if info.get(\"ok\"))\n    fail = n_docs - ok\n    total_input_tokens = 0\n    total_output_tokens = 0\n    est_cost = 0.0\n\n    # Prefer usage from metadata if available; otherwise estimate input tokens only\n    for (doc_id, data, info), (_, text) in zip(results, docs):\n        meta = info.get(\"meta\", {})\n        usage = meta.get(\"token_usage\", {}) if isinstance(meta, dict) else {}\n        in_tok = usage.get(\"input_tokens\", 0) or count_tokens(text)\n        out_tok = usage.get(\"output_tokens\", 0)\n        total_input_tokens += in_tok\n        total_output_tokens += out_tok\n        est_cost += estimate_cost(in_tok, out_tok, model)\n\n    print(f\"Processed: {n_docs} docs\")\n    print(f\"Success: {ok}, Failures: {fail}\")\n    print(f\"Elapsed: {elapsed:.2f}s, Avg per doc: {elapsed/n_docs:.2f}s\")\n    print(f\"Tokens ~ input: {total_input_tokens}, output: {total_output_tokens}\")\n    print(f\"Estimated cost: ${est_cost:.4f}\")\n    print(\"‚ö†Ô∏è Verify current pricing at https://openai.com/pricing and adjust PRICE_PER_1K accordingly.\")\n    \n    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the demo in a notebook cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For notebooks/Colab, use await directly or nest_asyncio if needed\nresults = await run_demo(n_docs=60, model=\"gpt-4o-mini\", concurrency=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect a single result with provenance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_id, data, info = results[0]\nprint(f\"Document: {doc_id}\")\nprint(f\"Extracted: {data}\")\nprint(f\"Provenance: {data.get('_provenance', [])}\")\nprint(f\"Warnings: {data.get('_warning', 'None')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Model Selection: Accuracy vs. Cost\n\nPick a model based on accuracy vs cost:\n\n- **gpt-4 class** (e.g., `gpt-4o`, `gpt-4-turbo`): best accuracy for complex schemas, noisy text, and multi-entity extraction. Docs: [OpenAI Models](https://platform.openai.com/docs/models)\n- **gpt-3.5-turbo class**: cheaper, faster, but less reliable for nuanced fields and JSON structure.\n\nIf you're unsure how to evaluate which model best fits your needs, our guide on [how to choose an AI model for your app](/article/how-to-choose-an-ai-model-for-your-app-speed-cost-reliability) breaks down performance, context length, and pricing considerations.\n\n**Recommended defaults:**\n\n- Model: `gpt-4o-mini`\n- Temperature: `0`\n- Max concurrency: `8`\n- Chunk size: `2000` tokens\n- Chunk overlap: `200` tokens\n- Retry attempts: `4` with exponential backoff\n\nUpgrade to `gpt-4o` if you see frequent validation errors or missing fields.\n\n---\n\n## Conclusion\n\nYou've built a production-ready invoice extraction pipeline that handles schema validation, long documents, retries, async batching, caching, and cost tracking. This system is modular‚Äîuse the core extraction logic standalone or scale to thousands of documents with the batch processor.\n\n**Next steps:**\n\n- **Add OCR/PDF preprocessing** ‚Äì Integrate `pdfminer`, `pypdf`, or AWS Textract to handle scanned invoices.\n- **Improve aggregation** ‚Äì Use semantic similarity (embeddings) to merge duplicate line items more robustly.\n- **Deploy as an API** ‚Äì Wrap the batch processor in FastAPI or Flask for production use.\n- **Monitor in production** ‚Äì Add structured logging, alerting, and dashboards to track extraction accuracy and cost over time."
      ]
    }
  ],
  "metadata": {
    "title": "Structured Data Extraction: How to Build a LangChain Pipeline",
    "description": "Build an end-to-end LangChain + OpenAI pipeline for structured data extraction from long documents‚Äîschema validation, batching, retries, and production-ready code.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}