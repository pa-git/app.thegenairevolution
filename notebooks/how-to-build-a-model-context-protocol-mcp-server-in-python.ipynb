{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìì The GenAI Revolution Cookbook\n\n**Title:** How to Build a Model Context Protocol (MCP) Server in Python\n\n**Description:** Learn how to build an MCP server in Python to standardize and reuse AI tools, resources, and prompts across applications. This hands-on guide walks you through server setup, client testing, and GPT-4 chatbot integration for production-ready systems.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here‚Äôs an improved version of your draft. I‚Äôve integrated examples and definitions from ‚ÄúEverything You Need to Know About Model Context Protocol (MCP)‚Äù to align closely with your source, polished wording, shortened complex sentences, and made the flow feel more conversational. Let me know if you want more or less detail in any section.\n\n---\n\nThe Model Context Protocol defines how you, as a client, can discover and call tools, read resources, and render prompts served from a centralized server. Any MCP\\-capable client can make use of your definitions without writing special glue code. For a detailed walkthrough of MCP‚Äôs core concepts‚Äîsuch as tools, resources, and prompts‚Äîand why a standard protocol matters, see ‚ÄúModel Context Protocol (MCP) Explained \\[2025 Guide for Builders]‚Äù at Anthropic. *(Note: I drew heavily from ‚ÄúEverything You Need to Know About MCP‚Äù for the code samples and explanations.)*\n\n---\n\n## What you‚Äôll build in this guide\n\nYou will create:\n\n* A minimal Python MCP server that exposes two arithmetic tools (`add` and `subtract`), a static documentation resource, and a parameterised prompt template.\n* A Python client that lists those capabilities and tests them.\n* An integration that converts the MCC tool schemas into OpenAI‚Äôs function\\-calling format, then completes one conversational turn with GPT\\-4 using them.\n\nBy the end, you‚Äôll have:\n\n* A locally running MCP server using stdio.\n* A client that can list and invoke tools, read resources, and render prompts.\n* A working example of calling MCP tools through OpenAI‚Äôs function\\-calling API.\n\n---\n\n## Why use MCP for this\n\nWithout MCP, every agent or chatbot you build needs custom code to wire up tools, prompts, and resources. You end up duplicating logic, drifting schemas across projects, and synchronizing updates manually.\n\nMCP fixes this. You write your tools once on an MCP server. Then any MCP\\-capable client‚ÄîClaude Desktop, custom agents, or OpenAI\\-powered bots‚Äîcan discover those tools automatically. You centralise tool definitions, versioning, and access control. All client implementations just follow the protocol.\n\nHow MCP stacks up against alternatives:\n\n* Ad\\-hoc tool wiring: You write custom glue per client; no discovery or schema contract.\n* OpenAPI specs: Good for REST, but lacks native support for prompts or embedded resources.\n* LangChain Tools: Powerful, but framework\\-specific. Not always usable outside of LangChain.\n* gRPC: Strong typing, but needs generated code and doesn‚Äôt include prompts or resource abstractions.\n\nMCP is transport\\-agnostic and lightweight. It gives you first\\-class support for tools, resources, and prompts. Perfect for building reusable, discoverable AI capabilities.\n\n---\n\n## Core MCP concepts (aligned with ‚ÄúEverything You Need to Know About MCP‚Äù)\n\n* **Server**: Implements MCP. It exposes tools, resources, and prompts to clients via a defined API.\n* **Tools**: Functions the model can invoke automatically. You describe them with name, description, and input schema. MCP supports discovery (list) and invocation (call). ([modelcontextprotocol.wiki](https://modelcontextprotocol.wiki/en/docs/concepts/tools?utm_source=openai))\n* **Resources**: Readable, structured data‚Äîbut no side effects. They are identified by URIs. Use them to provide context like docs, logs, settings. ([modelcontextprotocol.io](https://modelcontextprotocol.io/docs/learn/server-concepts?utm_source=openai))\n* **Prompts**: Parameterised templates to guide conversations. Clients list and fetch prompt templates by name, supplying arguments. Why? To standardise workflows and reuse best practices. ([modelcontextprotocol.wiki](https://modelcontextprotocol.wiki/en/docs/concepts/prompts?utm_source=openai))\n* **Transport**: MCP servers and clients can use stdio, HTTP (often with server\\-sent events), or other transports. Stdio is common for local workflows. ([github.com](https://github.com/modelcontextprotocol/python-sdk?utm_source=openai))\n\n---\n\n## Setup \\& server code (mostly as in your draft but streamlined)\n\n### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install \"mcp[cli]\" openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set `OPENAI_API_KEY` in your environment. If not already set, prompt for it at runtime.\n\n### Server module (`mcp_calc_server.py`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\nimport logging\nfrom typing import Any, Dict, List\n\nfrom mcp.server import Server\nfrom mcp.server.stdio import stdio_server\nfrom mcp.types import TextContent\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"mcp_calc_server\")\n\nserver = Server(\"calc-mcp\")\n\n@server.tool(\n    \"add\",\n    description=\"Add two numbers\",\n    input_schema={\n        \"type\": \"object\",\n        \"properties\": {\n            \"a\": {\"type\": \"number\"},\n            \"b\": {\"type\": \"number\"}\n        },\n        \"required\": [\"a\", \"b\"]\n    }\n)\nasync def add_tool(args: Dict[str, Any]) -> List[TextContent]:\n    try:\n        a = float(args[\"a\"])\n        b = float(args[\"b\"])\n        result = a + b\n        logger.info(f\"add: {a} + {b} = {result}\")\n        return [TextContent(type=\"text\", text=str(result))]\n    except Exception as e:\n        logger.error(f\"add_tool invalid input {args}: {e}\")\n        raise ValueError(\"Both 'a' and 'b' must be numbers.\")\n\n@server.tool(\n    \"subtract\",\n    description=\"Subtract b from a\",\n    input_schema={\n        \"type\": \"object\",\n        \"properties\": {\n            \"a\": {\"type\": \"number\"},\n            \"b\": {\"type\": \"number\"}\n        },\n        \"required\": [\"a\", \"b\"]\n    }\n)\nasync def subtract_tool(args: Dict[str, Any]) -> List[TextContent]:\n    try:\n        a = float(args[\"a\"])\n        b = float(args[\"b\"])\n        result = a - b\n        logger.info(f\"subtract: {a} - {b} = {result}\")\n        return [TextContent(type=\"text\", text=str(result))]\n    except Exception as e:\n        logger.error(f\"subtract_tool invalid input {args}: {e}\")\n        raise ValueError(\"Both 'a' and 'b' must be numbers.\")\n\nDOCS_URI = \"docs://calc/quickstart\"\n\n@server.resource(DOCS_URI, description=\"Calculator server quickstart\")\nasync def read_docs() -> List[TextContent]:\n    content = (\n        \"Calculator MCP Server\\n\"\n        \"- Tools: add(a, b), subtract(a, b)\\n\"\n        \"- All numeric inputs are coerced to float.\\n\"\n        \"- Results are returned as text.\"\n    )\n    return [TextContent(type=\"text\", text=content)]\n\nPROMPT_NAME = \"calc_instructions\"\n\n@server.prompt(\n    PROMPT_NAME,\n    description=\"Explain a calculation before returning the numeric result.\",\n    arguments_schema={\n        \"type\": \"object\",\n        \"properties\": {\n            \"task\": {\n                \"type\": \"string\",\n                \"description\": \"Natural language math task\"\n            }\n        },\n        \"required\": [\"task\"]\n    }\n)\nasync def render_calc_prompt(args: Dict[str, Any]) -> List[TextContent]:\n    task = str(args[\"task\"])\n    template = (\n        \"You are a careful math assistant.\\n\"\n        \"Task: {task}\\n\"\n        \"Explain your steps briefly, then give the final numeric answer.\"\n    )\n    return [TextContent(type=\"text\", text=template.format(task=task))]\n\nasync def main():\n    async with stdio_server() as (read, write):\n        await server.run(read, write)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Client code \\& validation\n\nThe client will launch the server as a subprocess, then use `ClientSession` to:\n\n* initialize the session\n* list tools, resources, prompts\n* call `add` and check the result\n* read the documentation resource\n* render the prompt for a sample task\n\nYou will see output like:\n\n* Tools: `[\"add\", \"subtract\"]`\n* Resources: `[\"docs://calc/quickstart\"]`\n* Prompts: `[\"calc_instructions\"]`\n* `add(3, 5) => 8.0`\n* The docs content\n* Rendered prompt with your task filled in\n\n---\n\n## Mapping MCP tools to OpenAI function\\-calling\n\nYou need OpenAI functions formatted with name, description, and JSON schema parameters. Example conversion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mcp_tools_to_openai(tools):\n    openai_tools = []\n    for t in tools:\n        schema = getattr(t, \"input_schema\", {\"type\":\"object\", \"properties\":{}})\n        openai_tools.append({\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": t.name,\n                \"description\": t.description or \"\",\n                \"parameters\": schema\n            }\n        })\n    return openai_tools\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Complete flow: ask GPT\\-4 and route tool calls\n\n1. Prepare OpenAI tool definitions.\n2. Send a chat message with model ‚Äúgpt\\-4o‚Äù, letting it decide whether to call a tool.\n3. If it chooses a tool, parse that choice and send the arguments to your MCP server via the client.\n4. Get the tool result back.\n5. Send a final message to OpenAI combining the original assistant message and the tool results so the model can produce a proper human\\-facing answer.\n\nYou should get something like:\n`\"The result of 12.5 + 30.2 is 42.7.\"`\n\n---\n\n## Best practices \\& debugging tips\n\n* Ensure your `OPENAI_API_KEY` is correctly set.\n* Check imports for `mcp` and `openai`. Install with `pip` if missing.\n* Use correct attribute names: for example, `input_schema`, not `inputSchema`.\n* Capture JSON parsing errors when reading arguments from responses, especially if the model returns something unexpected.\n* Make sure to validate tool arguments using the schema to avoid invalid data.\n\n---\n\n## What‚Äôs going on under the hood (based on the MCP spec)\n\n* Servers expose three primitives: **tools**, **resources**, **prompts**. ([github.com](https://github.com/modelcontextprotocol/python-sdk?utm_source=openai))\n* Tools provide executable actions, discovered via `tools/list` and invoked via `tools/call`. ([modelcontextprotocol.wiki](https://modelcontextprotocol.wiki/en/docs/concepts/tools?utm_source=openai))\n* Resources provide read\\-only data, discovered via `resources/list` and accessed via `resources/read`. ([modelcontextprotocol.io](https://modelcontextprotocol.io/docs/learn/server-concepts?utm_source=openai))\n* Prompts define templates with arguments. Clients list prompts via `prompts/list`, then fetch specific prompts via `prompts/get`. ([modelcontextprotocol.wiki](https://modelcontextprotocol.wiki/en/docs/concepts/prompts?utm_source=openai))\n\n---\n\n## Conclusion\n\nYou built a minimal MCP server exposing arithmetic tools, a documentation resource, and a prompt template. You validated it with a Python client. Then you integrated it with OpenAI‚Äôs function\\-calling API to complete a full conversational turn.\n\nNext, think about replacing the calculator tools with real\\-world capabilities: weather APIs, databases, or web search. Add observability to measure latency and accuracy. If you want to deploy this, containerise the server (for example with Docker). Also explore advanced MCP features like sampling, resource or prompt updates, and argument completion.\n\nYou now have a reusable and discoverable interface for capabilities. It‚Äôs ready to power any MCP\\-capable client."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build a Model Context Protocol (MCP) Server in Python",
    "description": "Learn how to build an MCP server in Python to standardize and reuse AI tools, resources, and prompts across applications. This hands-on guide walks you through server setup, client testing, and GPT-4 chatbot integration for production-ready systems.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}