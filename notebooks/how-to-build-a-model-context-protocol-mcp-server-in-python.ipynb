{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build a Model Context Protocol (MCP) Server in Python\n\n**Description:** Learn how to build an MCP server in Python to standardize and reuse AI tools, resources, and prompts across applications. This hands-on guide walks you through server setup, client testing, and GPT-4 chatbot integration for production-ready systems.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Model Context Protocol defines how clients discover and call tools, read resources, and render prompts from a server. Any MCP-capable client can connect and use your definitions without one-off glue code. For a comprehensive introduction to MCP's core concepts and why standardization matters, see our [Model Context Protocol (MCP) Explained [2025 Guide for Builders]](/article/model-context-protocol-mcp-explained-2025-guide-for-builders).\n\nThis guide walks you through building a minimal Python MCP server that exposes two arithmetic tools (`add` and `subtract`), a static documentation resource, and a parameterized prompt template. You'll validate the server with a Python client, then map its tool schemas to OpenAI's function-calling format and complete a single conversational turn with GPT-4.\n\nBy the end, you'll have:\n- A local MCP server exposing tools, resources, and prompts over stdio\n- A Python client that lists and calls these capabilities\n- A working integration that routes OpenAI tool calls to your MCP server and returns a final answer\n\n---\n\n## Why Use MCP for This Problem\n\nWithout MCP, every agent or chatbot you build requires custom glue code to wire tools, prompts, and data sources. You duplicate integration logic, drift schemas across projects, and manually sync updates.\n\nMCP solves this by standardizing discovery and invocation. Write your tools once as an MCP server; any MCP-capable clientâ€”Claude Desktop, custom agents, or OpenAI-backed chatbotsâ€”can discover and call them without per-client adapters. You centralize tool definitions, versioning, and access control in one place.\n\nCompared to alternatives:\n- **Ad-hoc tool wiring**: Requires custom code per client; no schema discovery.\n- **OpenAPI specs**: REST-only; no native support for prompts or resources.\n- **LangChain Tools**: Framework-specific; not interoperable outside LangChain.\n- **gRPC**: Requires code generation and lacks built-in prompt/resource abstractions.\n\nMCP provides a lightweight, transport-agnostic protocol with first-class support for tools, resources, and promptsâ€”ideal for reusable, discoverable AI capabilities.\n\n---\n\n## Core Concepts for This Use Case\n\n**Server**: An MCP server exposes capabilities (tools, resources, prompts) to clients. You define it using the `Server` class from the `mcp.server` module.\n\n**Tools**: Functions the client can invoke. Each tool has a name, description, and JSON Schema for input validation. Decorate functions with `@server.tool()` to register them.\n\n**Resources**: Static or dynamic content identified by URI (e.g., `docs://calc/quickstart`). Use `@server.resource()` to serve documentation, policies, or datasets.\n\n**Prompts**: Parameterized templates that clients can render with arguments. Use `@server.prompt()` to define reusable instruction patterns.\n\n**Stdio transport**: MCP servers and clients communicate over standard input/output. The `stdio_server()` context manager handles message framing; `StdioClient` launches the server as a subprocess and connects to it.\n\n**ClientSession**: Manages the lifecycle of a client connection. Use `session.initialize()` to handshake, then call `list_tools()`, `call_tool()`, `read_resource()`, and `render_prompt()` to interact with the server.\n\n---\n\n## Setup\n\nRun this cell to install the MCP SDK and OpenAI client library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q \"mcp[cli]\" \"openai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your OpenAI API key (required for the final integration step):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom getpass import getpass\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the server module file in the notebook environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nserver_code = '''\nimport asyncio\nimport logging\nfrom typing import Any, Dict, List\n\nfrom mcp.server import Server\nfrom mcp.server.stdio import stdio_server\nfrom mcp.types import TextContent\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"mcp_calc_server\")\n\nserver = Server(\"calc-mcp\")\n\n@server.tool(\n    \"add\",\n    description=\"Add two numbers\",\n    input_schema={\n        \"type\": \"object\",\n        \"properties\": {\n            \"a\": {\"type\": \"number\"},\n            \"b\": {\"type\": \"number\"}\n        },\n        \"required\": [\"a\", \"b\"],\n    },\n)\nasync def add_tool(args: Dict[str, Any]) -> List[TextContent]:\n    \"\"\"\n    Adds two numbers provided in the arguments.\n\n    Args:\n        args (Dict[str, Any]): Dictionary with keys 'a' and 'b' (numbers).\n\n    Returns:\n        List[TextContent]: List containing the sum as a TextContent object.\n\n    Raises:\n        ValueError: If 'a' or 'b' is missing or not a number.\n    \"\"\"\n    try:\n        a = float(args[\"a\"])\n        b = float(args[\"b\"])\n        result = a + b\n        logger.info(f\"add_tool called with a={a}, b={b}, result={result}\")\n        return [TextContent(type=\"text\", text=str(result))]\n    except (KeyError, ValueError, TypeError) as e:\n        logger.error(f\"Invalid input for add_tool: {args} ({e})\")\n        raise ValueError(\"Both 'a' and 'b' must be valid numbers.\")\n\n@server.tool(\n    \"subtract\",\n    description=\"Subtract b from a\",\n    input_schema={\n        \"type\": \"object\",\n        \"properties\": {\n            \"a\": {\"type\": \"number\"},\n            \"b\": {\"type\": \"number\"}\n        },\n        \"required\": [\"a\", \"b\"],\n    },\n)\nasync def subtract_tool(args: Dict[str, Any]) -> List[TextContent]:\n    \"\"\"\n    Subtracts 'b' from 'a' provided in the arguments.\n\n    Args:\n        args (Dict[str, Any]): Dictionary with keys 'a' and 'b' (numbers).\n\n    Returns:\n        List[TextContent]: List containing the difference as a TextContent object.\n\n    Raises:\n        ValueError: If 'a' or 'b' is missing or not a number.\n    \"\"\"\n    try:\n        a = float(args[\"a\"])\n        b = float(args[\"b\"])\n        result = a - b\n        logger.info(f\"subtract_tool called with a={a}, b={b}, result={result}\")\n        return [TextContent(type=\"text\", text=str(result))]\n    except (KeyError, ValueError, TypeError) as e:\n        logger.error(f\"Invalid input for subtract_tool: {args} ({e})\")\n        raise ValueError(\"Both 'a' and 'b' must be valid numbers.\")\n\nDOCS_URI = \"docs://calc/quickstart\"\n\n@server.resource(DOCS_URI, description=\"Calculator server quickstart\")\nasync def read_docs() -> List[TextContent]:\n    \"\"\"\n    Returns documentation for the calculator MCP server.\n\n    Returns:\n        List[TextContent]: List containing the documentation as a TextContent object.\n    \"\"\"\n    content = (\n        \"Calculator MCP Server\\\\n\"\n        \"- Tools: add(a,b), subtract(a,b)\\\\n\"\n        \"- All numeric inputs are coerced to float.\\\\n\"\n        \"- Results are returned as text.\"\n    )\n    return [TextContent(type=\"text\", text=content)]\n\nPROMPT_NAME = \"calc_instructions\"\n\n@server.prompt(\n    PROMPT_NAME,\n    description=\"Explain a calculation before returning the numeric result.\",\n    arguments_schema={\n        \"type\": \"object\",\n        \"properties\": {\n            \"task\": {\"type\": \"string\", \"description\": \"Natural language math task\"}\n        },\n        \"required\": [\"task\"],\n    },\n)\nasync def render_calc_prompt(args: Dict[str, Any]) -> List[TextContent]:\n    \"\"\"\n    Renders a prompt explaining a calculation task.\n\n    Args:\n        args (Dict[str, Any]): Dictionary with key 'task' (string).\n\n    Returns:\n        List[TextContent]: List containing the rendered prompt as a TextContent object.\n    \"\"\"\n    task = str(args[\"task\"])\n    template = (\n        \"You are a careful math assistant.\\\\n\"\n        \"Task: {task}\\\\n\"\n        \"Explain your steps briefly, then provide the final numeric answer.\"\n    )\n    return [TextContent(type=\"text\", text=template.format(task=task))]\n\nasync def main():\n    \"\"\"\n    Entry point for running the MCP server over stdio.\n    \"\"\"\n    async with stdio_server() as (read, write):\n        await server.run(read, write)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n'''\n\nPath(\"mcp_calc_server.py\").write_text(server_code)\nprint(\"Server module created: mcp_calc_server.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Build the MCP Server\n\nThe server code above defines:\n- **Two tools** (`add` and `subtract`) that accept numeric arguments and return text results\n- **One resource** (`docs://calc/quickstart`) that serves static documentation\n- **One prompt** (`calc_instructions`) that renders a parameterized instruction template\n\nEach tool validates inputs, coerces them to floats, and logs the operation. The server runs over stdio, reading JSON-RPC messages from stdin and writing responses to stdout.\n\n---\n\n## Validate with a Python Client\n\nThis cell launches the server as a subprocess, connects a client, and tests tool discovery, resource reading, and prompt rendering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\nimport sys\nfrom mcp.client.stdio import StdioClient\nfrom mcp.client.session import ClientSession\n\nasync def test_server():\n    \"\"\"\n    Launches the MCP server as a subprocess and tests tool/resource/prompt discovery and usage.\n    \"\"\"\n    cmd = [sys.executable, \"mcp_calc_server.py\"]\n    async with StdioClient(cmd) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n\n            # List available tools\n            tools = await session.list_tools()\n            print(\"Tools:\", [t.name for t in tools])\n\n            # List available resources\n            resources = await session.list_resources()\n            print(\"Resources:\", [r.uri for r in resources])\n\n            # List available prompts\n            prompts = await session.list_prompts()\n            print(\"Prompts:\", [p.name for p in prompts])\n\n            # Call 'add' tool and print result\n            result = await session.call_tool(\"add\", {\"a\": 3, \"b\": 5})\n            print(\"add(3,5) =>\", \"\".join([c.text for c in result if hasattr(c, \"text\")]))\n\n            # Read documentation resource and print content\n            doc_content = await session.read_resource(\"docs://calc/quickstart\")\n            print(\"Docs:\", \"\".join([c.text for c in doc_content if hasattr(c, \"text\")]))\n\n            # Render a prompt and print the result\n            rendered = await session.render_prompt(\"calc_instructions\", {\"task\": \"Add 12 and 30\"})\n            print(\"Rendered Prompt:\", \"\".join([c.text for c in rendered if hasattr(c, \"text\")]))\n\n            await session.shutdown()\n\nawait test_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see:\n- Tool names: `['add', 'subtract']`\n- Resource URI: `['docs://calc/quickstart']`\n- Prompt name: `['calc_instructions']`\n- `add(3,5) => 8.0`\n- Documentation text\n- Rendered prompt with the task filled in\n\n---\n\n## Map MCP Tools to OpenAI Function Calling\n\nOpenAI's function-calling API expects tool definitions in a specific format. This function converts MCP tool schemas to OpenAI-compatible definitions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mcp_tools_to_openai(tools):\n    \"\"\"\n    Converts a list of MCP tool objects to OpenAI function-calling tool definitions.\n\n    Args:\n        tools (list): List of MCP tool objects.\n\n    Returns:\n        list: List of OpenAI-compatible tool definitions (dicts).\n    \"\"\"\n    openai_tools = []\n    for t in tools:\n        # Access input_schema attribute (SDK uses snake_case)\n        input_schema = getattr(t, \"input_schema\", None) or {\"type\": \"object\", \"properties\": {}}\n        openai_tools.append({\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": t.name,\n                \"description\": t.description or \"\",\n                \"parameters\": input_schema\n            }\n        })\n    return openai_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Prepare the OpenAI Client and Tool Catalog\n\nThis cell initializes the OpenAI client and fetches tool definitions from the MCP server, converting them to OpenAI format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nasync def prepare_tools():\n    \"\"\"\n    Fetches tool definitions from the MCP server and converts them to OpenAI format.\n\n    Returns:\n        list: List of OpenAI-compatible tool definitions.\n    \"\"\"\n    cmd = [sys.executable, \"mcp_calc_server.py\"]\n    async with StdioClient(cmd) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n            tools = await session.list_tools()\n            await session.shutdown()\n            return mcp_tools_to_openai(tools)\n\nopenai_tools = await prepare_tools()\nprint(\"OpenAI tool catalog prepared:\", [t[\"function\"][\"name\"] for t in openai_tools])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Make an Initial Chat Completion\n\nThis cell sends a user question to GPT-4 with the tool catalog. The model decides whether to call a function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def first_turn(openai_tools):\n    \"\"\"\n    Sends an initial chat completion request to OpenAI with tool catalog.\n\n    Args:\n        openai_tools (list): List of OpenAI-compatible tool definitions.\n\n    Returns:\n        OpenAI response object.\n    \"\"\"\n    resp = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You can call functions to perform precise math.\"},\n            {\"role\": \"user\", \"content\": \"What is 12.5 + 30.2?\"}\n        ],\n        tools=openai_tools,\n        tool_choice=\"auto\",\n        temperature=0  # Deterministic for arithmetic\n    )\n    return resp\n\ninitial_resp = await first_turn(openai_tools)\nprint(\"Model response:\", initial_resp.choices[0].message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the model decides to call a function, `tool_calls` will be populated in the response.\n\n---\n\n## Route Tool Calls to the MCP Server\n\nThis cell detects tool calls in the OpenAI response, routes them to the MCP server, and formats the results for OpenAI:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n\nasync def route_tool_calls(resp, session):\n    \"\"\"\n    Routes tool calls from OpenAI response to the MCP server and formats results.\n\n    Args:\n        resp: OpenAI response object.\n        session: Active MCP ClientSession.\n\n    Returns:\n        list: List of tool result messages formatted for OpenAI.\n    \"\"\"\n    tool_calls = resp.choices[0].message.tool_calls or []\n    tool_results_msgs = []\n    for tc in tool_calls:\n        name = tc.function.name\n        try:\n            args = json.loads(tc.function.arguments or \"{}\")\n        except json.JSONDecodeError as e:\n            print(f\"Failed to parse arguments for {name}: {e}\")\n            args = {}\n        # Call the corresponding MCP tool\n        result_parts = await session.call_tool(name, args)\n        result_text = \"\".join([c.text for c in result_parts if hasattr(c, \"text\")])\n        tool_results_msgs.append({\n            \"role\": \"tool\",\n            \"tool_call_id\": tc.id,\n            \"content\": result_text\n        })\n    return tool_results_msgs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Finalize the Answer\n\nThis cell sends the original assistant message, tool call metadata, and tool results back to OpenAI to get a final user-facing answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def finalize_answer(initial_resp, tool_msgs):\n    \"\"\"\n    Sends the original assistant message, tool call metadata, and tool results to OpenAI for a final answer.\n\n    Args:\n        initial_resp: Initial OpenAI response object.\n        tool_msgs (list): List of tool result messages.\n\n    Returns:\n        OpenAI response object with the final answer.\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"You can call functions to perform precise math.\"},\n        {\"role\": \"user\", \"content\": \"What is 12.5 + 30.2?\"},\n        initial_resp.choices[0].message,\n    ]\n    messages.extend(tool_msgs)\n\n    final = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        temperature=0\n    )\n    return final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Run the Full Conversation\n\nThis cell orchestrates the entire flow: prepare tools, ask a question, route tool calls, and print the final answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def chat():\n    \"\"\"\n    Orchestrates a full conversation: fetches tools, asks a question, routes tool calls, and prints the final answer.\n    \"\"\"\n    openai_tools = await prepare_tools()\n    cmd = [sys.executable, \"mcp_calc_server.py\"]\n    async with StdioClient(cmd) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n\n            # First turn: ask the model a question\n            initial = await first_turn(openai_tools)\n            # Route any tool calls to the MCP server\n            tool_msgs = await route_tool_calls(initial, session)\n            # Get the final answer from OpenAI\n            final = finalize_answer(initial, tool_msgs)\n\n            print(\"Final answer:\", final.choices[0].message.content)\n            await session.shutdown()\n\nawait chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see a final answer like: `\"The result of 12.5 + 30.2 is 42.7.\"`\n\n---\n\n## Run and Evaluate\n\nRun the full conversation cell above to confirm the entire pipeline works end-to-end. You should observe:\n- The MCP server starts as a subprocess\n- The client fetches tool definitions and converts them to OpenAI format\n- GPT-4 receives the user question and decides to call the `add` tool\n- The client routes the tool call to the MCP server and receives `42.7`\n- GPT-4 receives the tool result and generates a natural language answer\n\nIf you encounter errors:\n- **Missing API key**: Ensure `OPENAI_API_KEY` is set in the environment.\n- **Import errors**: Verify `mcp` and `openai` are installed.\n- **Attribute errors**: Check that `input_schema` (not `inputSchema`) is used in `mcp_tools_to_openai`.\n- **JSON decode errors**: Inspect `tc.function.arguments` for malformed JSON; add logging to `route_tool_calls`.\n\nFor production use, add:\n- **Logging**: Use `logging.info()` to trace tool calls and results.\n- **Schema validation**: Validate tool arguments against the input schema before calling.\n- **Error handling**: Catch and surface exceptions from tool execution to the client.\n\n---\n\n## Conclusion\n\nYou've built a minimal MCP server exposing arithmetic tools, a documentation resource, and a parameterized prompt. You validated it with a Python client and integrated it with OpenAI's function-calling API to complete a conversational turn.\n\nNext steps:\n- Replace the calculator tools with real API calls (e.g., weather, database queries, or web search)\n- Add evaluation and observability with Langfuse to track tool call accuracy and latency\n- Containerize the server with Docker for deployment as a standalone service\n- Explore MCP's sampling and roots features for advanced agent workflows\n\nYour server is now a reusable, discoverable interface to capabilitiesâ€”ready to power any MCP-capable client."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build a Model Context Protocol (MCP) Server in Python",
    "description": "Learn how to build an MCP server in Python to standardize and reuse AI tools, resources, and prompts across applications. This hands-on guide walks you through server setup, client testing, and GPT-4 chatbot integration for production-ready systems.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}