{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build a Model Context Protocol (MCP) Server in Python\n\n**Description:** Learn how to build an MCP server in Python to standardize and reuse AI tools, resources, and prompts across applications. This hands-on guide walks you through server setup, client testing, and GPT-4 chatbot integration for production-ready systems.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nThe Model Context Protocol (MCP) lets you define tools, resources, and prompts once and expose them to any MCP-capable clientâ€”from CLIs to agents to chatbots. Instead of rewriting tool logic for each application, you build a single server that clients discover and invoke automatically.\n\nThis guide walks you through building a minimal Python MCP server over stdio, testing it with a client, and verifying that tools, resources, and prompts work as expected. By the end, you'll have a working server that exposes math tools, static documentation, and a prompt templateâ€”all runnable in a notebook or local environment.\n\nFor a comprehensive overview of how MCP standardizes tool and data access, see our [Model Context Protocol (MCP) Explained [2025 Guide for Builders]](/article/model-context-protocol-mcp-explained-2025-guide-for-builders).\n\n## Why Use MCP for This Problem\n\nWhen you need to share tools across multiple applications, you have several options:\n\n- **Shared Python package**: Requires every app to import and maintain the same library version; no runtime discovery or schema negotiation.\n- **Bespoke HTTP microservice**: Adds network overhead, requires custom API design, and lacks standardized tool metadata.\n- **OpenAI-native tools defined per app**: Forces you to duplicate tool definitions in every codebase; no single source of truth.\n- **Agent-framework-specific tools**: Locks you into one framework's API; porting to another requires rewriting.\n\nMCP solves these problems by providing:\n\n- **Automatic discovery**: Clients list available tools, resources, and prompts at runtime.\n- **Standardized schemas**: JSON Schema definitions ensure consistent validation and documentation.\n- **Transport abstraction**: Stdio, SSE, or WebSocketâ€”clients and servers negotiate capabilities without custom protocols.\n\nCentralizing these capabilities in one server removes repetition and lets clients discover and invoke standardized functionality automatically. If you want to avoid subtle bugs caused by tokenization quirks, check out our guide on [Tokenization Pitfalls: Invisible Characters That Break Prompts and RAG](/article/tokenization-pitfalls-invisible-characters-that-break-prompts-and-rag-2).\n\nA single MCP server means one place to update logic and metadata, one schema for validation, and consistent behavior across apps. However, remember that LLM context is not infinite memoryâ€”if you're scaling up prompt sizes or chaining calls, you should be aware of [Context Rot - Why LLMs \"Forget\" as Their Memory Grows](/article/context-rot-why-llms-forget-as-their-memory-grows-3).\n\n## Core Concepts for This Use Case\n\nBefore building, understand these MCP primitives:\n\n- **Tools**: Functions the client can invoke with typed arguments; the server executes and returns results.\n- **Resources**: Static or dynamic data (text, JSON, binary) identified by URI; clients read them on demand.\n- **Prompts**: Reusable templates that generate messages for LLM conversations; clients fetch and render them with arguments.\n- **Stdio transport**: Server and client communicate over standard input/output; simplest option for local development and subprocess integration.\n- **Schema negotiation**: Server advertises tool input schemas (JSON Schema) and resource MIME types; clients validate and adapt accordingly.\n\n## Setup\n\nInstall the required packages in a notebook cell or terminal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install \"mcp[cli]>=0.9.0\" anyio>=4.0.0 openai>=1.40.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This installs the MCP SDK, async runtime, and OpenAI client. Pin versions for reproducibility.\n\n## Using the Tool in Practice\n\n### Build the MCP Server\n\nCreate a server that exposes two math tools, a static markdown resource, and a prompt template. This example uses stdio transport for simplicity.\n\nWrite the server to a file using a notebook cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile mcp_server.py\n# Purpose: Minimal MCP server exposing math tools, a static resource, and a prompt template over stdio.\n\nimport anyio\nfrom typing import Annotated\n\n# MCP server APIs\nfrom mcp.server import Server\nfrom mcp.server.stdio import stdio_server\nfrom mcp.types import (\n    PromptMessage,\n    TextResourceContents,\n    Resource,\n    Prompt,\n)\n\n# Instantiate the MCP server with a unique name\nserver = Server(\"calc-server\")\n\n@server.tool()\nasync def add(a: Annotated[int, \"First integer\"], b: Annotated[int, \"Second integer\"]) -> int:\n    \"\"\"\n    Add two integers and return the sum.\n\n    Args:\n        a (int): First integer.\n        b (int): Second integer.\n\n    Returns:\n        int: The sum of a and b.\n    \"\"\"\n    # Simple addition; no edge cases for int\n    return a + b\n\n@server.tool()\nasync def subtract(a: Annotated[int, \"Minuend\"], b: Annotated[int, \"Subtrahend\"]) -> int:\n    \"\"\"\n    Subtract b from a and return the difference.\n\n    Args:\n        a (int): Minuend.\n        b (int): Subtrahend.\n\n    Returns:\n        int: The result of a - b.\n    \"\"\"\n    # Simple subtraction; no edge cases for int\n    return a - b\n\n# Resource: static markdown documentation\nDOCS_ID = \"docs/usage\"\nDOCS_CONTENT = \"\"\"# Calc Server Usage\n\nTools:\n- add(a: int, b: int): returns a + b\n- subtract(a: int, b: int): returns a - b\n\nPrompt:\n- math_helper(expression: string): step-by-step computation\n\"\"\"\n\n@server.resource(DOCS_ID, mime_type=\"text/markdown\")\nasync def read_docs() -> TextResourceContents:\n    \"\"\"\n    Return static markdown documentation for the server.\n\n    Returns:\n        TextResourceContents: Markdown-formatted usage documentation.\n    \"\"\"\n    # TextResourceContents includes text and optional annotations\n    return TextResourceContents(text=DOCS_CONTENT)\n\n@server.prompt(\"math_helper\")\nasync def math_helper(expression: Annotated[str, \"Math expression to compute\"]):\n    \"\"\"\n    Generate a prompt for step-by-step math computation.\n\n    Args:\n        expression (str): Math expression to compute.\n\n    Returns:\n        list[PromptMessage]: System and user prompt messages.\n    \"\"\"\n    # System prompt instructs the assistant to show work\n    system = PromptMessage(role=\"system\", content=\"You are a careful math assistant. Show your work.\")\n    # User prompt includes the expression to compute\n    user = PromptMessage(role=\"user\", content=f\"Compute the following expression step by step: {expression}\")\n    return [system, user]\n\nasync def main():\n    \"\"\"\n    Main entry point: runs the MCP server over stdio until EOF.\n    \"\"\"\n    # stdio transport: run until EOF\n    async with stdio_server() as (read_stream, write_stream):\n        await server.run(read_stream, write_stream)\n\nif __name__ == \"__main__\":\n    anyio.run(main)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `@server.tool()` decorator registers async functions as tools. The `@server.resource()` decorator exposes static or dynamic data by URI. The `@server.prompt()` decorator defines reusable prompt templates. The server runs over stdio, reading requests from stdin and writing responses to stdout.\n\n### Test the Server with a Client\n\nWrite a client that connects to the server, lists capabilities, calls a tool, reads a resource, and fetches a prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile mcp_client.py\n# Purpose: Async MCP client to test server tools, resources, and prompts over stdio.\n\nimport anyio\nfrom mcp.client.stdio import stdio_client\nfrom mcp.client.session import ClientSession\n\nasync def main():\n    \"\"\"\n    Connects to the MCP server, lists tools/resources/prompts, calls a tool, and fetches a prompt.\n\n    Raises:\n        Exception: If server connection or calls fail.\n    \"\"\"\n    # Launch the server as a subprocess and connect over stdio\n    async with stdio_client([\"python\", \"mcp_server.py\"]) as (read_stream, write_stream):\n        async with ClientSession(read_stream, write_stream) as session:\n            # List tools and print their names\n            tools = await session.list_tools()\n            print(\"Tools:\", [t.name for t in tools.tools])\n\n            # List resources and print their URIs\n            resources = await session.list_resources()\n            print(\"Resources:\", [r.uri for r in resources.resources])\n\n            # Read and print documentation resource if present\n            for r in resources.resources:\n                if r.uri.endswith(\"docs/usage\"):\n                    content = await session.read_resource(r.uri)\n                    # content.contents is a list of typed chunks (e.g., text, blob)\n                    for c in content.contents:\n                        if hasattr(c, \"text\"):\n                            print(\"Docs:\\n\", c.text)\n\n            # Call 'add' tool with arguments and print the result\n            result = await session.call_tool(\"add\", {\"a\": 5, \"b\": 7})\n            # result.content is list of output messages; pick first text\n            out = result.content[0].text if result.content else None\n            print(\"add(5,7) =\", out)\n\n            # Fetch prompt template and print its messages\n            prompts = await session.list_prompts()\n            print(\"Prompts:\", [p.name for p in prompts.prompts])\n            prompt = await session.get_prompt(\"math_helper\", {\"expression\": \"3*(4+2)\"})\n            print(\"Prompt messages:\", [(m.role, m.content) for m in prompt.messages])\n\nif __name__ == \"__main__\":\n    anyio.run(main)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The client launches the server as a subprocess, establishes a session over stdio, and exercises all three primitives: tools, resources, and prompts. The `stdio_client` context manager handles process lifecycle and stream wiring.\n\n## Run and Evaluate\n\nRun the client to verify the server works end-to-end:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python mcp_client.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tools: ['add', 'subtract']\nResources: ['docs/usage']\nDocs:\n # Calc Server Usage\n\nTools:\n- add(a: int, b: int): returns a + b\n- subtract(a: int, b: int): returns a - b\n\nPrompt:\n- math_helper(expression: string): step-by-step computation\n\nadd(5,7) = 12\nPrompts: ['math_helper']\nPrompt messages: [('system', 'You are a careful math assistant. Show your work.'), ('user', 'Compute the following expression step by step: 3*(4+2)')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you see this output, your server is correctly exposing tools, resources, and prompts. The client successfully discovered and invoked each capability.\n\n### Optional: Add Resilience to Tool Calls\n\nFor production use, wrap tool calls in a timeout and error handler. Create a helper function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile helpers/safe_call.py\n# Purpose: Utility for safe, timeout-guarded MCP tool calls.\n\nfrom anyio import fail_after, WouldBlock\nimport logging\n\nasync def safe_call_tool(mcp, name, args, seconds=10):\n    \"\"\"\n    Call an MCP tool with a timeout and error handling.\n\n    Args:\n        mcp: MCP client session.\n        name (str): Tool name.\n        args (dict): Tool arguments.\n        seconds (int): Timeout in seconds.\n\n    Returns:\n        Tool call result or None if failed/timed out.\n\n    Raises:\n        None: All exceptions are caught and logged.\n    \"\"\"\n    try:\n        with fail_after(seconds):\n            return await mcp.call_tool(name, args)\n    except WouldBlock:\n        # Timeout occurred\n        logging.warning(f\"Tool call to {name} timed out after {seconds}s.\")\n        return None\n    except Exception as e:\n        # Log error and return None\n        logging.error(f\"Tool call {name} failed: {e}\")\n        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use `safe_call_tool` in place of direct `session.call_tool` to prevent hanging or crashing on slow or failing tools.\n\n## Conclusion\n\nYou've built a minimal MCP server that exposes tools, resources, and prompts over stdio, and verified it with a test client. This pattern lets you define capabilities once and reuse them across any MCP-compatible applicationâ€”from CLIs to agents to chatbots.\n\n### Next Steps\n\n- **Integrate with OpenAI function calling**: Convert MCP tools to OpenAI tool schemas and route calls from GPT-4 to your server (covered in a separate guide).\n- **Add dynamic resources**: Serve real-time data (e.g., database queries, API responses) instead of static markdown.\n- **Explore other transports**: Use SSE or WebSocket for remote or browser-based clients.\n- **Package and deploy**: Containerize your server and expose it via a network transport for multi-user access."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build a Model Context Protocol (MCP) Server in Python",
    "description": "Learn how to build an MCP server in Python to standardize and reuse AI tools, resources, and prompts across applications. This hands-on guide walks you through server setup, client testing, and GPT-4 chatbot integration for production-ready systems.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}