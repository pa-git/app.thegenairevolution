{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Master Semantic Search: Build Custom AI Knowledge Base Tools\n\n**Description:** Learn to create AI tools with semantic search capabilities using vector databases. This step-by-step guide empowers you to enhance AI retrieval with real-world applications.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n\nRetrieval-Augmented Generation (RAG) is a powerful technique that enhances AI retrieval by combining language models with external knowledge bases. This approach addresses the limitations of standalone language models, which often struggle with up-to-date information and context-specific queries. By integrating retrieval mechanisms, RAG systems can access and utilize vast external datasets, improving the accuracy and relevance of AI-generated responses. For a deeper understanding of how to tailor these models to specific domains, see our [guide on customizing large language models](/blog/44830763/mastering-domain-specific-llm-customization-techniques-and-tools-unveiled).\n\n## 1. Environment Setup and Installation\n\nA well-configured development environment is crucial for building RAG systems. Begin by installing the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install core libraries required for building RAG systems\n# LangChain: Framework for building LLM applications with retrieval capabilities\n# Transformers: Hugging Face library for pre-trained language models and embeddings\n# ChromaDB: Vector database for efficient similarity search and storage\n!pip install langchain transformers chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [LangChain Documentation](https://python.langchain.com/docs/introduction/)\n- [Hugging Face Documentation](https://huggingface.co/docs)\n- [ChromaDB Documentation](https://docs.trychroma.com/)\n\nThese tools provide the foundational components for building and deploying RAG systems, ensuring scalability and efficiency. For best practices on fine-tuning models with these tools, explore our [fine-tuning guide with Hugging Face Transformers](/blog/44830763/mastering-fine-tuning-of-large-language-models-with-hugging-face).\n\n## 2. Data Preparation and Knowledge Base Construction\n\nPrepare your data by loading and preprocessing documents to build a high-quality knowledge base for RAG:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n\ndef prepare_knowledge_base(file_path):\n    \"\"\"\n    Load and preprocess documents from a CSV file for RAG system.\n    \n    Args:\n        file_path (str): Path to the CSV file containing documents\n        \n    Returns:\n        pd.DataFrame: Preprocessed dataframe with cleaned text\n        \n    Raises:\n        FileNotFoundError: If the CSV file doesn't exist\n        ValueError: If required columns are missing\n    \"\"\"\n    try:\n        data = pd.read_csv(file_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Document file not found at {file_path}\")\n    \n    if 'text' not in data.columns:\n        raise ValueError(\"CSV must contain a 'text' column with document content\")\n    \n    data = data.dropna(subset=['text'])\n    data['text'] = data['text'].apply(lambda x: x.lower().strip())\n    data = data.drop_duplicates(subset=['text'], keep='first')\n    data = data[data['text'].str.len() >= 10]\n    \n    print(f\"Loaded {len(data)} documents after preprocessing\")\n    \n    return data\n\n# Example usage\ndata = prepare_knowledge_base('documents.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Embedding and Vectorization Techniques\n\nConvert text documents into dense vector embeddings for semantic search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\nimport numpy as np\n\ndef generate_embeddings(texts, model_name='all-MiniLM-L6-v2', batch_size=32):\n    \"\"\"\n    Generate embeddings for a list of text documents using sentence transformers.\n    \n    Args:\n        texts (list): List of text strings to embed\n        model_name (str): Name of the sentence transformer model to use\n        batch_size (int): Number of texts to process in each batch for memory efficiency\n        \n    Returns:\n        np.ndarray: Array of embeddings with shape (num_texts, embedding_dim)\n        \n    Raises:\n        ValueError: If texts list is empty\n    \"\"\"\n    if not texts or len(texts) == 0:\n        raise ValueError(\"Text list cannot be empty\")\n    \n    model = SentenceTransformer(model_name)\n    embeddings = model.encode(\n        texts,\n        batch_size=batch_size,\n        show_progress_bar=True,\n        convert_to_numpy=True\n    )\n    \n    print(f\"Generated {len(embeddings)} embeddings with dimension {embeddings.shape[1]}\")\n    \n    return embeddings\n\n# Example usage: Generate embeddings from preprocessed data\nembeddings = generate_embeddings(data['text'].tolist())\n\n# Verify embedding quality by checking shape and data type\nassert embeddings.shape[0] == len(data), \"Mismatch between number of texts and embeddings\"\nassert embeddings.dtype == np.float32, \"Embeddings should be float32 for efficiency\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Integrating Vector Databases for Retrieval\n\nStore embeddings in ChromaDB vector database for efficient similarity search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import chromadb\nfrom chromadb.config import Settings\n\ndef setup_vector_database(embeddings, texts, metadata=None, collection_name=\"rag_documents\"):\n    \"\"\"\n    Initialize ChromaDB and store document embeddings with metadata.\n    \n    Args:\n        embeddings (np.ndarray): Document embeddings to store\n        texts (list): Original text documents corresponding to embeddings\n        metadata (list): Optional list of metadata dicts for each document\n        collection_name (str): Name of the ChromaDB collection\n        \n    Returns:\n        chromadb.Collection: ChromaDB collection object for querying\n        \n    Raises:\n        ValueError: If embeddings and texts have different lengths\n    \"\"\"\n    if len(embeddings) != len(texts):\n        raise ValueError(\"Number of embeddings must match number of texts\")\n    \n    client = chromadb.PersistentClient(\n        path=\"./vector_db_path\",\n        settings=Settings(anonymized_telemetry=False)\n    )\n    \n    collection = client.get_or_create_collection(\n        name=collection_name,\n        metadata={\"description\": \"RAG document embeddings for semantic search\"}\n    )\n    \n    ids = [f\"doc_{i}\" for i in range(len(texts))]\n    \n    if metadata is None:\n        metadata = [{\"source\": \"documents.csv\", \"index\": i} for i in range(len(texts))]\n    \n    collection.add(\n        embeddings=embeddings.tolist(),\n        documents=texts,\n        metadatas=metadata,\n        ids=ids\n    )\n    \n    print(f\"Stored {len(embeddings)} embeddings in collection '{collection_name}'\")\n    \n    return collection\n\ndef query_vector_database(collection, query_text, model, top_k=5):\n    \"\"\"\n    Query the vector database to retrieve most similar documents.\n    \n    Args:\n        collection (chromadb.Collection): ChromaDB collection to query\n        query_text (str): Text query to search for\n        model (SentenceTransformer): Model to encode the query\n        top_k (int): Number of top results to return\n        \n    Returns:\n        dict: Query results containing documents, distances, and metadata\n    \"\"\"\n    query_embedding = model.encode([query_text])[0]\n    \n    results = collection.query(\n        query_embeddings=[query_embedding.tolist()],\n        n_results=top_k\n    )\n    \n    return results\n\n# Example usage: Setup vector database and perform a query\ncollection = setup_vector_database(embeddings, data['text'].tolist())\n\n# Test retrieval with a sample query\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nresults = query_vector_database(collection, \"What is machine learning?\", model, top_k=3)\nprint(f\"Retrieved {len(results['documents'][0])} relevant documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Building the RAG Pipeline\n\nBuild an end-to-end RAG pipeline integrating retrieval and generation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nclass RAGPipeline:\n    \"\"\"\n    Retrieval-Augmented Generation pipeline combining vector search with LLM generation.\n    \n    This pipeline retrieves relevant documents from a vector database and uses them\n    as context for a language model to generate informed, accurate responses.\n    \"\"\"\n    \n    def __init__(self, collection, embedding_model, llm_model_name=\"gpt2\", top_k=3):\n        \"\"\"\n        Initialize the RAG pipeline with retriever and generator components.\n        \n        Args:\n            collection (chromadb.Collection): Vector database collection for retrieval\n            embedding_model (SentenceTransformer): Model for encoding queries\n            llm_model_name (str): Name of the Hugging Face LLM to use for generation\n            top_k (int): Number of documents to retrieve for context\n        \"\"\"\n        self.collection = collection\n        self.embedding_model = embedding_model\n        self.top_k = top_k\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n        self.llm = AutoModelForCausalLM.from_pretrained(llm_model_name)\n        \n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        print(f\"RAG Pipeline initialized with {llm_model_name}\")\n    \n    def retrieve(self, query):\n        query_embedding = self.embedding_model.encode([query])[0]\n        \n        results = self.collection.query(\n            query_embeddings=[query_embedding.tolist()],\n            n_results=self.top_k\n        )\n        \n        documents = results['documents'][0] if results['documents'] else []\n        \n        return documents\n    \n    def generate(self, query, context_docs):\n        context = \"\\n\\n\".join([f\"Document {i+1}: {doc}\" for i, doc in enumerate(context_docs)])\n        prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n        \n        inputs = self.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=512\n        )\n        \n        with torch.no_grad():\n            outputs = self.llm.generate(\n                inputs['input_ids'],\n                max_new_tokens=150,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=self.tokenizer.pad_token_id\n            )\n        \n        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        answer = response[len(prompt):].strip()\n        \n        return answer\n    \n    def query(self, query_text):\n        retrieved_docs = self.retrieve(query_text)\n        \n        if not retrieved_docs:\n            return {\n                \"answer\": \"No relevant documents found to answer the query.\",\n                \"sources\": []\n            }\n        \n        answer = self.generate(query_text, retrieved_docs)\n        \n        return {\n            \"answer\": answer,\n            \"sources\": retrieved_docs\n        }\n\n# Example usage: Initialize and test the RAG pipeline\npipeline = RAGPipeline(\n    collection=collection,\n    embedding_model=model,\n    llm_model_name=\"gpt2\",\n    top_k=3\n)\n\n# Test the pipeline with a sample query\nresponse = pipeline.query(\"What is RAG?\")\nprint(f\"Answer: {response['answer']}\")\nprint(f\"Retrieved {len(response['sources'])} source documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Advanced Techniques: Agentic Systems and Fine-Tuning\n\nEnhancing RAG systems with advanced techniques like agentic systems and model fine-tuning can significantly improve performance. Agents enable multi-step reasoning and query refinement, while fine-tuning adapts models to specific domains. For a comprehensive walkthrough on fine-tuning, refer to our [fine-tuning guide with Hugging Face Transformers](/blog/44830763/mastering-fine-tuning-of-large-language-models-with-hugging-face)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\nfrom transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import Dataset\n\nclass QueryRefinementAgent:\n    \"\"\"\n    Agent for multi-step query refinement to improve retrieval quality.\n    \n    This agent analyzes queries, expands them with synonyms, and reformulates\n    them for better semantic matching with documents.\n    \"\"\"\n    \n    def __init__(self, embedding_model):\n        \"\"\"\n        Initialize the query refinement agent.\n        \n        Args:\n            embedding_model (SentenceTransformer): Model for semantic similarity\n        \"\"\"\n        self.embedding_model = embedding_model\n    \n    def refine_query(self, query):\n        refined = re.sub(r'\\s+', ' ', query.strip())\n        refined = re.sub(r'[^\\w\\s?]', '', refined)\n        \n        expansion_map = {\n            'RAG': 'RAG Retrieval-Augmented Generation',\n            'AI': 'AI artificial intelligence machine learning',\n            'ML': 'ML machine learning',\n        }\n        \n        for term, expansion in expansion_map.items():\n            if term.lower() in refined.lower():\n                refined = f\"{refined} {expansion}\"\n        \n        if '?' not in refined and not any(word in refined.lower() for word in ['what', 'how', 'why', 'when', 'where']):\n            refined = f\"What is {refined}?\"\n        \n        return refined\n    \n    def multi_query_retrieval(self, original_query, collection, top_k=5):\n        queries = [\n            original_query,\n            self.refine_query(original_query),\n            f\"Explain {original_query}\",\n            f\"Definition of {original_query}\"\n        ]\n        \n        all_results = []\n        seen_docs = set()\n        \n        for query in queries:\n            query_embedding = self.embedding_model.encode([query])[0]\n            results = collection.query(\n                query_embeddings=[query_embedding.tolist()],\n                n_results=top_k\n            )\n            \n            for doc in results['documents'][0]:\n                if doc not in seen_docs:\n                    all_results.append(doc)\n                    seen_docs.add(doc)\n        \n        return all_results[:top_k]\n\ndef fine_tune_embedding_model(texts, labels, model_name='all-MiniLM-L6-v2', output_dir='./fine_tuned_model'):\n    from sentence_transformers import SentenceTransformer, InputExample, losses\n    from torch.utils.data import DataLoader\n    \n    model = SentenceTransformer(model_name)\n    \n    train_examples = [\n        InputExample(texts=[text, label]) \n        for text, label in zip(texts, labels)\n    ]\n    \n    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n    \n    train_loss = losses.CosineSimilarityLoss(model)\n    \n    model.fit(\n        train_objectives=[(train_dataloader, train_loss)],\n        epochs=3,\n        warmup_steps=100,\n        output_path=output_dir,\n        show_progress_bar=True\n    )\n    \n    print(f\"Model fine-tuned and saved to {output_dir}\")\n    \n    return model\n\n# Example usage: Query refinement agent\nagent = QueryRefinementAgent(embedding_model=model)\nrefined_query = agent.refine_query(\"RAG\")\nprint(f\"Original: 'RAG' -> Refined: '{refined_query}'\")\n\n# Example usage: Multi-query retrieval\nresults = agent.multi_query_retrieval(\"What is RAG?\", collection, top_k=5)\nprint(f\"Retrieved {len(results)} documents using multi-query approach\")\n\n# Example usage: Fine-tuning (requires paired training data)\nsample_texts = data['text'].tolist()[:100]\nsample_labels = data['text'].tolist()[:100]\nfine_tuned_model = fine_tune_embedding_model(sample_texts, sample_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Testing, Validation, and Performance Evaluation\n\nEvaluate RAG system performance using multiple metrics and test cases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nclass RAGEvaluator:\n    \"\"\"\n    Comprehensive evaluation framework for RAG systems.\n    \n    Measures retrieval quality, generation accuracy, and system performance\n    using industry-standard metrics.\n    \"\"\"\n    \n    def __init__(self, pipeline):\n        \"\"\"\n        Initialize evaluator with a RAG pipeline.\n        \n        Args:\n            pipeline (RAGPipeline): RAG pipeline to evaluate\n        \"\"\"\n        self.pipeline = pipeline\n    \n    def evaluate_retrieval_quality(self, test_queries, ground_truth_docs):\n        precisions = []\n        recalls = []\n        reciprocal_ranks = []\n        \n        for query, relevant_docs in zip(test_queries, ground_truth_docs):\n            retrieved = self.pipeline.retrieve(query)\n            \n            relevant_retrieved = sum(1 for doc in retrieved if doc in relevant_docs)\n            precision = relevant_retrieved / len(retrieved) if retrieved else 0\n            precisions.append(precision)\n            \n            recall = relevant_retrieved / len(relevant_docs) if relevant_docs else 0\n            recalls.append(recall)\n            \n            rank = next((i + 1 for i, doc in enumerate(retrieved) if doc in relevant_docs), 0)\n            reciprocal_ranks.append(1 / rank if rank > 0 else 0)\n        \n        return {\n            'precision': np.mean(precisions),\n            'recall': np.mean(recalls),\n            'mrr': np.mean(reciprocal_ranks),\n            'f1_score': 2 * np.mean(precisions) * np.mean(recalls) / (np.mean(precisions) + np.mean(recalls)) if (np.mean(precisions) + np.mean(recalls)) > 0 else 0\n        }\n    \n    def evaluate_generation_quality(self, test_queries, reference_answers):\n        similarities = []\n        \n        for query, reference in zip(test_queries, reference_answers):\n            response = self.pipeline.query(query)\n            generated = response['answer']\n            \n            gen_embedding = self.pipeline.embedding_model.encode([generated])\n            ref_embedding = self.pipeline.embedding_model.encode([reference])\n            \n            similarity = cosine_similarity(gen_embedding, ref_embedding)[0][0]\n            similarities.append(similarity)\n        \n        return {\n            'avg_semantic_similarity': np.mean(similarities),\n            'min_similarity': np.min(similarities),\n            'max_similarity': np.max(similarities)\n        }\n    \n    def evaluate_latency(self, test_queries, num_runs=10):\n        latencies = []\n        \n        for query in test_queries:\n            query_latencies = []\n            \n            for _ in range(num_runs):\n                start_time = time.time()\n                self.pipeline.query(query)\n                end_time = time.time()\n                \n                latency_ms = (end_time - start_time) * 1000\n                query_latencies.append(latency_ms)\n            \n            latencies.append(np.median(query_latencies))\n        \n        return {\n            'avg_latency_ms': np.mean(latencies),\n            'p50_latency_ms': np.percentile(latencies, 50),\n            'p95_latency_ms': np.percentile(latencies, 95),\n            'p99_latency_ms': np.percentile(latencies, 99)\n        }\n    \n    def run_full_evaluation(self, test_data):\n        print(\"Starting RAG system evaluation...\")\n        \n        print(\"Evaluating retrieval quality...\")\n        retrieval_metrics = self.evaluate_retrieval_quality(\n            test_data['queries'],\n            test_data['ground_truth_docs']\n        )\n        \n        print(\"Evaluating generation quality...\")\n        generation_metrics = self.evaluate_generation_quality(\n            test_data['queries'],\n            test_data['reference_answers']\n        )\n        \n        print(\"Evaluating system latency...\")\n        latency_metrics = self.evaluate_latency(test_data['queries'])\n        \n        results = {\n            'retrieval': retrieval_metrics,\n            'generation': generation_metrics,\n            'latency': latency_metrics\n        }\n        \n        return results\n\ndef print_evaluation_results(results):\n    print(\"\\n\" + \"=\"*50)\n    print(\"RAG SYSTEM EVALUATION RESULTS\")\n    print(\"=\"*50)\n    \n    print(\"\\nRetrieval Metrics:\")\n    print(f\"  Precision: {results['retrieval']['precision']:.3f} (higher is better, max 1.0)\")\n    print(f\"  Recall: {results['retrieval']['recall']:.3f} (higher is better, max 1.0)\")\n    print(f\"  F1 Score: {results['retrieval']['f1_score']:.3f} (harmonic mean of precision and recall)\")\n    print(f\"  MRR: {results['retrieval']['mrr']:.3f} (Mean Reciprocal Rank, higher is better)\")\n    \n    print(\"\\nGeneration Metrics:\")\n    print(f\"  Avg Semantic Similarity: {results['generation']['avg_semantic_similarity']:.3f}\")\n    print(f\"  Min Similarity: {results['generation']['min_similarity']:.3f}\")\n    print(f\"  Max Similarity: {results['generation']['max_similarity']:.3f}\")\n    \n    print(\"\\nLatency Metrics:\")\n    print(f\"  Average Latency: {results['latency']['avg_latency_ms']:.2f} ms\")\n    print(f\"  P50 Latency: {results['latency']['p50_latency_ms']:.2f} ms\")\n    print(f\"  P95 Latency: {results['latency']['p95_latency_ms']:.2f} ms\")\n    print(f\"  P99 Latency: {results['latency']['p99_latency_ms']:.2f} ms\")\n    print(\"=\"*50 + \"\\n\")\n\n# Example usage: Prepare test data\ntest_data = {\n    'queries': [\n        \"What is RAG?\",\n        \"How does semantic search work?\",\n        \"What are vector databases?\"\n    ],\n    'ground_truth_docs': [\n        [data['text'].tolist()[0], data['text'].tolist()[1]],\n        [data['text'].tolist()[2]],\n        [data['text'].tolist()[3], data['text'].tolist()[4]]\n    ],\n    'reference_answers': [\n        \"RAG is Retrieval-Augmented Generation, combining retrieval with language models.\",\n        \"Semantic search uses embeddings to find semantically similar content.\",\n        \"Vector databases store and retrieve high-dimensional embeddings efficiently.\"\n    ]\n}\n\n# Run evaluation\nevaluator = RAGEvaluator(pipeline)\nresults = evaluator.run_full_evaluation(test_data)\n\n# Display results with interpretation\nprint_evaluation_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This comprehensive guide walks you through setting up a RAG system, integrating advanced techniques, and evaluating performance, ensuring you can build scalable, production-ready AI applications."
      ]
    }
  ],
  "metadata": {
    "title": "Master Semantic Search: Build Custom AI Knowledge Base Tools",
    "description": "Learn to create AI tools with semantic search capabilities using vector databases. This step-by-step guide empowers you to enhance AI retrieval with real-world applications.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}