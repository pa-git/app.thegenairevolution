{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Multi-Document Agent with LlamaIndex: The Ultimate Guide [2025]\n\n**Description:** Build a production-ready multi-document agent with LlamaIndex, turning PDFs into retrieval and summarization tools using semantic selection for accurate answers.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What You're Building\n\nYou'll build a multi\\-document research assistant that answers questions across multiple PDFs with precise citations. The agent uses semantic vector search for targeted queries, hierarchical summarization for high\\-level synthesis, and function calling to route queries to the right tool. By the end, you'll have a runnable notebook that handles cross\\-document Q\\&A, enforces consistent citations in `[file_name p.page_label]` format, and includes a minimal validation suite.\n\n**Prerequisites:**\n\n* Python 3\\.10\\+\n\n* OpenAI API key\n\n* 2â€“3 sample PDFs (research papers, reports, or technical documents)\n\n* Expected cost: \\~$0\\.10â€“$0\\.50 per summary\\-heavy query depending on document size\n\n## Why This Approach Works\n\n**Per\\-Document Tool Isolation**\nEach PDF gets its own vector and summary tool. This prevents cross\\-contamination, enables precise citations, and lets the agent reason about which document to query for a given question.\n\n**Semantic Tool Retrieval**\nAn object index embeds tool descriptions and retrieves the top\\-k relevant tools per query. This scales to dozens of documents without overwhelming the agent's context window.\n\n**Dual Retrieval Strategy**\nVector tools handle narrow, fact\\-based queries (\"What dataset did the authors use?\"). Summary tools handle broad synthesis (\"Compare the main contributions across papers\"). The agent picks the right mode based on query semantics.\n\n**Citation Enforcement**\nEvery tool attaches file name and page metadata to results. The system prompt instructs the agent to cite sources after each claim, and you can post\\-process responses to format citations programmatically.\n\n## How It Works (High\\-Level Overview)\n\n1. **Load and chunk PDFs** â€“ Extract text, split into sentence\\-aware chunks, normalize metadata for citations.\n\n2. **Build per\\-document tools** â€“ Create vector and summary tools for each PDF; wrap them with clear descriptions.\n\n3. **Index tools semantically** â€“ Embed tool descriptions in an object index for dynamic retrieval.\n\n4. **Assemble the agent** â€“ Use function calling with a strict system prompt to route queries and enforce citations.\n\n5. **Validate and iterate** â€“ Run test queries, inspect tool selection, tune retrieval thresholds and temperature.\n\n## Setup \\& Installation\n\nRun this cell first to install all required packages with pinned versions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade -q \"llama-index==0.10.40\" \"llama-index-llms-openai>=0.1.0\" \"llama-index-embeddings-openai>=0.1.0\" \"pypdf>=4.0.0\" nest_asyncio python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, configure your OpenAI API key. If running in Colab, add your key to Secrets (Settings â†’ Secrets â†’ OPENAI\\_API\\_KEY). Otherwise, create a `.env` file with `OPENAI_API_KEY=your_key`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Fail early if key is missing\nassert os.getenv(\"OPENAI_API_KEY\"), \"Set OPENAI_API_KEY in .env or Colab Secrets\"\nprint(\"API key loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up logging, suppress warnings, and enable async support for a clean notebook environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\nimport warnings\nimport nest_asyncio\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.INFO)\nnest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure the LLM and embedding model globally for all LlamaIndex operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# Use GPT-4o for reliable function calling; fallback to gpt-4o-mini if needed\nSettings.llm = OpenAI(model=\"gpt-4o\", temperature=0.1)\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a data directory and download sample PDFs programmatically so the notebook runs end\\-to\\-end:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib.request\n\nDATA_DIR = \"data\"\nos.makedirs(DATA_DIR, exist_ok=True)\n\n# Example: download public arXiv papers (replace with your own PDFs)\nsample_urls = [\n    (\"https://arxiv.org/pdf/2005.11401.pdf\", \"paper1.pdf\"),  # GPT-3 paper\n    (\"https://arxiv.org/pdf/2303.08774.pdf\", \"paper2.pdf\"),  # GPT-4 paper\n]\n\nfor url, fname in sample_urls:\n    fpath = os.path.join(DATA_DIR, fname)\n    if not os.path.exists(fpath):\n        print(f\"Downloading {fname}...\")\n        urllib.request.urlretrieve(url, fpath)\n\npdf_files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(\".pdf\")]\nprint(f\"Found {len(pdf_files)} PDFs:\", pdf_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step\\-by\\-Step Implementation\n\n### Step 1: Load and Chunk PDFs\n\nLoad documents from the data directory. The PDF reader attaches page metadata automatically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader, Document\n\ndocs = SimpleDirectoryReader(DATA_DIR, recursive=False).load_data()\nprint(f\"Loaded {len(docs)} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split documents into sentence\\-aware chunks for semantic retrieval. Sentence\\-aware splitting avoids fragmenting thoughts mid\\-sentence, giving the vector index better semantic units. This directly improves retrieval quality, especially for dense technical writing like research papers or legal clauses. For more strategies to boost retrieval accuracy in RAG systems, see our guide on [retrieval tricks to boost answer accuracy](/article/rag-application-7-retrieval-tricks-to-boost-answer-accuracy-2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n\nsplitter = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\nnodes = splitter.get_nodes_from_documents(docs, show_progress=True)\nprint(f\"Total chunks: {len(nodes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalize metadata for accurate citations. Ensure every node has `file_name` and `page_label`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for n in nodes:\n    meta = n.metadata or {}\n    if \"file_name\" not in meta:\n        file_path = meta.get(\"file_path\", meta.get(\"source\", \"unknown\"))\n        meta[\"file_name\"] = os.path.basename(file_path) if isinstance(file_path, str) else \"unknown\"\n    if \"page_label\" not in meta:\n        meta[\"page_label\"] = str(meta.get(\"page_number\", \"N/A\"))\n    n.metadata = meta\n\nprint(\"Sample chunk metadata:\", nodes[0].metadata)\nprint(\"Sample chunk text:\", nodes[0].text[:300], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Group nodes by document for per\\-document tool creation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n\nnodes_by_file = defaultdict(list)\nfor n in nodes:\n    nodes_by_file[n.metadata[\"file_name\"]].append(n)\n\nprint({k: len(v) for k, v in nodes_by_file.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Build Per\\-Document Vector Tools\n\nCreate a vector index for each document to enable precise passage retrieval:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\nfrom llama_index.core.tools import QueryEngineTool\n\nvector_tools = {}\n\nfor fname, doc_nodes in nodes_by_file.items():\n    v_index = VectorStoreIndex(doc_nodes, show_progress=True)\n    v_engine = v_index.as_query_engine(similarity_top_k=5)\n    v_tool = QueryEngineTool.from_defaults(\n        name=f\"vector_{fname}\",\n        query_engine=v_engine,\n        description=(\n            f\"Semantic vector search for {fname}. \"\n            \"Use for targeted, specific questions that require exact passages and citations.\"\n        ),\n        metadata={\"file_name\": fname, \"tool_type\": \"vector\"}\n    )\n    vector_tools[fname] = v_tool\n\nprint(f\"Vector tools created: {len(vector_tools)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test a vector tool to verify retrieval quality:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_file = next(iter(vector_tools.keys()))\nresp = vector_tools[sample_file].query_engine.query(\"What problem does this paper address?\")\nprint(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Build Per\\-Document Summary Tools\n\nCreate a summary index for each document to enable hierarchical summarization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import SummaryIndex\n\nsummary_tools = {}\n\nfor fname, doc_nodes in nodes_by_file.items():\n    s_index = SummaryIndex(doc_nodes)\n    s_engine = s_index.as_query_engine(\n        response_mode=\"tree_summarize\",\n        use_async=True\n    )\n    s_tool = QueryEngineTool.from_defaults(\n        name=f\"summary_{fname}\",\n        query_engine=s_engine,\n        description=(\n            f\"Hierarchical summarization for {fname}. \"\n            \"Use for overviews, key contributions, limitations, and document-wide synthesis.\"\n        ),\n        metadata={\"file_name\": fname, \"tool_type\": \"summary\"}\n    )\n    summary_tools[fname] = s_tool\n\nprint(f\"Summary tools created: {len(summary_tools)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test a summary tool to verify synthesis quality:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_file = next(iter(summary_tools.keys()))\nresp = summary_tools[sample_file].query_engine.query(\"Provide a 5-bullet executive summary.\")\nprint(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Index Tools Semantically\n\nBuild an object index over all tools for semantic tool selection. This embeds tool descriptions and retrieves the top\\-k relevant tools per query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.objects import ObjectIndex\n\nall_tools = list(vector_tools.values()) + list(summary_tools.values())\n\nobj_index = ObjectIndex.from_objects(\n    all_tools,\n    index_cls=VectorStoreIndex,\n    show_progress=True\n)\n\ntool_retriever = obj_index.as_retriever(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect which tools are retrieved for different queries to debug tool selection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inspect_tools(query: str):\n    tools = tool_retriever.retrieve(query)\n    print(f\"Query: {query}\")\n    for i, t in enumerate(tools, 1):\n        tool = getattr(t, \"object\", None) or t.node.metadata.get(\"object\")\n        print(f\"#{i} -> {tool.metadata.get('tool_type')} | {tool.metadata.get('file_name')} | {tool.name}\")\n\ninspect_tools(\"Summarize key contributions across the papers.\")\ninspect_tools(\"What dataset did the authors use for evaluation?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Assemble the Agent\n\nCreate the agent using function calling and a strict system prompt that enforces citation format. While frameworks like LangChain and CrewAI are solid, LlamaIndex specializes in document workflows with first\\-class support for indexing, retrieval, summarization, and agentic tool use that map cleanly to this problem. If you're interested in foundational agent patterns, check out our step\\-by\\-step tutorial on [building an LLM agent from scratch with GPT\\-4 ReAct](/article/how-to-build-an-llm-agent-from-scratch-with-gpt-4-react-5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker, AgentRunner\n\nSYSTEM_PROMPT = \"\"\"You are a multi-document research assistant.\n- Use only the provided tools.\n- Prefer vector tools for specific, narrow questions.\n- Prefer summary tools for high-level synthesis.\n- Always cite sources as [file_name p.page_label] after each relevant sentence.\n- If you cannot find relevant evidence, say so explicitly.\"\"\"\n\nagent_worker = FunctionCallingAgentWorker.from_tools(\n    tools=all_tools,\n    llm=Settings.llm,\n    system_prompt=SYSTEM_PROMPT,\n    tool_retriever=tool_retriever,\n)\n\nagent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Validate\n\nRun a cross\\-document query and verify the agent synthesizes answers with citations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = agent.chat(\n    \"Compare the main challenges and proposed collaboration mechanisms across the papers.\"\n)\nprint(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run a suite of test queries to validate agent routing, retrieval, and summarization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tests = [\n    \"List the datasets used by each paper and compare evaluation metrics.\",\n    \"Provide a high-level summary of the main contributions across documents.\",\n    \"According to the authors, what are the primary limitations?\"\n]\n\nfor q in tests:\n    print(\"\\nQ:\", q)\n    resp = agent.chat(q)\n    print(\"A:\", str(resp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect tool selection for observability and tuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_selected_tools(query: str):\n    cands = tool_retriever.retrieve(query)\n    print(f\"Query: {query}\")\n    for i, c in enumerate(cands, 1):\n        tool = getattr(c, \"object\", None) or c.node.metadata.get(\"object\")\n        print(f\"  {i}. {tool.name} | {tool.metadata['tool_type']} | {tool.metadata['file_name']} | score={c.score:.3f}\")\n\nprint_selected_tools(\"Provide an executive summary across all documents.\")\nprint_selected_tools(\"Which sections discuss model architecture details?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add a simple guardrail to report when no sufficiently relevant evidence is found:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MIN_SCORE = 0.25\n\ndef safe_query(query: str) -> str:\n    cands = tool_retriever.retrieve(query)\n    if not cands or max(c.score for c in cands) < MIN_SCORE:\n        return \"No sufficiently relevant sources found. Please rephrase or specify a document/section.\"\n    return str(agent.chat(query))\n\nprint(safe_query(\"What is the capital of Mars?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nYou've built a multi\\-document research assistant that routes queries to the right tool, retrieves precise passages, and enforces consistent citations. Key decisions include per\\-document tool isolation for clean attribution, semantic tool retrieval for scalability, and dual retrieval modes (vector for specifics, summary for synthesis).\n\n**Next steps to harden for production:**\n\n1. **Persist indices** â€“ Save vector and summary indices to disk or a vector database (e.g., pgvector, Pinecone) to avoid re\\-embedding on every run.\n\n2. **Add retries and rate limits** â€“ Wrap LLM calls with exponential backoff and timeout handling for robustness.\n\n3. **Implement structured logging** â€“ Use LlamaIndex callbacks or a logging framework to trace tool calls, latency, and token usage.\n\n4. **Cache answers** â€“ Use an in\\-memory LRU cache or a persistent store like Redis for repeated queries. For a deep dive into implementing semantic caching with Redis Vector to optimize LLM costs, see [how to implement semantic cache with Redis Vector](/article/semantic-cache-llm-how-to-implement-with-redis-vector-to-cut-costs-6).\n\n5. **Post\\-process citations** â€“ Extract `source_nodes` from responses and format citations programmatically to ensure consistency beyond prompt\\-based enforcement."
      ]
    }
  ],
  "metadata": {
    "title": "Multi-Document Agent with LlamaIndex: The Ultimate Guide [2025]",
    "description": "Build a production-ready multi-document agent with LlamaIndex, turning PDFs into retrieval and summarization tools using semantic selection for accurate answers.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}