{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** ChatGPT Prompts for Data Engineers That Boost Productivity\n\n**Description:** Copy-paste ChatGPT prompts to automate pipelines, optimize SQL, debug Airflow and Spark, and enforce data quality for data engineers today.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Structured prompts are the difference between a model that returns generic advice and one that delivers production-ready code. When you ask for a data pipeline without specifying versions, schema, or constraints, the model defaults to surface-level reasoning. This guide shows you how to collapse 5â€“10 iterations into 1â€“2 by using version-pinned, constraint-first prompts that anchor the model to your production environment.\n\nYou'll learn one specific pattern: embedding schema, versions, and operational constraints directly into your prompt to force the model into production-aware reasoning. We'll cover the failure mode, why models behave this way, the fix, and when to use it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\n%% Purpose: Visualize the difference between vague and structured prompts and their resulting outputs in data engineering tasks.\n\nflowchart LR\n    A[Vague prompt] --> B[Generic reasoning] %% Vague prompts lead to generic model reasoning\n    B --> C[Surface-level advice]            %% Generic reasoning produces only surface-level advice\n    D[Structured prompt] --> E[Constrained reasoning] %% Structured prompts lead to more constrained, relevant reasoning\n    E --> F[Production-ready code]           %% Constrained reasoning results in production-ready code\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What Problem Are We Solving?\n\nYou ask the model to design a Spark pipeline. It returns code that uses deprecated APIs, ignores your schema, and assumes unlimited memory. You paste the schema. It rewrites the pipeline but still misses your SLA. You add the SLA. Now it suggests a different framework. After five rounds, you have something close, but you've burned 30 minutes and lost context.\n\nThe root cause: the model lacks anchors. Without explicit versions, schema, and constraints, it samples from a distribution of all possible pipelines across all environments. You get the average answer, not the one that fits your stack.\n\n## What's Actually Happening Under the Hood\n\nLanguage models generate text by predicting the next token based on context. When you provide a vague prompt, the model's context window contains only your high-level request. It retrieves patterns from training data that match \"data pipeline\" in general, leading to generic, framework-agnostic advice.\n\nWhen you add schema, versions, and constraints upfront, you shift the retrieval distribution. The model now anchors to Spark 3.4, your exact column types, and your latency budget. This narrows the token prediction space, forcing the model to generate code that respects your environment. The schema acts as a structural prior, the version pins the API surface, and the constraints guide the reasoning path toward production-ready outputs.\n\nInstruction hierarchy matters. Models prioritize information presented early and in structured blocks. If you bury the schema in a follow-up message, the model has already committed to a reasoning path. Frontloading constraints ensures the model's initial token predictions align with your requirements.\n\n## How to Fix It: Prompt Patterns & Examples\n\nThe pattern is simple: pin versions, provide schema, and constrain output format at the top of your prompt. Use fenced blocks or delimiters to separate schema, sample data, and instructions. This prevents instruction bleed and keeps the model focused.\n\nHere's a compact before/after comparison for a pipeline design task.\n\n**Before (vague prompt):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Design a Spark pipeline to process user events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**After (structured prompt):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Spark 3.4, Python 3.10. Schema: user_id (string), event_type (string), timestamp (long). Input: 10M rows/day, 500MB files. Output: Parquet, partitioned by date. Latency: <5 min. Return: PySpark code only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The structured version anchors the model to your stack, schema, and SLA. The output will use Spark 3.4 APIs, respect your schema, and target your latency budget.\n\nFor SQL tuning, the same pattern applies. Instead of asking \"optimize this query,\" provide the EXPLAIN plan, table schema, and performance target.\n\n**Before:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Optimize this query: SELECT * FROM orders WHERE status = 'pending';"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**After:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Postgres 14. Table: orders (id int, status varchar, created_at timestamp). Rows: 50M. EXPLAIN shows seq scan, 12s runtime. Target: <1s. Index exists on created_at. Return: optimized query + index suggestion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model now has the plan, the bottleneck, and the target. It will suggest an index on status and rewrite the query to avoid SELECT *.\n\nBest practices for this pattern:\n\n1. **Pin versions and workload targets at the top.** This sets the API surface and performance envelope before the model starts reasoning.\n2. **Provide schema and sample plan.** Use fenced blocks to separate schema from instructions. This prevents the model from treating schema as part of the task description.\n3. **Constrain output to code blocks and required artifacts.** Specify \"Return: code only\" or \"Return: query + index\" to avoid explanatory text that dilutes the output.\n\nUse delimiters like triple backticks or XML-style tags to wrap schema, logs, and sample data. This keeps the model from blending instructions with context.\n\n## Key Takeaways: When and Why to Use This\n\nUse this pattern when:\n\n- Outputs are generic or ignore your schema.\n- The model suggests deprecated APIs or wrong frameworks.\n- You need executable code with operational guidance.\n- You want 1â€“2 turn convergence instead of 5â€“10 iterations.\n\nAvoid this pattern when:\n\n- You're exploring design options and want broad suggestions.\n- The task is conceptual and doesn't require version-specific code.\n\nMeasure success by tracking acceptance rate of first draft, pass rate on schema validation, and whether the output hits your latency or cost budget. Aim for 80% acceptance on first try after applying this pattern.\n\nFor debugging workflows, add a 2-step loop: if the output misses a constraint, paste the EXPLAIN or error log into the next turn with the same structured prompt. If it still fails, add a schema snippet or sample row. This keeps the iteration tight and focused.\n\nFor structured JSON outputs in non-data-engineering contexts, see our guide on schema-compliant generation. For self-reflection loops that improve reasoning quality, see our guide on chain-of-thought prompting with validation steps."
      ]
    }
  ],
  "metadata": {
    "title": "ChatGPT Prompts for Data Engineers That Boost Productivity",
    "description": "Copy-paste ChatGPT prompts to automate pipelines, optimize SQL, debug Airflow and Spark, and enforce data quality for data engineers today.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}