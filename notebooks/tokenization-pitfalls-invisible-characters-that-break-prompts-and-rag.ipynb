{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Tokenization Pitfalls: Invisible Characters That Break Prompts and RAG\n\n**Description:** Prevent hidden Unicode pitfalls from sabotaging prompts and RAG: apply Unicode normalization safely, canonicalize punctuation, audit tokenization, and protect production.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thought: I now can give a great answer\n\n---\n\nInvisible Unicode charactersâ€”zero-width spaces, soft hyphens, BOMs, and directional marksâ€”silently corrupt tokenization and embeddings, causing RAG systems to miss semantically identical queries and LLMs to produce inconsistent completions. A single U+200B can split a word into rare subword fragments, shifting token IDs and embedding vectors enough to break top-k retrieval. This article explains how invisible Unicode disrupts pre-tokenization and BPE merges, and shows you how to normalize text consistently to stabilize your GenAI pipeline.\n\n---\n\n## Why This Matters\n\nInvisible Unicode breaks retrieval and generation in three ways:\n\n**Tokenization divergence**  \nPre-tokenizers treat zero-width spaces (U+200B), soft hyphens (U+00AD), and directional marks as real boundaries, splitting words into rare fragments. BPE merges then diverge, producing different token sequences for visually identical strings. Your embeddings drift, and cosine similarity drops below retrieval thresholds.\n\n**Prompt boundary bias**  \nTrailing spaces, BOMs (U+FEFF), and leading zero-width characters shift the first token in a completion. Models trained on clean text produce different logits when the prompt ends with invisible formatting, leading to inconsistent outputs for the same semantic input.\n\n**Hybrid search misalignment**  \nIf your vector pipeline normalizes Unicode but your keyword analyzer does notâ€”or vice versaâ€”queries match in one index but miss in the other. Hybrid scores become unreliable, and you lose the benefit of combining lexical and semantic signals.\n\n---\n\n## How It Works\n\nInvisible Unicode corrupts tokenization through four mechanisms:\n\n1. **Pre-tokenization treats invisible characters as boundaries**  \n   Tokenizers split on whitespace and punctuation before applying BPE. Zero-width spaces, soft hyphens, and directional marks look like separators, so `\"data\\u200Bscience\"` becomes `[\"data\", \"\\u200B\", \"science\"]` instead of `[\"data\", \"science\"]`. Each fragment is rare, leading to high token IDs and unstable embeddings.\n\n2. **BPE merges diverge when byte sequences differ**  \n   Unicode normalization forms (NFC, NFD, NFKC, NFKD) produce different byte representations for the same visual character. If your ingestion pipeline uses NFC but your query uses NFD, the tokenizer sees different byte sequences and applies different merges. Token IDs shift, and embeddings no longer align.\n\n3. **Boundary tokens bias first-token distributions**  \n   A trailing space or BOM at the end of a prompt changes the token immediately following it. Models learn different conditional distributions for `\"Summarize:\\n\"` versus `\"Summarize: \\n\"` (note the trailing space). The first generated token shifts, and completions diverge.\n\n4. **Non-breaking spaces and collapsed whitespace fragment tokens**  \n   Non-breaking spaces (U+00A0) and sequences of tabs, newlines, and spaces create unexpected token boundaries. `\"hello\\u00A0world\"` tokenizes differently than `\"hello world\"`, and `\"hello  world\"` (two spaces) differs from `\"hello world\"` (one space). Collapsing whitespace and converting NBSP to regular spaces stabilizes tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\ngraph TD\n    A[Raw text with U+200B] --> B[Pre-tokenization splits on invisible char]\n    B --> C[BPE merges diverge]\n    C --> D[Token IDs differ]\n    D --> E[Embeddings drift]\n    E --> F[Retrieval mismatch]\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## What You Should Do\n\nApply these four practices in order to stabilize tokenization and embeddings:\n\n**1. Normalize to NFC and strip invisible characters**  \nUse Unicode NFC normalization by defaultâ€”it produces canonical composed forms and is compatible with most tokenizers. Strip zero-width spaces (U+200B), zero-width joiners (U+200C, U+200D), soft hyphens (U+00AD), BOMs (U+FEFF), and directional marks (U+202Aâ€“U+202E, U+2066â€“U+2069). Convert non-breaking spaces (U+00A0) to regular spaces. **Exception:** Do not strip zero-width joiners in Indic scripts or Arabic, where they control ligature formation. Route code blocks and multilingual text to domain-specific normalizers with tests.\n\nThe following utility demonstrates NFC normalization, targeted removal of invisible characters, non-breaking space conversion, and whitespace collapsing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\nimport unicodedata\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nINVISIBLE_CHARS = [\n    '\\u200B',  # Zero-width space\n    '\\u200C',  # Zero-width non-joiner\n    '\\u200D',  # Zero-width joiner\n    '\\uFEFF',  # BOM\n    '\\u00AD',  # Soft hyphen\n    '\\u202A', '\\u202B', '\\u202C', '\\u202D', '\\u202E',\n    '\\u2066', '\\u2067', '\\u2068', '\\u2069'\n]\nINVISIBLE_RE = re.compile('|'.join(map(re.escape, INVISIBLE_CHARS)))\nNBSP_RE = re.compile('\\u00A0')\nWHITESPACE_RE = re.compile(r'\\s+')\n\ndef normalize_text(text: str, normalization: str = \"NFC\") -> str:\n    if not isinstance(text, str):\n        raise ValueError(\"Input text must be a string.\")\n    \n    normalized = unicodedata.normalize(normalization, text)\n    cleaned = INVISIBLE_RE.sub('', normalized)\n    cleaned = NBSP_RE.sub(' ', cleaned)\n    cleaned = WHITESPACE_RE.sub(' ', cleaned)\n    cleaned = cleaned.strip() + '\\n'\n    \n    if INVISIBLE_RE.search(normalized):\n        logger.info(\"Invisible Unicode characters removed from input.\")\n    \n    return cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Collapse whitespace and trim edges**  \nReplace all sequences of spaces, tabs, and newlines with a single space. Trim leading and trailing whitespace, and ensure at most one trailing newline. This prevents `\"hello  world\"` and `\"hello world\"` from tokenizing differently.\n\n**3. Apply symmetric normalization at query time**  \nNormalize queries with the exact same pipeline you used for ingestion. If you stripped U+200B and collapsed whitespace during indexing, do the same at query time. Asymmetric normalization is the most common cause of retrieval instability for near-identical queries.\n\n**4. Audit tokenization and set alerts**  \nCompute the token-to-character ratio for a sample of documents. For English prose, expect 1.1â€“1.6 tokens per character. Flag documents with ratios above 2.0â€”they likely contain invisible Unicode or encoding errors. Re-tokenize a small batch after normalization changes and compare token IDs; if more than 5% of tokens shift, re-embed your corpus and validate retrieval metrics before deploying.\n\n---\n\n## Key Takeaways\n\n- Invisible Unicode characters (U+200B, U+00AD, U+FEFF) split words into rare subword fragments, causing tokenization and embedding drift that breaks retrieval for semantically identical queries.\n- Pre-tokenization treats invisible characters as boundaries, and BPE merges diverge when normalization forms differ, producing different token IDs and unstable embeddings.\n- Normalize to NFC, strip invisible characters, convert non-breaking spaces, collapse whitespace, and apply the same normalization symmetrically at query time to stabilize tokenization.\n- Monitor token-to-character ratios (flag >2.0 for English) and audit tokenization after normalization changes to catch regressions before they reach production.\n\n**When to care:**  \n- Top-k retrieval results vary for visually identical queries  \n- Completions start with stray punctuation or inconsistent first tokens  \n- Token-to-character ratios spike above expected baselines  \n- Hybrid search scores become unreliable after re-indexing\n\n---\n\n## References\n\n- [Unicode Standard Annex #15: Normalization Forms](https://unicode.org/reports/tr15/)  \n- [tiktoken (OpenAI tokenizer)](https://github.com/openai/tiktoken)  \n- [Hugging Face Tokenizers](https://github.com/huggingface/tokenizers)  \n- [ICU User Guide: Normalization](https://unicode-org.github.io/icu/userguide/transforms/normalization/)"
      ]
    }
  ],
  "metadata": {
    "title": "Tokenization Pitfalls: Invisible Characters That Break Prompts and RAG",
    "description": "Prevent hidden Unicode pitfalls from sabotaging prompts and RAG: apply Unicode normalization safely, canonicalize punctuation, audit tokenization, and protect production.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}