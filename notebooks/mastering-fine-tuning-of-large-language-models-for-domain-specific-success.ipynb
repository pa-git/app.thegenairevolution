{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Mastering Fine-Tuning of Large Language Models for Domain-Specific Success\n\n**Description:** Unlock the potential of large language models with our step-by-step guide to fine-tuning for specialized applications, enhancing performance and achieving production-ready solutions.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning Large Language Models: A Step-by-Step Guide\n\n## Introduction\n\nFine-tuning large language models (LLMs) is a transformative approach to adapting pre-trained models for specific domains. Unlike prompt engineering, which can be limited in handling complex tasks, fine-tuning allows for deeper integration of domain-specific knowledge, resulting in enhanced model performance and utility. This guide will walk you through the process of fine-tuning LLMs, providing you with the tools and techniques needed to achieve domain-specific success. For more insights, refer to [Privatai's tutorial on LLM fine-tuning](https://www.privatai.co.uk/tutorials/llm-fine-tuning?utm_source=openai) and our article on [customizing LLMs for domain-specific applications](/blog/44830763/mastering-domain-specific-llm-customization-techniques-and-tools-unveiled).\n\n## Installation\n\nTo begin, let's install the necessary libraries. These are essential for loading pre-trained models, handling datasets, and performing fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install core libraries for LLM fine-tuning\n# transformers: Hugging Face library for working with pre-trained models\n# datasets: Library for loading and processing training datasets\n# accelerate: Enables distributed training and mixed precision (recommended)\n# peft: Parameter-Efficient Fine-Tuning methods like LoRA (recommended)\n!pip install transformers datasets accelerate peft\n\n# Optional: Install specific versions for reproducibility\n# !pip install transformers==4.35.0 datasets==2.14.0 accelerate==0.24.0 peft==0.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project Setup\n\nDefine environment variables and configuration files necessary for the project. This includes setting up paths for data storage and model outputs.\n\n## Step-by-Step Build\n\n### Dataset Preparation\n\nLoad and prepare a domain-specific dataset for fine-tuning. This involves cleaning, tokenization, and preparation for model input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare a domain-specific dataset for fine-tuning\nfrom datasets import load_dataset, Dataset\nfrom transformers import AutoTokenizer\nimport pandas as pd\n\n# Initialize tokenizer for the base model\nmodel_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Set padding token if not already defined (required for batch processing)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ndef load_and_clean_data(data_path=None):\n    \"\"\"\n    Load and clean domain-specific data.\n    \n    Args:\n        data_path (str, optional): Path to custom dataset. If None, uses example data.\n    \n    Returns:\n        Dataset: Cleaned Hugging Face dataset object\n    \n    Note:\n        Adjust cleaning logic based on your domain requirements\n    \"\"\"\n    if data_path:\n        df = pd.read_csv(data_path)\n    else:\n        data = {\n            \"text\": [\n                \"Example domain-specific text 1\",\n                \"Example domain-specific text 2\",\n                \"Example domain-specific text 3\"\n            ]\n        }\n        df = pd.DataFrame(data)\n    \n    df = df.drop_duplicates(subset=['text'])\n    df = df[df['text'].str.len() >= 10]\n    df['text'] = df['text'].str.strip()\n    \n    dataset = Dataset.from_pandas(df)\n    \n    return dataset\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples['text'],\n        truncation=True,\n        padding='max_length',\n        max_length=512,\n        return_tensors=None\n    )\n\ndataset = load_and_clean_data()\ntokenized_dataset = dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=dataset.column_names,\n    desc=\"Tokenizing dataset\"\n)\n\ntrain_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = train_test_split['train']\neval_dataset = train_test_split['test']\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Validation samples: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine-Tuning Techniques\n\nVarious fine-tuning methods can be employed, including Full Fine-Tuning, Parameter-Efficient Fine-Tuning (PEFT), LoRA, and QLoRA. Here's a basic code snippet using Hugging Face Transformers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"gpt2\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    save_steps=10_000,\n    save_total_limit=2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For more detailed techniques, see [Privatai's guide on fine-tuning methods](https://www.privatai.co.uk/tutorials/llm-fine-tuning?utm_source=openai) and our comprehensive guide on [fine-tuning LLMs with Hugging Face Transformers](/blog/44830763/mastering-fine-tuning-of-large-language-models-with-hugging-face).\n\n### Full End-to-End Application\n\nIntegrate all components into a single, runnable script that produces a working demo. This includes error handling, configuration management, and clear execution flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete end-to-end fine-tuning pipeline\n# This script combines dataset preparation, model selection, training, and evaluation\nimport os\nimport argparse\nimport logging\nfrom pathlib import Path\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling\n)\n\n# Configure logging for the entire pipeline\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\nclass LLMFineTuner:\n    \"\"\"\n    End-to-end pipeline for fine-tuning large language models.\n    \n    This class encapsulates the entire workflow from data loading to model evaluation,\n    making it easy to configure and execute fine-tuning jobs.\n    \"\"\"\n    \n    def __init__(self, config):\n        self.config = config\n        self.model = None\n        self.tokenizer = None\n        self.trainer = None\n        \n        Path(config['output_dir']).mkdir(parents=True, exist_ok=True)\n        \n        logger.info(f\"Initialized LLMFineTuner with config: {config}\")\n    \n    def load_and_prepare_data(self):\n        logger.info(\"Loading and preparing dataset...\")\n        \n        data_path = self.config['data_path']\n        \n        if data_path.endswith('.csv'):\n            dataset = load_dataset('csv', data_files=data_path)['train']\n        elif data_path.endswith('.json'):\n            dataset = load_dataset('json', data_files=data_path)['train']\n        elif data_path.endswith('.txt'):\n            dataset = load_dataset('text', data_files=data_path)['train']\n        else:\n            raise ValueError(f\"Unsupported file format: {data_path}\")\n        \n        if len(dataset) == 0:\n            raise ValueError(\"Dataset is empty\")\n        \n        logger.info(f\"Loaded {len(dataset)} examples\")\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(self.config['model_name'])\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        def tokenize_function(examples):\n            text_column = 'text' if 'text' in examples else list(examples.keys())[0]\n            \n            return self.tokenizer(\n                examples[text_column],\n                truncation=True,\n                padding='max_length',\n                max_length=self.config['max_length'],\n                return_tensors=None\n            )\n        \n        tokenized_dataset = dataset.map(\n            tokenize_function,\n            batched=True,\n            remove_columns=dataset.column_names,\n            desc=\"Tokenizing\"\n        )\n        \n        split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n        \n        logger.info(f\"Train size: {len(split_dataset['train'])}, Eval size: {len(split_dataset['test'])}\")\n        \n        return split_dataset['train'], split_dataset['test']\n    \n    def initialize_model(self):\n        logger.info(f\"Loading model: {self.config['model_name']}\")\n        \n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.config['model_name'],\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n            device_map=\"auto\" if torch.cuda.is_available() else None\n        )\n        \n        if self.tokenizer.pad_token is None:\n            self.model.config.pad_token_id = self.model.config.eos_token_id\n        \n        logger.info(f\"Model loaded: {self.model.num_parameters():,} parameters\")\n        \n        return self.model\n    \n    def train(self, train_dataset, eval_dataset):\n        logger.info(\"Starting training...\")\n        \n        training_args = TrainingArguments(\n            output_dir=self.config['output_dir'],\n            num_train_epochs=self.config['num_epochs'],\n            per_device_train_batch_size=self.config['batch_size'],\n            per_device_eval_batch_size=self.config['batch_size'] * 2,\n            learning_rate=self.config['learning_rate'],\n            warmup_steps=500,\n            weight_decay=0.01,\n            logging_dir=f\"{self.config['output_dir']}/logs\",\n            logging_steps=100,\n            evaluation_strategy=\"steps\",\n            eval_steps=500,\n            save_strategy=\"steps\",\n            save_steps=1000,\n            save_total_limit=2,\n            load_best_model_at_end=True,\n            metric_for_best_model=\"eval_loss\",\n            fp16=torch.cuda.is_available(),\n            report_to=\"tensorboard\",\n            seed=42\n        )\n        \n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.tokenizer,\n            mlm=False\n        )\n        \n        self.trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            data_collator=data_collator,\n            tokenizer=self.tokenizer\n        )\n        \n        train_result = self.trainer.train()\n        \n        self.trainer.save_model()\n        self.tokenizer.save_pretrained(self.config['output_dir'])\n        \n        metrics = train_result.metrics\n        self.trainer.log_metrics(\"train\", metrics)\n        self.trainer.save_metrics(\"train\", metrics)\n        \n        logger.info(f\"Training completed. Final loss: {metrics['train_loss']:.4f}\")\n        \n        return metrics\n    \n    def evaluate(self, eval_dataset):\n        logger.info(\"Evaluating model...\")\n        \n        metrics = self.trainer.evaluate(eval_dataset)\n        self.trainer.log_metrics(\"eval\", metrics)\n        self.trainer.save_metrics(\"eval\", metrics)\n        \n        logger.info(f\"Evaluation completed. Perplexity: {torch.exp(torch.tensor(metrics['eval_loss'])):.2f}\")\n        \n        return metrics\n    \n    def run_pipeline(self):\n        try:\n            train_dataset, eval_dataset = self.load_and_prepare_data()\n            self.initialize_model()\n            train_metrics = self.train(train_dataset, eval_dataset)\n            eval_metrics = self.evaluate(eval_dataset)\n            \n            results = {\n                'train': train_metrics,\n                'eval': eval_metrics\n            }\n            \n            logger.info(\"Pipeline completed successfully!\")\n            \n            return results\n            \n        except Exception as e:\n            logger.error(f\"Pipeline failed: {str(e)}\")\n            raise\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Fine-tune a language model\")\n    parser.add_argument(\"--model_name\", type=str, default=\"gpt2\", help=\"Base model name\")\n    parser.add_argument(\"--data_path\", type=str, required=True, help=\"Path to training data\")\n    parser.add_argument(\"--output_dir\", type=str, default=\"./fine_tuned_model\", help=\"Output directory\")\n    parser.add_argument(\"--max_length\", type=int, default=512, help=\"Maximum sequence length\")\n    parser.add_argument(\"--num_epochs\", type=int, default=3, help=\"Number of training epochs\")\n    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"Training batch size\")\n    parser.add_argument(\"--learning_rate\", type=float, default=5e-5, help=\"Learning rate\")\n    \n    args = parser.parse_args()\n    \n    config = {\n        'model_name': args.model_name,\n        'data_path': args.data_path,\n        'output_dir': args.output_dir,\n        'max_length': args.max_length,\n        'num_epochs': args.num_epochs,\n        'batch_size': args.batch_size,\n        'learning_rate': args.learning_rate\n    }\n    \n    fine_tuner = LLMFineTuner(config)\n    results = fine_tuner.run_pipeline()\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"FINE-TUNING COMPLETED\")\n    print(\"=\"*50)\n    print(f\"Model saved to: {config['output_dir']}\")\n    print(f\"Final training loss: {results['train']['train_loss']:.4f}\")\n    print(f\"Final eval loss: {results['eval']['eval_loss']:.4f}\")\n    print(f\"Perplexity: {torch.exp(torch.tensor(results['eval']['eval_loss'])):.2f}\")\n\n# Example usage:\n# python fine_tune_llm.py --data_path ./my_data.csv --output_dir ./my_model --num_epochs 5\n\nif __name__ == \"__main__\":\n    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing & Validation\n\nAdd comprehensive testing and validation code with multiple evaluation strategies, including quantitative metrics and qualitative analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive testing and validation for fine-tuned models\nimport torch\nimport numpy as np\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score, f1_score\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass ModelEvaluator:\n    \"\"\"\n    Comprehensive evaluation suite for fine-tuned language models.\n    \n    Provides multiple evaluation strategies:\n    - Perplexity on hold-out test set\n    - Generation quality assessment\n    - Task-specific metrics\n    - Human evaluation templates\n    \"\"\"\n    \n    def __init__(self, model_path, test_data_path=None):\n        logger.info(f\"Loading model from {model_path}\")\n        \n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n            device_map=\"auto\" if torch.cuda.is_available() else None\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        self.generator = pipeline(\n            \"text-generation\",\n            model=self.model,\n            tokenizer=self.tokenizer,\n            device=0 if torch.cuda.is_available() else -1\n        )\n        \n        self.test_data = None\n        if test_data_path:\n            self.test_data = self._load_test_data(test_data_path)\n    \n    def _load_test_data(self, data_path):\n        if data_path.endswith('.csv'):\n            dataset = load_dataset('csv', data_files=data_path)['train']\n        elif data_path.endswith('.json'):\n            dataset = load_dataset('json', data_files=data_path)['train']\n        elif data_path.endswith('.txt'):\n            dataset = load_dataset('text', data_files=data_path)['train']\n        else:\n            raise ValueError(f\"Unsupported file format: {data_path}\")\n        \n        logger.info(f\"Loaded {len(dataset)} test examples\")\n        return dataset\n    \n    def calculate_perplexity(self, test_texts=None):\n        logger.info(\"Calculating perplexity...\")\n        \n        if test_texts is None:\n            if self.test_data is None:\n                raise ValueError(\"No test data provided\")\n            text_column = 'text' if 'text' in self.test_data.column_names else self.test_data.column_names[0]\n            test_texts = self.test_data[text_column]\n        \n        total_loss = 0\n        total_tokens = 0\n        \n        self.model.eval()\n        with torch.no_grad():\n            for text in test_texts:\n                inputs = self.tokenizer(\n                    text,\n                    return_tensors=\"pt\",\n                    truncation=True,\n                    max_length=512\n                ).to(self.model.device)\n                \n                outputs = self.model(**inputs, labels=inputs[\"input_ids\"])\n                \n                total_loss += outputs.loss.item() * inputs[\"input_ids\"].size(1)\n                total_tokens += inputs[\"input_ids\"].size(1)\n        \n        avg_loss = total_loss / total_tokens\n        perplexity = np.exp(avg_loss)\n        \n        logger.info(f\"Perplexity: {perplexity:.2f}\")\n        \n        return perplexity\n    \n    def evaluate_generation_quality(self, prompts, max_length=100, num_return_sequences=3):\n        logger.info(f\"Generating {num_return_sequences} responses for {len(prompts)} prompts...\")\n        \n        results = []\n        \n        for prompt in prompts:\n            generations = self.generator(\n                prompt,\n                max_length=max_length,\n                num_return_sequences=num_return_sequences,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                pad_token_id=self.tokenizer.pad_token_id\n            )\n            \n            generated_texts = [gen['generated_text'] for gen in generations]\n            \n            results.append({\n                'prompt': prompt,\n                'generations': generated_texts\n            })\n            \n            logger.info(f\"\\nPrompt: {prompt}\")\n            logger.info(f\"Generation: {generated_texts[0]}\")\n        \n        return results\n    \n    def evaluate_task_specific_metrics(self, test_examples, task_type=\"classification\"):\n        logger.info(f\"Evaluating {task_type} task...\")\n        \n        predictions = []\n        ground_truth = []\n        \n        for example in test_examples:\n            input_text = example['input']\n            expected = example['expected_output']\n            \n            output = self.generator(\n                input_text,\n                max_length=len(input_text.split()) + 50,\n                num_return_sequences=1,\n                do_sample=False,\n                pad_token_id=self.tokenizer.pad_token_id\n            )[0]['generated_text']\n            \n            prediction = output[len(input_text):].strip()\n            \n            predictions.append(prediction)\n            ground_truth.append(expected)\n        \n        if task_type == \"classification\":\n            accuracy = accuracy_score(ground_truth, predictions)\n            f1 = f1_score(ground_truth, predictions, average='weighted')\n            \n            logger.info(f\"Accuracy: {accuracy:.2f}\")\n            logger.info(f\"F1 Score: {f1:.2f}\")\n            \n            return {\"accuracy\": accuracy, \"f1\": f1}\n        \n        return {}\n\n# Example usage\n# evaluator = ModelEvaluator(model_path=\"./fine_tuned_model\", test_data_path=\"./test_data.csv\")\n# perplexity = evaluator.calculate_perplexity()\n# generation_results = evaluator.evaluate_generation_quality(prompts=[\"Example prompt 1\", \"Example prompt 2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this tutorial, we've explored the process of fine-tuning large language models for domain-specific applications. From dataset preparation to model evaluation, each step is crucial for achieving high-performance results. While this guide provides a comprehensive overview, there are always more techniques and optimizations to explore. Consider diving deeper into advanced fine-tuning methods or exploring deployment strategies for production environments. For further reading, check out our articles on [advanced fine-tuning techniques](/blog/44830763/advanced-fine-tuning-techniques) and [deploying LLMs in production](/blog/44830763/deploying-llms-in-production)."
      ]
    }
  ],
  "metadata": {
    "title": "Mastering Fine-Tuning of Large Language Models for Domain-Specific Success",
    "description": "Unlock the potential of large language models with our step-by-step guide to fine-tuning for specialized applications, enhancing performance and achieving production-ready solutions.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}