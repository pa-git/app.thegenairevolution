{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Mastering Fine-Tuning of Large Language Models for Domain-Specific Success\n\n**Description:** Unlock the potential of large language models with our step-by-step guide to fine-tuning for specialized applications, enhancing performance and achieving production-ready solutions.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n\nIn this tutorial, we will walk through the process of fine-tuning a large language model (LLM) using the Hugging Face Transformers library. This guide is designed for AI Builders who are looking to deepen their expertise in deploying real-world applications with Generative AI. By the end of this tutorial, you will have a comprehensive understanding of how to fine-tune a model for domain-specific tasks, optimize it for production, and evaluate its performance. Fine-tuning LLMs is crucial for enhancing model accuracy and utility in specialized applications, such as creating a memory-aware chatbot or a document summarization assistant.\n\n# Installation\n\nTo get started, we need to install the necessary libraries. These installations will provide you with the tools required to work with Hugging Face Transformers and other dependencies. For detailed documentation, visit the [Hugging Face Transformers documentation](https://huggingface.co/docs/transformers). For a more comprehensive guide on fine-tuning with these tools, see our [in-depth walkthrough on fine-tuning LLMs with Hugging Face](/blog/44830763/mastering-fine-tuning-of-large-language-models-with-hugging-face)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries for LLM fine-tuning with Hugging Face ecosystem\n# transformers: Provides pre-trained models and training utilities\n!pip install transformers\n\n# datasets: Enables easy loading and processing of training datasets\n!pip install datasets\n\n# torch: PyTorch backend required for model training and inference\n!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project Setup\n\nBefore diving into the code, ensure you have set up your environment variables and configuration files. This step is crucial for managing API keys, dataset paths, and other sensitive information securely.\n\n# Step-by-Step Build\n\n## Data Handling and Tokenization\n\nFirst, we need to load and tokenize our dataset. This step involves preparing the data for training by converting text into a format that the model can understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\nfrom datasets import load_dataset\n\ndef prepare_dataset(dataset_name, tokenizer, max_length=512):\n    \"\"\"\n    Load and tokenize dataset for fine-tuning.\n    \n    Args:\n        dataset_name (str): Name or path of the dataset to load\n        tokenizer: Hugging Face tokenizer instance\n        max_length (int): Maximum sequence length for tokenization\n        \n    Returns:\n        tuple: Tokenized train and evaluation datasets\n    \"\"\"\n    dataset = load_dataset(dataset_name)\n    \n    def tokenize_function(examples):\n        return tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=max_length\n        )\n    \n    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n    \n    return tokenized_dataset[\"train\"], tokenized_dataset[\"validation\"]\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# Prepare dataset\ntrain_dataset, eval_dataset = prepare_dataset(\"wikitext\", tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Integration and Training\n\nNext, we integrate our pre-trained model and configure it for fine-tuning. This involves setting up training arguments and initiating the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\nimport torch\n\ndef fine_tune_model(\n    model_name=\"gpt2\",\n    dataset_name=\"wikitext\",\n    output_dir=\"./results\",\n    num_epochs=3,\n    batch_size=4,\n    learning_rate=5e-5\n):\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    train_dataset, eval_dataset = prepare_dataset(dataset_name, tokenizer)\n    \n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=num_epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        learning_rate=learning_rate,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        logging_dir=f\"{output_dir}/logs\",\n        logging_steps=100,\n        save_steps=1000,\n        save_total_limit=2,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        seed=42\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer\n    )\n    \n    trainer.train()\n    trainer.save_model(f\"{output_dir}/final_model\")\n    tokenizer.save_pretrained(f\"{output_dir}/final_model\")\n    \n    return trainer\n\n# Fine-tune model\ntrainer = fine_tune_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full End-to-End Application\n\nNow, let's put all the components together in a single, runnable script. This script will demonstrate a working example that you can replicate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    try:\n        trainer = fine_tune_model(\n            model_name=\"gpt2\",\n            dataset_name=\"wikitext\",\n            output_dir=\"./fine_tuned_model\",\n            num_epochs=3,\n            batch_size=4\n        )\n        print(\"Fine-tuning completed successfully!\")\n        \n    except Exception as e:\n        print(f\"Fine-tuning failed with error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing & Validation\n\nTesting and validation are crucial to ensure the model's performance and reliability. Here, we show test runs and evaluation results to validate the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example test run\nsample_input = \"Once upon a time\"\ninput_ids = tokenizer.encode(sample_input, return_tensors=\"pt\")\noutput = model.generate(input_ids, max_length=50, num_return_sequences=1)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n\nIn this tutorial, we explored the process of fine-tuning a large language model using Hugging Face Transformers. We covered data handling, model integration, and training, culminating in a complete, runnable application. Fine-tuning LLMs can significantly enhance their performance for domain-specific tasks, making them more effective and useful in real-world applications. For further strategies and techniques, explore our [detailed guide on mastering fine-tuning techniques](/blog/44830763/mastering-fine-tuning-of-large-language-models-with-hugging-face)."
      ]
    }
  ],
  "metadata": {
    "title": "Mastering Fine-Tuning of Large Language Models for Domain-Specific Success",
    "description": "Unlock the potential of large language models with our step-by-step guide to fine-tuning for specialized applications, enhancing performance and achieving production-ready solutions.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}