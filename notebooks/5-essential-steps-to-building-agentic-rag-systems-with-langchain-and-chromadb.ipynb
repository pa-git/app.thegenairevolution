{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** 5 Essential Steps to Building Agentic RAG Systems with LangChain and ChromaDB\n\n**Description:** Unlock the power of agentic RAG systems with LangChain and ChromaDB. Follow these steps to enhance AI adaptability and relevance in real-world applications.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nBuilding production-ready GenAI applications requires more than just connecting an LLM to a vector database. Traditional RAG systems follow rigid, predefined retrieval patterns that can't adapt to complex queries or changing contexts. Agentic RAG systems solve this by introducing autonomous decision-makingâ€”allowing your AI to intelligently decide when to retrieve information, what tools to use, and how to combine multiple data sources dynamically.\n\nIn this notebook, you'll learn how to build, optimize, and deploy an agentic RAG system using [LangChain](https://docs.langchain.com), [ChromaDB](https://www.trychroma.com), and OpenAI. You'll implement autonomous retrieval agents, add production-grade optimizations like caching and reranking, and set up monitoring to ensure your system performs reliably at scale. By the end, you'll have a fully functional, production-ready agentic RAG system that you can adapt to your own use cases.\n\n## Setup & Installation\n\nFirst, install all required dependencies and configure your environment. This setup includes LangChain for agent orchestration, ChromaDB for vector storage, and OpenAI for embeddings and language models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install and configure the development environment for agentic RAG system\n!pip install langchain chromadb openai langchain-openai langchain-community cachetools pypdf\n\nimport os\nimport logging\nfrom typing import List, Dict, Any\n\n# Configure logging for better debugging and monitoring\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Set up API keys - replace with your actual keys\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n\n# Import and verify installations\ntry:\n    import langchain\n    import chromadb\n    from langchain_openai import OpenAIEmbeddings\n    logger.info(f\"LangChain version: {langchain.__version__}\")\n    logger.info(\"All dependencies installed successfully\")\nexcept ImportError as e:\n    logger.error(f\"Import error: {e}\")\n    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the Vector Store\n\nBefore creating the agentic layer, you need a knowledge base. This section shows how to load documents, split them into optimal chunks, and create a ChromaDB vector store for efficient semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data and set up ChromaDB vector store for document retrieval\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nimport chromadb\n\ndef setup_vector_store(pdf_path: str, collection_name: str = \"documents\") -> Chroma:\n    \"\"\"\n    Load documents, split into chunks, and create a ChromaDB vector store.\n    \n    Args:\n        pdf_path (str): Path to the PDF document to load\n        collection_name (str): Name for the ChromaDB collection\n        \n    Returns:\n        Chroma: Configured ChromaDB vector store instance\n        \n    Raises:\n        FileNotFoundError: If PDF file doesn't exist\n        Exception: If embedding or storage fails\n    \"\"\"\n    try:\n        # Load PDF document - handles multi-page documents efficiently\n        loader = PyPDFLoader(pdf_path)\n        documents = loader.load()\n        logger.info(f\"Loaded {len(documents)} pages from {pdf_path}\")\n        \n        # Split documents into optimal chunks for retrieval\n        # chunk_size=1000 balances context preservation with retrieval precision\n        # chunk_overlap=200 ensures continuity across chunk boundaries\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200,\n            length_function=len,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        chunks = text_splitter.split_documents(documents)\n        logger.info(f\"Split into {len(chunks)} chunks\")\n        \n        # Initialize OpenAI embeddings for semantic similarity\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n        \n        # Create ChromaDB vector store with persistent storage\n        # This enables efficient similarity search and retrieval\n        vectorstore = Chroma.from_documents(\n            documents=chunks,\n            embedding=embeddings,\n            collection_name=collection_name,\n            persist_directory=\"./chroma_db\"\n        )\n        \n        # Test retrieval functionality with a sample query\n        test_results = vectorstore.similarity_search(\"sample query\", k=3)\n        logger.info(f\"Vector store created successfully with {len(test_results)} test results\")\n        \n        return vectorstore\n        \n    except FileNotFoundError:\n        logger.error(f\"PDF file not found: {pdf_path}\")\n        raise\n    except Exception as e:\n        logger.error(f\"Error setting up vector store: {e}\")\n        raise\n\n# Example usage - replace with your actual PDF path\n# vectorstore = setup_vector_store(\"sample_document.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing the Agentic Layer\n\nAgentic Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional RAG systems by incorporating autonomous decision-making capabilities. Unlike static RAG systems, which rely on predefined rules for data retrieval, agentic systems dynamically decide what information to retrieve, how to use it, and when to act. This adaptability is crucial for production AI applications, where context and requirements can change rapidly. For a deeper understanding of how to tailor these systems to specific domains, you might find our guide on [customizing LLMs for domain-specific applications](/article/mastering-domain-specific-llm-customization-techniques-and-tools-unveiled) helpful.\n\nThe following implementation uses LangChain's ReAct agent framework, which enables reasoning and acting in iterative steps. The agent autonomously decides whether to use retrieval tools based on the query complexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement agentic layer with autonomous retrieval capabilities\nfrom langchain.agents import create_react_agent, AgentExecutor\nfrom langchain.tools import Tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain import hub\nfrom typing import Optional\n\nclass AgenticRAGSystem:\n    \"\"\"\n    Agentic RAG system that autonomously decides when and how to retrieve information.\n    \n    Combines LangChain agents with ChromaDB for intelligent document retrieval.\n    \"\"\"\n    \n    def __init__(self, vectorstore: Chroma, model_name: str = \"gpt-3.5-turbo\"):\n        \"\"\"\n        Initialize the agentic RAG system.\n        \n        Args:\n            vectorstore (Chroma): Configured ChromaDB vector store\n            model_name (str): OpenAI model to use for reasoning\n        \"\"\"\n        self.vectorstore = vectorstore\n        self.llm = ChatOpenAI(model=model_name, temperature=0)\n        self.tools = self._create_tools()\n        self.agent = self._create_agent()\n        \n    def _create_tools(self) -> List[Tool]:\n        \"\"\"\n        Create retrieval tools for the agent to use autonomously.\n        \n        Returns:\n            List[Tool]: List of tools available to the agent\n        \"\"\"\n        def similarity_search(query: str) -> str:\n            \"\"\"\n            Perform similarity search in the vector store.\n            \n            Args:\n                query (str): Search query\n                \n            Returns:\n                str: Formatted search results\n            \"\"\"\n            try:\n                # Retrieve top 3 most relevant documents\n                # k=3 balances relevance with response length\n                results = self.vectorstore.similarity_search(query, k=3)\n                \n                if not results:\n                    return \"No relevant documents found.\"\n                \n                # Format results for agent consumption\n                formatted_results = []\n                for i, doc in enumerate(results, 1):\n                    formatted_results.append(f\"Document {i}: {doc.page_content[:500]}...\")\n                \n                return \"\\n\\n\".join(formatted_results)\n                \n            except Exception as e:\n                logger.error(f\"Error in similarity search: {e}\")\n                return f\"Search error: {str(e)}\"\n        \n        # Define tools available to the agent\n        tools = [\n            Tool(\n                name=\"document_search\",\n                description=\"Search through documents to find relevant information. Use this when you need specific information from the knowledge base.\",\n                func=similarity_search\n            )\n        ]\n        \n        return tools\n    \n    def _create_agent(self) -> AgentExecutor:\n        \"\"\"\n        Create ReAct agent with autonomous decision-making capabilities.\n        \n        Returns:\n            AgentExecutor: Configured agent executor\n        \"\"\"\n        # Get ReAct prompt template from LangChain hub\n        # ReAct enables reasoning and acting in iterative steps\n        prompt = hub.pull(\"hwchase17/react\")\n        \n        # Create agent with reasoning capabilities\n        agent = create_react_agent(self.llm, self.tools, prompt)\n        \n        # Configure agent executor with error handling\n        agent_executor = AgentExecutor(\n            agent=agent,\n            tools=self.tools,\n            verbose=True,  # Enable detailed logging\n            handle_parsing_errors=True,  # Graceful error handling\n            max_iterations=3  # Prevent infinite loops\n        )\n        \n        return agent_executor\n    \n    def query(self, question: str) -> str:\n        \"\"\"\n        Process user query with autonomous retrieval decisions.\n        \n        Args:\n            question (str): User's question\n            \n        Returns:\n            str: Agent's response with retrieved information\n            \n        Raises:\n            Exception: If agent execution fails\n        \"\"\"\n        try:\n            # Agent autonomously decides whether to use retrieval tools\n            # Based on query complexity and information requirements\n            response = self.agent.invoke({\"input\": question})\n            return response[\"output\"]\n            \n        except Exception as e:\n            logger.error(f\"Error processing query: {e}\")\n            return f\"I encountered an error processing your question: {str(e)}\"\n\n# Example usage\n# agentic_system = AgenticRAGSystem(vectorstore)\n# response = agentic_system.query(\"What are the main topics discussed in the document?\")\n# print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Production Optimizations\n\nProduction environments require more than basic functionality. This section implements critical optimizations including response caching, relevance-based reranking, and performance monitoring. These improvements reduce latency, improve answer quality, and provide visibility into system behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimize system performance with caching, reranking, and monitoring\nfrom cachetools import TTLCache, cached\nfrom functools import wraps\nimport time\nfrom typing import Tuple\n\nclass OptimizedAgenticRAG(AgenticRAGSystem):\n    \"\"\"\n    Production-ready agentic RAG system with performance optimizations.\n    \n    Includes caching, reranking, monitoring, and error handling.\n    \"\"\"\n    \n    def __init__(self, vectorstore: Chroma, model_name: str = \"gpt-3.5-turbo\"):\n        super().__init__(vectorstore, model_name)\n        # TTL cache with 5-minute expiration to balance freshness and performance\n        self.cache = TTLCache(maxsize=100, ttl=300)\n        self.query_metrics = {\"total_queries\": 0, \"cache_hits\": 0, \"avg_response_time\": 0}\n    \n    def _enhanced_similarity_search(self, query: str, k: int = 5) -> str:\n        \"\"\"\n        Enhanced similarity search with reranking and relevance scoring.\n        \n        Args:\n            query (str): Search query\n            k (int): Number of documents to retrieve initially\n            \n        Returns:\n            str: Reranked and formatted search results\n        \"\"\"\n        try:\n            # Retrieve more documents initially for better reranking\n            results = self.vectorstore.similarity_search_with_score(query, k=k)\n            \n            if not results:\n                return \"No relevant documents found.\"\n            \n            # Rerank results by relevance score (lower scores = higher similarity)\n            # Filter out results with low relevance (score > 0.8)\n            relevant_results = [(doc, score) for doc, score in results if score < 0.8]\n            \n            if not relevant_results:\n                return \"No sufficiently relevant documents found.\"\n            \n            # Format top 3 results for agent consumption\n            formatted_results = []\n            for i, (doc, score) in enumerate(relevant_results[:3], 1):\n                confidence = max(0, (1 - score) * 100)  # Convert to confidence percentage\n                formatted_results.append(\n                    f\"Document {i} (Confidence: {confidence:.1f}%): {doc.page_content[:500]}...\"\n                )\n            \n            return \"\\n\\n\".join(formatted_results)\n            \n        except Exception as e:\n            logger.error(f\"Error in enhanced similarity search: {e}\")\n            return f\"Search error: {str(e)}\"\n    \n    @cached(cache=lambda self: self.cache)\n    def _cached_query(self, question: str) -> Tuple[str, bool]:\n        \"\"\"\n        Execute query with caching to improve response times.\n        \n        Args:\n            question (str): User's question\n            \n        Returns:\n            Tuple[str, bool]: (response, was_cached)\n        \"\"\"\n        # This method will be cached automatically by the decorator\n        response = super().query(question)\n        return response, False  # False indicates this was not from cache\n    \n    def query(self, question: str) -> str:\n        \"\"\"\n        Process query with performance monitoring and caching.\n        \n        Args:\n            question (str): User's question\n            \n        Returns:\n            str: Optimized response with performance metrics\n        \"\"\"\n        start_time = time.time()\n        self.query_metrics[\"total_queries\"] += 1\n        \n        try:\n            # Check cache first\n            cache_key = hash(question)\n            if cache_key in self.cache:\n                self.query_metrics[\"cache_hits\"] += 1\n                response = self.cache[cache_key]\n                logger.info(f\"Cache hit for query: {question[:50]}...\")\n                return response\n            \n            # Execute query if not in cache\n            response = super().query(question)\n            \n            # Cache successful responses\n            self.cache[cache_key] = response\n            \n            # Update performance metrics\n            response_time = time.time() - start_time\n            self._update_metrics(response_time)\n            \n            logger.info(f\"Query processed in {response_time:.2f}s\")\n            return response\n            \n        except Exception as e:\n            logger.error(f\"Error in optimized query: {e}\")\n            return f\"I encountered an error: {str(e)}. Please try rephrasing your question.\"\n    \n    def _update_metrics(self, response_time: float) -> None:\n        \"\"\"\n        Update performance metrics for monitoring.\n        \n        Args:\n            response_time (float): Time taken to process the query\n        \"\"\"\n        # Calculate rolling average response time\n        current_avg = self.query_metrics[\"avg_response_time\"]\n        total_queries = self.query_metrics[\"total_queries\"]\n        \n        # Update average using incremental formula\n        self.query_metrics[\"avg_response_time\"] = (\n            (current_avg * (total_queries - 1) + response_time) / total_queries\n        )\n    \n    def get_performance_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Get system performance statistics for monitoring.\n        \n        Returns:\n            Dict[str, Any]: Performance metrics\n        \"\"\"\n        cache_hit_rate = (\n            self.query_metrics[\"cache_hits\"] / max(1, self.query_metrics[\"total_queries\"]) * 100\n        )\n        \n        return {\n            \"total_queries\": self.query_metrics[\"total_queries\"],\n            \"cache_hit_rate\": f\"{cache_hit_rate:.1f}%\",\n            \"avg_response_time\": f\"{self.query_metrics['avg_response_time']:.2f}s\",\n            \"cache_size\": len(self.cache)\n        }\n    \n    def health_check(self) -> Dict[str, str]:\n        \"\"\"\n        Perform system health check for production monitoring.\n        \n        Returns:\n            Dict[str, str]: Health status of system components\n        \"\"\"\n        health_status = {\"overall\": \"healthy\"}\n        \n        try:\n            # Test vector store connectivity\n            test_results = self.vectorstore.similarity_search(\"test\", k=1)\n            health_status[\"vectorstore\"] = \"healthy\"\n        except Exception as e:\n            health_status[\"vectorstore\"] = f\"unhealthy: {str(e)}\"\n            health_status[\"overall\"] = \"degraded\"\n        \n        try:\n            # Test LLM connectivity\n            test_response = self.llm.invoke(\"test\")\n            health_status[\"llm\"] = \"healthy\"\n        except Exception as e:\n            health_status[\"llm\"] = f\"unhealthy: {str(e)}\"\n            health_status[\"overall\"] = \"degraded\"\n        \n        return health_status\n\n# Example production deployment setup\ndef deploy_agentic_rag():\n    \"\"\"\n    Example deployment function for production environments.\n    \n    Returns:\n        OptimizedAgenticRAG: Production-ready system instance\n    \"\"\"\n    try:\n        # Initialize vector store (replace with your actual setup)\n        # vectorstore = setup_vector_store(\"your_documents.pdf\")\n        \n        # Create optimized system\n        # system = OptimizedAgenticRAG(vectorstore)\n        \n        # Perform health check\n        # health = system.health_check()\n        # logger.info(f\"System health: {health}\")\n        \n        # return system\n        \n        logger.info(\"Deployment template ready - uncomment and configure with your data\")\n        return None\n        \n    except Exception as e:\n        logger.error(f\"Deployment failed: {e}\")\n        raise\n\n# Example usage for monitoring\n# system = deploy_agentic_rag()\n# if system:\n#     response = system.query(\"What are the key findings?\")\n#     stats = system.get_performance_stats()\n#     print(f\"Response: {response}\")\n#     print(f\"Performance: {stats}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing & Validation\n\nBefore deploying to production, thoroughly test your agentic RAG system to ensure it performs reliably under various conditions. This section demonstrates how to validate functionality, measure performance, and verify system health."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive testing suite for agentic RAG system\nimport unittest\nfrom typing import List\n\nclass AgenticRAGTester:\n    \"\"\"\n    Testing utilities for validating agentic RAG system functionality.\n    \"\"\"\n    \n    def __init__(self, system: OptimizedAgenticRAG):\n        self.system = system\n        self.test_results = []\n    \n    def test_basic_retrieval(self) -> bool:\n        \"\"\"\n        Test basic document retrieval functionality.\n        \n        Returns:\n            bool: True if test passes\n        \"\"\"\n        try:\n            test_query = \"What is the main topic?\"\n            response = self.system.query(test_query)\n            \n            # Verify response is not empty and doesn't contain error messages\n            success = (\n                len(response) > 0 and \n                \"error\" not in response.lower() and\n                response != \"No relevant documents found.\"\n            )\n            \n            self.test_results.append({\n                \"test\": \"basic_retrieval\",\n                \"passed\": success,\n                \"response_length\": len(response)\n            })\n            \n            logger.info(f\"Basic retrieval test: {'PASSED' if success else 'FAILED'}\")\n            return success\n            \n        except Exception as e:\n            logger.error(f\"Basic retrieval test failed: {e}\")\n            return False\n    \n    def test_cache_performance(self) -> bool:\n        \"\"\"\n        Test caching functionality and performance improvement.\n        \n        Returns:\n            bool: True if caching works correctly\n        \"\"\"\n        try:\n            test_query = \"What are the key findings?\"\n            \n            # First query (uncached)\n            start_time = time.time()\n            response1 = self.system.query(test_query)\n            first_query_time = time.time() - start_time\n            \n            # Second query (should be cached)\n            start_time = time.time()\n            response2 = self.system.query(test_query)\n            cached_query_time = time.time() - start_time\n            \n            # Verify responses match and cached query is faster\n            success = (\n                response1 == response2 and\n                cached_query_time < first_query_time\n            )\n            \n            self.test_results.append({\n                \"test\": \"cache_performance\",\n                \"passed\": success,\n                \"speedup\": f\"{first_query_time / max(cached_query_time, 0.001):.2f}x\"\n            })\n            \n            logger.info(f\"Cache performance test: {'PASSED' if success else 'FAILED'}\")\n            return success\n            \n        except Exception as e:\n            logger.error(f\"Cache performance test failed: {e}\")\n            return False\n    \n    def test_health_monitoring(self) -> bool:\n        \"\"\"\n        Test health check functionality.\n        \n        Returns:\n            bool: True if health check works\n        \"\"\"\n        try:\n            health_status = self.system.health_check()\n            \n            # Verify all required components are checked\n            required_components = [\"overall\", \"vectorstore\", \"llm\"]\n            success = all(component in health_status for component in required_components)\n            \n            self.test_results.append({\n                \"test\": \"health_monitoring\",\n                \"passed\": success,\n                \"status\": health_status\n            })\n            \n            logger.info(f\"Health monitoring test: {'PASSED' if success else 'FAILED'}\")\n            return success\n            \n        except Exception as e:\n            logger.error(f\"Health monitoring test failed: {e}\")\n            return False\n    \n    def run_all_tests(self) -> Dict[str, Any]:\n        \"\"\"\n        Run complete test suite and return results.\n        \n        Returns:\n            Dict[str, Any]: Test results summary\n        \"\"\"\n        logger.info(\"Starting test suite...\")\n        \n        tests = [\n            self.test_basic_retrieval,\n            self.test_cache_performance,\n            self.test_health_monitoring\n        ]\n        \n        passed = sum(test() for test in tests)\n        total = len(tests)\n        \n        summary = {\n            \"total_tests\": total,\n            \"passed\": passed,\n            \"failed\": total - passed,\n            \"success_rate\": f\"{(passed/total)*100:.1f}%\",\n            \"details\": self.test_results\n        }\n        \n        logger.info(f\"Test suite completed: {passed}/{total} tests passed\")\n        return summary\n\n# Example usage\n# tester = AgenticRAGTester(system)\n# results = tester.run_all_tests()\n# print(f\"Test Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By implementing these optimizations and testing strategies, you can ensure your agentic RAG system is reliable and efficient in a production environment. To further enhance the performance of your system, consider exploring techniques for [fine-tuning language models with Hugging Face Transformers](/article/mastering-fine-tuning-of-large-language-models-with-hugging-face), which can improve the accuracy and relevance of the generated outputs.\n\n## Conclusion\n\nYou've now built a production-ready agentic RAG system with autonomous decision-making, performance optimizations, and comprehensive monitoring. The key lessons learned include:\n\n- **Autonomous retrieval** enables more flexible and context-aware information gathering compared to static RAG systems\n- **Caching and reranking** significantly improve response times and answer quality in production environments\n- **Health checks and metrics** provide essential visibility for maintaining reliable AI systems at scale\n- **Comprehensive testing** ensures your system performs correctly under various conditions before deployment\n\nTo take your system to the next level, consider exploring advanced patterns such as multi-agent systems for complex workflows, hybrid search combining vector and keyword retrieval, and dynamic tool selection for multi-modal data sources. For more information, visit the [LangChain documentation](https://docs.langchain.com) and [ChromaDB documentation](https://www.trychroma.com). Additionally, our article on [customizing LLMs for domain-specific applications](/article/mastering-domain-specific-llm-customization-techniques-and-tools-unveiled) provides insights into advanced customization techniques that can be applied to enhance your agentic RAG systems.\n\nNext steps for production deployment include setting up CI/CD pipelines for automated testing and deployment, implementing distributed caching with Redis for multi-instance deployments, and integrating with observability platforms like Prometheus or Datadog for comprehensive monitoring. With these foundations in place, you're ready to build and scale intelligent, autonomous AI systems that adapt to real-world complexity."
      ]
    }
  ],
  "metadata": {
    "title": "5 Essential Steps to Building Agentic RAG Systems with LangChain and ChromaDB",
    "description": "Unlock the power of agentic RAG systems with LangChain and ChromaDB. Follow these steps to enhance AI adaptability and relevance in real-world applications.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}