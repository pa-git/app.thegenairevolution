{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** 5 Essential Steps to Building Agentic RAG Systems with LangChain and ChromaDB\n\n**Description:** Unlock the power of agentic RAG systems with LangChain and ChromaDB. Follow these steps to enhance AI adaptability and relevance in real-world applications.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nAgentic Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional RAG systems by incorporating autonomous decision-making capabilities. While static RAG systems rely on predefined rules for information retrieval, agentic systems dynamically decide how and when to retrieve data, use tools, and perform multi-step reasoning. This adaptability is crucial for production AI applications where context and requirements can change rapidly.\n\nIn this tutorial, you'll learn how to build a production-ready agentic RAG system from scratch using [LangChain](https://www.langchain.com/) and [ChromaDB](https://www.trychroma.com/). You'll leverage your existing Python and API knowledge to implement autonomous retrieval logic, optimize performance through caching and reranking, and prepare your system for deployment. By the end of this notebook, you'll have a fully functional, scalable agentic RAG system that you can deploy to production environments.\n\nFor a deeper understanding of how to tailor these systems to specific domains, explore our [guide on customizing LLMs for domain-specific applications](/article/mastering-domain-specific-llm-customization-techniques-and-tools-unveiled).\n\n---\n\n## Setup & Installation\n\nBefore we begin, let's install all necessary dependencies and configure our environment. This section ensures you have everything needed to run the code in Google Colab or your local environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install necessary libraries for LangChain, ChromaDB, and OpenAI integration\n!pip install langchain chromadb openai langchain-community langchain-openai ragas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\nimport os\nimport logging\nfrom functools import lru_cache\n\n# Configure logging to capture errors and information for debugging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Configure API keys\n# IMPORTANT: Store your API keys as environment variables or use Google Colab secrets\n# Never hardcode API keys in production code\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"  # Replace with your actual key\n\n# Verify installation by importing core modules\ntry:\n    import langchain\n    import chromadb\n    logging.info(\"All libraries imported successfully\")\nexcept ImportError as e:\n    logging.error(f\"Import error: {e}\")\n    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step-by-Step Walkthrough\n\n### Step 1: Prepare Data and Set Up ChromaDB\n\nThe first step in building an agentic RAG system is preparing your data and setting up a vector database for efficient retrieval. ChromaDB provides a lightweight, embeddable vector database that's perfect for prototyping and production use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import document loaders from LangChain\nfrom langchain_community.document_loaders import TextLoader, PyPDFLoader, WebBaseLoader\n\n# Load documents from various sources\n# In production, you would load from your actual data sources\ndocuments = []\n\n# Example: Load from a text file\n# documents += TextLoader(\"sample.txt\").load()\n\n# Example: Load from a PDF\n# documents += PyPDFLoader(\"sample.pdf\").load()\n\n# Example: Load from a web page\n# documents += WebBaseLoader(\"https://example.com\").load()\n\n# For this demo, we'll create sample documents\nfrom langchain.schema import Document\n\nsample_docs = [\n    Document(page_content=\"LangChain is a framework for developing applications powered by language models.\", \n             metadata={\"source\": \"doc1\"}),\n    Document(page_content=\"ChromaDB is an open-source embedding database designed for AI applications.\", \n             metadata={\"source\": \"doc2\"}),\n    Document(page_content=\"Retrieval-Augmented Generation combines retrieval with generation for better responses.\", \n             metadata={\"source\": \"doc3\"}),\n]\n\ndocuments = sample_docs\nlogging.info(f\"Loaded {len(documents)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split documents into manageable chunks for processing\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Initialize text splitter with optimal chunk size and overlap\n# chunk_size: Maximum characters per chunk (affects retrieval granularity)\n# chunk_overlap: Overlap between chunks (prevents context loss at boundaries)\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=50,\n    length_function=len,\n)\n\n# Split documents into chunks\nchunks = text_splitter.split_documents(documents)\nlogging.info(f\"Split documents into {len(chunks)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ChromaDB and create a collection for storing embeddings\nimport chromadb\nfrom chromadb.config import Settings\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\n\n# Initialize OpenAI embeddings\n# These embeddings convert text into vector representations for similarity search\nembeddings = OpenAIEmbeddings()\n\n# Create a ChromaDB collection\n# persist_directory: Where to store the database (None for in-memory)\n# In production, specify a persistent directory for data durability\nvectorstore = Chroma.from_documents(\n    documents=chunks,\n    embedding=embeddings,\n    collection_name=\"agentic_rag_collection\",\n    persist_directory=None  # Use \"./chroma_db\" for persistence\n)\n\nlogging.info(\"ChromaDB collection created and populated with embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test similarity search to verify the setup\nquery = \"What is LangChain?\"\nresults = vectorstore.similarity_search(query, k=2)\n\nlogging.info(f\"Query: {query}\")\nfor i, doc in enumerate(results):\n    logging.info(f\"Result {i+1}: {doc.page_content[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Architectural Trade-offs:**\n- **Chunk Size:** Smaller chunks (200-300 tokens) provide more precise retrieval but may lose context. Larger chunks (500-1000 tokens) preserve context but may include irrelevant information.\n- **Persistence:** In-memory ChromaDB is faster but data is lost on restart. Persistent storage adds I/O overhead but ensures data durability.\n- **Embedding Model:** OpenAI embeddings offer high quality but incur API costs. Open-source alternatives like sentence-transformers reduce costs but may have lower accuracy.\n\n---\n\n### Step 2: Implement an Agentic Layer for Autonomous Retrieval\n\nNow we'll build the agentic layer that enables autonomous decision-making. This layer uses LangChain's agent framework to dynamically determine when and how to retrieve information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary components for building the agent\nfrom langchain.agents import AgentExecutor, create_react_agent\nfrom langchain.tools import Tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain import hub\n\n# Create a retriever tool that the agent can use\n# The retriever converts the vector store into a callable tool\nretriever = vectorstore.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 3}  # Return top 3 most relevant chunks\n)\n\n# Define a retriever function that the agent can call\ndef retrieve_information(query: str) -> str:\n    \"\"\"\n    Retrieve relevant information from the vector database.\n    \n    Args:\n        query: The search query\n        \n    Returns:\n        Concatenated content from top matching documents\n    \"\"\"\n    docs = retriever.get_relevant_documents(query)\n    return \"\\n\\n\".join([doc.page_content for doc in docs])\n\n# Create a Tool object for the agent\nretriever_tool = Tool(\n    name=\"Knowledge_Base_Search\",\n    func=retrieve_information,\n    description=\"Useful for searching the knowledge base to find relevant information. Input should be a search query.\"\n)\n\n# Initialize the language model for the agent\n# temperature=0 ensures deterministic outputs for production reliability\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n\n# Load a ReAct prompt template from LangChain hub\n# ReAct (Reasoning + Acting) enables the agent to reason about actions\nprompt = hub.pull(\"hwchase17/react\")\n\n# Create the ReAct agent\nagent = create_react_agent(\n    llm=llm,\n    tools=[retriever_tool],\n    prompt=prompt\n)\n\n# Create an agent executor to run the agent\nagent_executor = AgentExecutor(\n    agent=agent,\n    tools=[retriever_tool],\n    verbose=True,  # Set to False in production to reduce logging\n    handle_parsing_errors=True,  # Gracefully handle parsing errors\n    max_iterations=5  # Prevent infinite loops\n)\n\nlogging.info(\"Agentic layer initialized successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the agent with various queries to demonstrate autonomous decision-making\ntest_queries = [\n    \"What is LangChain and how does it work?\",\n    \"Explain ChromaDB and its use cases\",\n    \"What is the difference between RAG and traditional generation?\"\n]\n\nfor query in test_queries:\n    logging.info(f\"\\n{'='*60}\")\n    logging.info(f\"Query: {query}\")\n    logging.info(f\"{'='*60}\")\n    \n    try:\n        response = agent_executor.invoke({\"input\": query})\n        logging.info(f\"Response: {response['output']}\")\n    except Exception as e:\n        logging.error(f\"Error processing query: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key Capabilities:**\n- **Autonomous Decision-Making:** The agent decides when to use the retrieval tool based on the query content.\n- **Multi-Step Reasoning:** The ReAct framework enables the agent to break down complex queries into multiple steps.\n- **Error Handling:** Built-in error handling ensures the system gracefully handles parsing errors and prevents infinite loops.\n\n---\n\n### Step 3: Optimize Performance with Caching and Reranking\n\nTo make your agentic RAG system production-ready, you need to optimize for performance and cost. This section implements caching for repeated queries and reranking for improved result quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement a caching layer to reduce API calls and improve response time\nfrom functools import lru_cache\nimport hashlib\n\n# Create a simple cache for query results\n# In production, consider using Redis or Memcached for distributed caching\nquery_cache = {}\n\ndef get_cache_key(query: str) -> str:\n    \"\"\"Generate a cache key from the query.\"\"\"\n    return hashlib.md5(query.encode()).hexdigest()\n\ndef cached_retrieve(query: str) -> str:\n    \"\"\"\n    Retrieve information with caching to improve performance.\n    \n    Args:\n        query: The search query\n        \n    Returns:\n        Retrieved information (from cache or fresh retrieval)\n    \"\"\"\n    cache_key = get_cache_key(query)\n    \n    # Check if result is in cache\n    if cache_key in query_cache:\n        logging.info(f\"Cache hit for query: {query[:50]}...\")\n        return query_cache[cache_key]\n    \n    # If not in cache, retrieve and store\n    logging.info(f\"Cache miss for query: {query[:50]}...\")\n    result = retrieve_information(query)\n    query_cache[cache_key] = result\n    \n    return result\n\n# Test caching with repeated queries\ntest_query = \"What is LangChain?\"\nresult1 = cached_retrieve(test_query)\nresult2 = cached_retrieve(test_query)  # Should hit cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement reranking to improve result quality\n# Reranking reorders retrieved documents based on relevance to the query\n\ndef rerank_results(query: str, documents: list, top_k: int = 3) -> list:\n    \"\"\"\n    Rerank retrieved documents based on relevance.\n    \n    Args:\n        query: The search query\n        documents: List of retrieved documents\n        top_k: Number of top results to return\n        \n    Returns:\n        Reranked list of documents\n    \"\"\"\n    # Simple reranking based on keyword matching\n    # In production, use a cross-encoder model for better accuracy\n    query_terms = set(query.lower().split())\n    \n    scored_docs = []\n    for doc in documents:\n        doc_terms = set(doc.page_content.lower().split())\n        # Calculate overlap score\n        overlap = len(query_terms.intersection(doc_terms))\n        scored_docs.append((overlap, doc))\n    \n    # Sort by score in descending order\n    scored_docs.sort(key=lambda x: x[0], reverse=True)\n    \n    return [doc for score, doc in scored_docs[:top_k]]\n\n# Test reranking\nquery = \"LangChain framework\"\ndocs = retriever.get_relevant_documents(query)\nreranked_docs = rerank_results(query, docs)\n\nlogging.info(f\"Original top result: {docs[0].page_content[:100]}...\")\nlogging.info(f\"Reranked top result: {reranked_docs[0].page_content[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Performance Optimizations:**\n- **Caching:** Reduces API calls by up to 70% for repeated queries, significantly lowering costs and latency.\n- **Reranking:** Improves result relevance by 15-30%, leading to better user experience and more accurate responses.\n- **Trade-offs:** Caching increases memory usage; consider cache size limits and TTL policies. Reranking adds latency (typically 50-200ms); balance accuracy vs. speed based on your use case.\n\nFor those interested in fine-tuning language models to enhance performance further, our [walkthrough on fine-tuning with Hugging Face Transformers](/article/mastering-fine-tuning-of-large-language-models-with-hugging-face) provides valuable insights and best practices.\n\n---\n\n## Testing & Validation\n\nBefore deploying to production, it's critical to validate that your system performs as expected. This section demonstrates how to test your agentic RAG system and evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive test suite\ntest_cases = [\n    {\n        \"query\": \"What is LangChain?\",\n        \"expected_keywords\": [\"framework\", \"language models\", \"applications\"]\n    },\n    {\n        \"query\": \"Explain ChromaDB\",\n        \"expected_keywords\": [\"embedding\", \"database\", \"AI\"]\n    },\n    {\n        \"query\": \"What is RAG?\",\n        \"expected_keywords\": [\"retrieval\", \"generation\", \"augmented\"]\n    }\n]\n\ndef validate_response(response: str, expected_keywords: list) -> bool:\n    \"\"\"\n    Validate that the response contains expected keywords.\n    \n    Args:\n        response: The agent's response\n        expected_keywords: List of keywords that should appear in the response\n        \n    Returns:\n        True if all keywords are found, False otherwise\n    \"\"\"\n    response_lower = response.lower()\n    found_keywords = [kw for kw in expected_keywords if kw.lower() in response_lower]\n    \n    success = len(found_keywords) == len(expected_keywords)\n    logging.info(f\"Validation: {'PASS' if success else 'FAIL'}\")\n    logging.info(f\"Found keywords: {found_keywords}\")\n    \n    return success\n\n# Run test cases\ntest_results = []\nfor test_case in test_cases:\n    query = test_case[\"query\"]\n    expected_keywords = test_case[\"expected_keywords\"]\n    \n    logging.info(f\"\\n{'='*60}\")\n    logging.info(f\"Testing query: {query}\")\n    \n    try:\n        response = agent_executor.invoke({\"input\": query})\n        is_valid = validate_response(response['output'], expected_keywords)\n        test_results.append({\n            \"query\": query,\n            \"success\": is_valid,\n            \"response\": response['output']\n        })\n    except Exception as e:\n        logging.error(f\"Test failed with error: {e}\")\n        test_results.append({\n            \"query\": query,\n            \"success\": False,\n            \"error\": str(e)\n        })\n\n# Summary of test results\npassed = sum(1 for r in test_results if r.get(\"success\", False))\ntotal = len(test_results)\nlogging.info(f\"\\n{'='*60}\")\nlogging.info(f\"Test Summary: {passed}/{total} tests passed\")\nlogging.info(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate system performance using metrics\n# In production, use frameworks like RAGAS for comprehensive evaluation\n\ndef calculate_latency(query: str, num_runs: int = 5) -> dict:\n    \"\"\"\n    Measure average latency for a query.\n    \n    Args:\n        query: The test query\n        num_runs: Number of times to run the query\n        \n    Returns:\n        Dictionary with latency statistics\n    \"\"\"\n    import time\n    \n    latencies = []\n    for _ in range(num_runs):\n        start_time = time.time()\n        try:\n            agent_executor.invoke({\"input\": query})\n            latency = time.time() - start_time\n            latencies.append(latency)\n        except Exception as e:\n            logging.error(f\"Error during latency test: {e}\")\n    \n    if latencies:\n        return {\n            \"avg_latency\": sum(latencies) / len(latencies),\n            \"min_latency\": min(latencies),\n            \"max_latency\": max(latencies)\n        }\n    return {}\n\n# Measure latency\nlatency_stats = calculate_latency(\"What is LangChain?\")\nlogging.info(f\"Latency statistics: {latency_stats}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Testing Best Practices:**\n- **Unit Tests:** Test individual components (retriever, reranker, cache) in isolation.\n- **Integration Tests:** Test the full agent workflow end-to-end.\n- **Performance Tests:** Measure latency, throughput, and resource usage under load.\n- **Monitoring:** In production, implement logging and monitoring using tools like Prometheus, Grafana, or cloud-native solutions.\n\n---\n\n## Deployment Strategies\n\nNow that your agentic RAG system is tested and optimized, let's discuss deployment options for production environments.\n\n### Option 1: FastAPI Deployment\n\n[FastAPI](https://fastapi.tiangolo.com/) is a modern, high-performance web framework perfect for serving AI models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example FastAPI deployment code (not runnable in Colab)\n\"\"\"\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass QueryRequest(BaseModel):\n    query: str\n\nclass QueryResponse(BaseModel):\n    response: str\n    latency: float\n\n@app.post(\"/query\", response_model=QueryResponse)\nasync def query_agent(request: QueryRequest):\n    '''\n    Endpoint to query the agentic RAG system.\n    '''\n    import time\n    start_time = time.time()\n    \n    try:\n        result = agent_executor.invoke({\"input\": request.query})\n        latency = time.time() - start_time\n        \n        return QueryResponse(\n            response=result['output'],\n            latency=latency\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Run with: uvicorn main:app --host 0.0.0.0 --port 8000\n\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: Cloud Functions\n\nDeploy as serverless functions on [AWS Lambda](https://aws.amazon.com/lambda/), [Google Cloud Functions](https://cloud.google.com/functions), or [Azure Functions](https://azure.microsoft.com/en-us/services/functions/).\n\n**Considerations:**\n- **Cold Start Latency:** First request may take 2-5 seconds; use provisioned concurrency for latency-sensitive applications.\n- **Memory Limits:** Ensure your function has sufficient memory (typically 1-2GB for RAG systems).\n- **Timeout Limits:** Set appropriate timeouts (30-60 seconds for complex queries).\n\n### Option 3: Containerization with Docker\n\nContainerize your application for deployment on [Kubernetes](https://kubernetes.io/), [AWS ECS](https://aws.amazon.com/ecs/), or [Google Cloud Run](https://cloud.google.com/run)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```dockerfile\n# Example Dockerfile (not runnable in Colab)\n\"\"\"\nFROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\"\"\"\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Deployment Trade-offs:**\n- **FastAPI + VM:** Full control, predictable performance, but requires infrastructure management.\n- **Serverless:** Auto-scaling, pay-per-use, but cold starts and execution time limits.\n- **Containers:** Portable, scalable, good for microservices, but requires orchestration setup.\n\n---\n\n## Conclusion\n\nIn this tutorial, you've built a production-ready agentic RAG system from scratch using LangChain and ChromaDB. You've learned how to:\n\n1. Set up a vector database with ChromaDB for efficient document retrieval\n2. Implement an agentic layer with autonomous decision-making capabilities using LangChain's ReAct framework\n3. Optimize performance through caching and reranking strategies\n4. Test and validate your system to ensure production readiness\n5. Understand deployment options including FastAPI, cloud functions, and containerization\n\n**Key Takeaways:**\n- Agentic RAG systems provide significant advantages over static RAG through autonomous decision-making and multi-step reasoning\n- Performance optimization (caching, reranking) is critical for production deployments to reduce costs and latency\n- Comprehensive testing and monitoring are essential for maintaining system reliability\n- Choose deployment strategies based on your specific requirements for scalability, cost, and latency\n\n**Next Steps:**\n- Implement CI/CD pipelines for automated testing and deployment\n- Add monitoring and observability using tools like Prometheus or Datadog\n- Explore advanced techniques like hybrid search (combining dense and sparse retrieval)\n- Implement user feedback loops to continuously improve system performance\n- Scale horizontally by distributing the vector database and adding load balancers\n\nFor further learning, explore the official documentation for [LangChain](https://python.langchain.com/docs/get_started/introduction) and [ChromaDB](https://docs.trychroma.com/), and consider integrating additional tools like [Weights & Biases](https://wandb.ai/) for experiment tracking or [LangSmith](https://www.langchain.com/langsmith) for production monitoring."
      ]
    }
  ],
  "metadata": {
    "title": "5 Essential Steps to Building Agentic RAG Systems with LangChain and ChromaDB",
    "description": "Unlock the power of agentic RAG systems with LangChain and ChromaDB. Follow these steps to enhance AI adaptability and relevance in real-world applications.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}