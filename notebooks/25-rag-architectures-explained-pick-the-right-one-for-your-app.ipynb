{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** 25 RAG Architectures Explained â€” Pick the Right One for Your App\n\n**Description:** Quickly choose the right RAG architecture for your use case: reduce hallucinations, cut latency, and ship with project-ready examples today.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieval-Augmented Generation (RAG) is a powerful technique for grounding LLM responses in external knowledge. This guide walks you through a minimal, runnable baseline RAG pipeline using LangChain, then demonstrates how to extend it with corrective, speculative, and agentic patternsâ€”all executable in Google Colab with a single dataset.\n\n## Why Use LangChain for RAG\n\nLangChain provides a unified interface for building RAG pipelines, abstracting retriever setup, prompt templating, and chain orchestration. For AI Builders, this means faster iteration on retrieval strategies, easier integration of multiple LLMs, and modular components that can be swapped or extended without rewriting core logic.\n\n## Core Concepts for This Use Case\n\n- **Retriever**: Fetches relevant document chunks from a vector store based on semantic similarity.\n- **Chain**: Orchestrates retrieval and generation, passing context to the LLM.\n- **Prompt Template**: Structures the input to the LLM, ensuring answers are grounded in retrieved context.\n- **Evaluation**: Measures relevance, groundedness, and latency to compare RAG variants.\n\n## Setup\n\nInstall the required packages and set your OpenAI API key. This guide uses a minimal dependency set for a single-tool tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-openai langchain-community chromadb sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your OpenAI API key securely. If running in Colab, use the snippet below to prompt for the key at runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom getpass import getpass\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify that the API key is set before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise EnvironmentError(\n        \"Missing OPENAI_API_KEY. Please set it in the environment or use the cell above.\"\n    )\n\nprint(\"API key is set. Ready to proceed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using LangChain for RAG in Practice\n\n### Baseline RAG: Fast, Simple Retrieval for FAQs\n\nStart with a minimal RAG pipeline. This example uses a small internal knowledge base for \"Acme\" and retrieves relevant chunks to answer user questions.\n\nPrepare a minimal document and split it into chunks for retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain.chains import RetrievalQA\nfrom langchain.docstore.document import Document\n\ndocs = [Document(page_content=\"\"\"\nAcme Docs: Reset your password via Settings > Security. For MFA issues, contact support@acme.com.\n\"\"\")]\n\nsplitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\nchunks = splitter.split_documents(docs)\n\nembeddings = OpenAIEmbeddings()\nvectordb = Chroma.from_documents(chunks, embedding=embeddings)\nretriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nqa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n\nprint(\"Baseline RAG setup complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run a sample query to verify the pipeline works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(qa.run(\"How do I reset my password?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Corrective RAG: Two-Pass Generation for Higher Precision\n\nCorrective RAG adds a verification step to catch and correct unsupported claims in the initial answer.\n\nGenerate an initial answer, then verify and correct it using a second LLM pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nverify_prompt = PromptTemplate.from_template(\n    \"Given the question and answer, identify unsupported claims and correct them using ONLY the context.\\nContext:\\n{context}\\nQ:{q}\\nA:{a}\\nRevised answer:\"\n)\n\ninitial = qa.run(\"Explain MFA recovery steps.\")\ncontext = retriever.get_relevant_documents(\"Explain MFA recovery steps.\")\n\nverify = LLMChain(llm=llm, prompt=verify_prompt)\ncorrected = verify.run({\n    \"context\": \"\\n\".join([d.page_content for d in context]),\n    \"q\": \"Explain MFA recovery steps.\",\n    \"a\": initial\n})\n\nprint(corrected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Speculative RAG: Draft with Fast Model, Verify with Strong Model\n\nSpeculative RAG uses a smaller, faster LLM for the initial draft, then refines it with a stronger model to improve factual accuracy.\n\nGenerate a draft answer with a fast model, then verify and improve it with the baseline model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fast_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\ndraft = RetrievalQA.from_chain_type(fast_llm, retriever=retriever).run(\"Summarize MFA steps.\")\nfinal = qa.run(f\"Improve factual accuracy of: {draft}. Keep it concise.\")\n\nprint(final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agentic RAG: Use an Agent to Plan, Retrieve, and Act\n\nAgentic RAG gives the LLM access to tools, allowing it to plan multi-step queries and cite sources.\n\nDefine a tool for document search and initialize a zero-shot agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import initialize_agent, Tool, AgentType\n\ndef search_docs(q):\n    \"\"\"Search internal docs for relevant content.\"\"\"\n    return \"\\n\".join([d.page_content for d in retriever.get_relevant_documents(q)])\n\ntools = [Tool(name=\"doc_search\", func=search_docs, description=\"Search internal docs\")]\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False)\n\nprint(agent.run(\"Create a step-by-step policy summary; cite documents.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Evaluate\n\nMeasure latency and evaluate answer quality for each RAG variant. Use a consistent test query to compare performance.\n\nTime each variant and print the duration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n\ndef time_call(fn, *args, **kwargs):\n    \"\"\"Measure execution time of a function call.\"\"\"\n    start = time.time()\n    output = fn(*args, **kwargs)\n    duration = time.time() - start\n    return output, duration\n\ntest_query = \"How do I reset my password?\"\n\nvariants = [\n    (\"Baseline RAG\", lambda q: qa.run(q)),\n    (\"Corrective RAG\", lambda q: verify.run({\n        \"context\": \"\\n\".join([d.page_content for d in retriever.get_relevant_documents(q)]),\n        \"q\": q,\n        \"a\": qa.run(q)\n    })),\n    (\"Speculative RAG\", lambda q: qa.run(f\"Improve factual accuracy of: {RetrievalQA.from_chain_type(fast_llm, retriever=retriever).run(q)}. Keep it concise.\")),\n]\n\nfor name, fn in variants:\n    output, duration = time_call(fn, test_query)\n    print(f\"{name}: {round(duration, 2)}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate answer relevance and groundedness using an LLM-as-judge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def judge_answer(q, ctx, a):\n    \"\"\"Use LLM to rate answer relevance and groundedness.\"\"\"\n    prompt = (\n        f\"Question: {q}\\nContext: {ctx}\\nAnswer: {a}\\n\"\n        \"Rate 0-5 for relevance and 0-5 for groundedness in context. Respond in JSON format.\"\n    )\n    return llm.predict(prompt)\n\ndocs = retriever.get_relevant_documents(test_query)\ncontext = \"\\n\".join([d.page_content for d in docs])\nanswer = qa.run(test_query)\n\nprint(judge_answer(test_query, context, answer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nYou've built a baseline RAG pipeline with LangChain and extended it with corrective, speculative, and agentic patterns. Each variant trades off latency, accuracy, and complexity. Use the evaluation harness to measure these trade-offs on your own dataset and choose the pattern that best fits your use case. Next, explore reranking strategies or integrate external tools to further refine retrieval quality."
      ]
    }
  ],
  "metadata": {
    "title": "25 RAG Architectures Explained â€” Pick the Right One for Your App",
    "description": "Quickly choose the right RAG architecture for your use case: reduce hallucinations, cut latency, and ship with project-ready examples today.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}