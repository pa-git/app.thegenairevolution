{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build a Multi-Agent Chatbot with CrewAI, ChromaDB, Gradio\n\n**Description:** Build a production-ready multi-agent chatbot with analyst and reviewer agents, ChromaDB RAG, CrewAI, and Gradio, delivering clearer, verified answers consistently.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A multi-agent chatbot with two rolesâ€”Analyst (retrieve/synthesize) and Reviewer (verify/refine)â€”orchestrated by CrewAI. The system uses ChromaDB for vector search (RAG) and a Gradio frontend for a chat UI. The result: fewer hallucinations, clearer answers, and traceable sources, ready to ship or extend. For more on orchestrating multi-agent workflows, see our guide on [how to build multi-agent AI systems with CrewAI and YAML](/article/how-to-build-multi-agent-ai-systems-with-crewai-and-yaml-2).\n\n## What you'll build\n\nA chatbot that answers questions from a knowledge base (e.g., Bank of Canada reports) with citations. Two agents collaborate: the Analyst retrieves and drafts, the Reviewer validates and refines. You'll implement document chunking, embeddings, vector storage, tool creation, agent orchestration, and a chat UI. For practical tips on improving retrieval accuracy in your RAG pipeline, check out [7 retrieval tricks to boost answer accuracy](/article/rag-application-7-retrieval-tricks-to-boost-answer-accuracy-2).\n\n**Stack:**\n- **ChromaDB** â€“ persistent vector store for semantic search\n- **OpenAI embeddings** â€“ text-embedding-3-small for document encoding\n- **CrewAI** â€“ multi-agent orchestration with sequential tasks\n- **Gradio** â€“ chat interface with history and examples\n- **LangChain** â€“ standardized embedding interface for provider flexibility\n\n**Why this stack?**\n- **ChromaDB** persists embeddings on disk, supports metadata filtering, and requires no external service.\n- **CrewAI** handles agent coordination, tool calling, and memory without custom loops.\n- **Gradio** provides a production-ready chat UI in ~10 lines.\n- **LangChain** wraps OpenAI embeddings with batching and provider swap ease.\n\n## How it works\n\n1. **Load and chunk** markdown documents by headings, merge to ~1200 chars with overlap.\n2. **Embed and index** chunks in ChromaDB with source/section metadata.\n3. **Define tools** for semantic search and ambiguity detection.\n4. **Create agents**: Analyst (search + draft) and Reviewer (validate + refine).\n5. **Orchestrate** with CrewAI sequential tasks: Analyst â†’ Reviewer.\n6. **Serve** via Gradio chat interface with conversation history.\n\nUser query â†’ Analyst retrieves from Chroma (RAG) and drafts â†’ Reviewer validates, cross-checks, and refines â†’ Gradio returns a final, cited answer.\n\n## Setup\n\nInstall dependencies in a Colab or notebook cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install \"langchain>=0.2\" \"langchain-openai>=0.1.7\" crewai \"crewai-tools>=0.1.0\" chromadb gradio python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Securely load API keys from Colab userdata:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom google.colab import userdata\nfrom google.colab.userdata import SecretNotFoundError\n\nkeys = [\"OPENAI_API_KEY\", \"ANTHROPIC_API_KEY\"]\nmissing = []\nfor k in keys:\n    value = None\n    try:\n        value = userdata.get(k)\n    except SecretNotFoundError:\n        pass\n\n    os.environ[k] = value if value is not None else \"\"\n\n    if not os.environ[k]:\n        missing.append(k)\n\nif missing:\n    raise EnvironmentError(f\"Missing keys: {', '.join(missing)}. Add them in Colab â†’ Settings â†’ Secrets.\")\n\nprint(\"All keys loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create directories and download sample documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\nimport urllib.request\n\nDOCS_DIR = Path(\"data/docs\")\nDOCS_DIR.mkdir(parents=True, exist_ok=True)\n\nsample_url = \"https://raw.githubusercontent.com/example/sample-docs/main/sample.md\"\nsample_path = DOCS_DIR / \"sample.md\"\nif not sample_path.exists():\n    urllib.request.urlretrieve(sample_url, sample_path)\n    print(f\"Downloaded sample document to {sample_path}\")\nelse:\n    print(f\"Sample document already exists at {sample_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 1: Document loading and semantic chunking\n\nRead markdown files, split by headings, and merge into ~1200-character chunks with 200-character overlap. This balances context size and retrieval precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\nimport uuid\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nDOCS_DIR = Path(\"data/docs\")\nPERSIST_DIR = \"data/chroma\"\n\ndef read_markdown_files(folder: Path) -> List[Dict[str, Any]]:\n    \"\"\"\n    Read all markdown files from a folder.\n\n    Args:\n        folder (Path): Path to the folder containing .md files.\n\n    Returns:\n        List[Dict[str, Any]]: List of dicts with 'path' and 'text' keys.\n    \"\"\"\n    docs = []\n    for path in folder.glob(\"**/*.md\"):\n        text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n        docs.append({\"path\": str(path), \"text\": text})\n    return docs\n\ndef split_by_headings(text: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    Split markdown text into sections by headings.\n\n    Args:\n        text (str): Markdown document as a string.\n\n    Returns:\n        List[Dict[str, Any]]: List of dicts with 'heading' and 'text'.\n    \"\"\"\n    parts = re.split(r'(?m)(^#{1,6}\\s.*$)', text)\n    chunks = []\n    current = \"\"\n    current_heading = \"Introduction\"\n    for i in range(len(parts)):\n        if re.match(r'(?m)^#{1,6}\\s', parts[i] or \"\"):\n            if current.strip():\n                chunks.append({\"heading\": current_heading, \"text\": current.strip()})\n            current_heading = parts[i].strip().lstrip(\"#\").strip()\n            current = \"\"\n        else:\n            current += parts[i] or \"\"\n    if current.strip():\n        chunks.append({\"heading\": current_heading, \"text\": current.strip()})\n    return chunks\n\ndef merge_to_target_size(sections: List[Dict[str, str]], max_chars=1200, overlap=200) -> List[Dict[str, str]]:\n    \"\"\"\n    Merge sections into chunks of approximately max_chars, with overlap.\n\n    Args:\n        sections (List[Dict[str, str]]): List of section dicts.\n        max_chars (int): Target maximum characters per chunk (1200 balances context and precision).\n        overlap (int): Number of characters to overlap between chunks (200 preserves context across boundaries).\n\n    Returns:\n        List[Dict[str, str]]: List of merged chunk dicts.\n    \"\"\"\n    merged = []\n    buf = \"\"\n    start_idx = 0\n    heading = sections[0][\"heading\"] if sections else \"Document\"\n    for idx, sec in enumerate(sections):\n        candidate = (buf + \"\\n\\n\" + f\"# {sec['heading']}\\n{sec['text']}\".strip()).strip()\n        if len(candidate) > max_chars and buf:\n            merged.append({\"heading\": heading, \"text\": buf.strip(), \"section_start\": start_idx, \"section_end\": idx - 1})\n            buf = (buf[-overlap:] + \"\\n\\n\" + f\"# {sec['heading']}\\n{sec['text']}\".strip()) if overlap > 0 else f\"# {sec['heading']}\\n{sec['text']}\".strip()\n            heading = sec[\"heading\"]\n            start_idx = idx\n        else:\n            buf = candidate\n    if buf.strip():\n        merged.append({\"heading\": heading, \"text\": buf.strip(), \"section_start\": start_idx, \"section_end\": len(sections)-1})\n    return merged\n\ndef build_chunks(docs: List[Dict[str, Any]], max_chars=1200, overlap=200) -> List[Dict[str, Any]]:\n    \"\"\"\n    Build semantic chunks from loaded documents.\n\n    Args:\n        docs (List[Dict[str, Any]]): List of loaded documents.\n        max_chars (int): Max characters per chunk.\n        overlap (int): Overlap in characters between chunks.\n\n    Returns:\n        List[Dict[str, Any]]: List of chunk dicts with metadata.\n    \"\"\"\n    all_chunks = []\n    for d in docs:\n        sections = split_by_headings(d[\"text\"])\n        merged = merge_to_target_size(sections, max_chars=max_chars, overlap=overlap)\n        for i, m in enumerate(merged):\n            all_chunks.append({\n                \"id\": str(uuid.uuid4()),\n                \"text\": m[\"text\"],\n                \"metadata\": {\n                    \"source\": d[\"path\"],\n                    \"section\": m[\"heading\"],\n                    \"chunk_index\": i\n                }\n            })\n    return all_chunks\n\ndocs = read_markdown_files(DOCS_DIR)\nprint(f\"Loaded {len(docs)} markdown files\")\nchunks = build_chunks(docs)\nprint(f\"Produced {len(chunks)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 2: Embedding and vector store\n\nEmbed chunks with OpenAI text-embedding-3-small and store in ChromaDB with cosine similarity. Persist to disk for reuse across sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import chromadb\nfrom langchain_openai import OpenAIEmbeddings\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nassert OPENAI_API_KEY, \"Set OPENAI_API_KEY in your environment or Colab secrets\"\n\ndef get_embeddings():\n    \"\"\"\n    Initialize OpenAI embeddings for document encoding.\n\n    Returns:\n        OpenAIEmbeddings: Embedding model instance.\n    \"\"\"\n    return OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=OPENAI_API_KEY)\n\ndef get_chroma_collection(persist_dir=PERSIST_DIR, name=\"kb\"):\n    \"\"\"\n    Get or create a persistent ChromaDB collection.\n\n    Args:\n        persist_dir (str): Directory for ChromaDB persistence.\n        name (str): Collection name.\n\n    Returns:\n        chromadb.Collection: ChromaDB collection object.\n    \"\"\"\n    client = chromadb.PersistentClient(path=persist_dir)\n    col = client.get_or_create_collection(name=name, metadata={\"hnsw:space\": \"cosine\"})\n    return col\n\ndef index_chunks(chunks: List[Dict[str, Any]], col, embeddings):\n    \"\"\"\n    Embed and index document chunks in ChromaDB.\n\n    Args:\n        chunks (List[Dict[str, Any]]): List of chunk dicts.\n        col (chromadb.Collection): ChromaDB collection.\n        embeddings (OpenAIEmbeddings): Embedding model.\n    \"\"\"\n    texts = [c[\"text\"] for c in chunks]\n    ids = [c[\"id\"] for c in chunks]\n    metas = [c[\"metadata\"] for c in chunks]\n    print(f\"Embedding {len(texts)} chunks...\")\n    vecs = embeddings.embed_documents(texts)\n    col.add(documents=texts, metadatas=metas, ids=ids, embeddings=vecs)\n    print(f\"Stored {col.count()} documents in vector database\")\n\nemb = get_embeddings()\ncol = get_chroma_collection()\n\nif col.count() == 0 and chunks:\n    index_chunks(chunks, col, emb)\nelse:\n    print(f\"Chroma collection has {col.count()} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test retrieval with a sample query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_retrieval(col, embeddings, query: str, k=5, min_sim=0.7):\n    \"\"\"\n    Test semantic retrieval from ChromaDB.\n\n    Args:\n        col (chromadb.Collection): ChromaDB collection.\n        embeddings (OpenAIEmbeddings): Embedding model.\n        query (str): User query.\n        k (int): Number of results.\n        min_sim (float): Minimum similarity threshold (0.7 filters low-relevance results).\n\n    Returns:\n        List[Dict[str, Any]]: List of retrieved docs with similarity and metadata.\n    \"\"\"\n    qvec = embeddings.embed_query(query)\n    res = col.query(query_embeddings=[qvec], n_results=k, include=[\"documents\", \"metadatas\", \"distances\"])\n    docs = []\n    for doc, meta, dist in zip(res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]):\n        sim = 1.0 - float(dist)\n        if sim >= min_sim:\n            docs.append({\"similarity\": round(sim, 3), \"meta\": meta, \"snippet\": (doc[:300] + \"...\")})\n    return docs\n\nexamples = test_retrieval(col, emb, \"What is the Bank of Canada's inflation target?\", k=3)\nfor d in examples:\n    print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 3: CrewAI tools for search and clarification\n\nDefine tools for semantic search and ambiguity detection. The Analyst and Reviewer will call these during task execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Tuple, Optional\nfrom crewai_tools import tool\nimport json\n\ndef chroma_search(col, embeddings, query: str, k=5, min_sim=0.68, where: Optional[Dict[str, Any]] = None):\n    \"\"\"\n    Perform semantic search over ChromaDB.\n\n    Args:\n        col (chromadb.Collection): ChromaDB collection.\n        embeddings (OpenAIEmbeddings): Embedding model.\n        query (str): Search query.\n        k (int): Number of results.\n        min_sim (float): Minimum similarity threshold (0.68 balances recall and precision).\n        where (Optional[Dict[str, Any]]): Metadata filter (e.g., {\"source\": {\"$contains\": \"report\"}}).\n\n    Returns:\n        List[Dict[str, Any]]: List of matching items.\n    \"\"\"\n    qvec = embeddings.embed_query(query)\n    res = col.query(query_embeddings=[qvec], n_results=k, where=where, include=[\"documents\", \"metadatas\", \"distances\"])\n    items = []\n    for doc, meta, dist in zip(res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]):\n        sim = 1.0 - float(dist)\n        if sim >= min_sim:\n            items.append({\"similarity\": float(sim), \"metadata\": meta, \"document\": doc})\n    return items\n\ndef is_ambiguous(query: str) -> Tuple[bool, str]:\n    \"\"\"\n    Heuristically determine if a query is ambiguous.\n\n    Args:\n        query (str): User query.\n\n    Returns:\n        Tuple[bool, str]: (Is ambiguous, clarification message)\n    \"\"\"\n    q = query.strip().lower()\n    too_short = len(q) < 12\n    vague_terms = any(t in q for t in [\"tell me more\", \"details\", \"explain\", \"what about it\", \"how is it\"])\n    if too_short or vague_terms:\n        return True, \"Your question seems broad. Please specify the topic, time period, or metric you care about.\"\n    return False, \"\"\n\n_EMB = emb\n_COL = col\n\n@tool(\"search_kb\", return_direct=False)\ndef search_kb(query: str, k: int = 5, min_similarity: float = 0.68, source_filter: str = \"\") -> str:\n    \"\"\"\n    Search the knowledge base using semantic similarity.\n\n    Args:\n        query (str): The user query.\n        k (int): Number of results to return.\n        min_similarity (float): Minimum cosine similarity threshold (0-1).\n        source_filter (str): Substring that must be in the source path.\n\n    Returns:\n        str: JSON with results: [{\"similarity\": float, \"source\": str, \"section\": str, \"text\": str}, ...]\n    \"\"\"\n    global _EMB, _COL\n    where = {\"source\": {\"$contains\": source_filter}} if source_filter else None\n    items = chroma_search(_COL, _EMB, query, k=k, min_sim=min_similarity, where=where)\n    results = []\n    for it in items:\n        m = it[\"metadata\"]\n        results.append({\n            \"similarity\": round(it[\"similarity\"], 3),\n            \"source\": m.get(\"source\"),\n            \"section\": m.get(\"section\"),\n            \"text\": it[\"document\"][:1200]\n        })\n    return json.dumps(results)\n\n@tool(\"maybe_ask_for_clarification\", return_direct=True)\ndef maybe_ask_for_clarification(query: str) -> str:\n    \"\"\"\n    If the query is ambiguous or too broad, return a clarifying question. Otherwise return an empty string.\n\n    Args:\n        query (str): User query.\n\n    Returns:\n        str: Clarification message or empty string.\n    \"\"\"\n    amb, msg = is_ambiguous(query)\n    if amb:\n        return f\"CLARIFICATION_NEEDED: {msg} Example: 'Compare Q2 2023 inflation vs. Q2 2024 and cite sources.'\"\n    return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 4: CrewAI agents and tasks\n\nDefine two agents: Analyst (retrieve and draft) and Reviewer (validate and refine). Each agent has a role, goal, backstory, and tools. If you're interested in foundational agent design patterns, see our walkthrough on [how to build an LLM agent from scratch with GPT-4 ReAct](/article/how-to-build-an-llm-agent-from-scratch-with-gpt-4-react-5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from crewai import Agent, Task, Crew, Process\n\ndef build_agents():\n    \"\"\"\n    Build Analyst and Reviewer agents for CrewAI.\n\n    Returns:\n        Tuple[Agent, Agent]: (Analyst, Reviewer)\n    \"\"\"\n    analyst = Agent(\n        role=\"Financial Research Analyst\",\n        goal=\"Find precise, sourced information from the knowledge base and draft a structured answer.\",\n        backstory=(\n            \"You are a diligent analyst specializing in central bank reports. \"\n            \"You always verify with the search_kb tool and provide citations.\"\n        ),\n        tools=[search_kb, maybe_ask_for_clarification],\n        allow_delegation=False,\n        verbose=True,\n        memory=True,\n        llm=\"gpt-4o-mini\"\n    )\n\n    reviewer = Agent(\n        role=\"Fact-Checking Reviewer\",\n        goal=\"Verify the Analyst's draft against sources, fix errors, and improve clarity.\",\n        backstory=(\n            \"You are a critical editor. You cross-check every claim against the provided sources and ensure the answer is unambiguous.\"\n        ),\n        tools=[search_kb],\n        allow_delegation=False,\n        verbose=True,\n        memory=True,\n        llm=\"gpt-4o-mini\"\n    )\n    return analyst, reviewer\n\ndef build_tasks(analyst: Agent, reviewer: Agent):\n    \"\"\"\n    Build Analyst and Reviewer tasks for CrewAI.\n\n    Args:\n        analyst (Agent): Analyst agent.\n        reviewer (Agent): Reviewer agent.\n\n    Returns:\n        Tuple[Task, Task]: (Analyst task, Reviewer task)\n    \"\"\"\n    analyst_task = Task(\n        description=(\n            \"Use the tools to address the user's question.\\n\"\n            \"Inputs:\\n\"\n            \"User Query: {user_query}\\n\"\n            \"Conversation Summary: {conv_summary}\\n\"\n            \"Instructions:\\n\"\n            \"1) If the query is ambiguous, call maybe_ask_for_clarification and STOP until user responds.\\n\"\n            \"2) Otherwise, call search_kb with a focused query; review top results; synthesize.\\n\"\n            \"3) Produce a structured DRAFT including:\\n\"\n            \"   - Direct answer in 2-4 sentences\\n\"\n            \"   - Key evidence (bullet list, short quotes)\\n\"\n            \"   - Citations as [source:path#section]\\n\"\n            \"   - Notes on uncertainty\"\n        ),\n        agent=analyst,\n        expected_output=\"DRAFT with 'Answer', 'Evidence', 'Citations', 'Uncertainty' sections.\"\n    )\n\n    reviewer_task = Task(\n        description=(\n            \"Review the Analyst's DRAFT for factual accuracy and clarity.\\n\"\n            \"Steps:\\n\"\n            \"1) Re-run search_kb for any claims you doubt.\\n\"\n            \"2) Remove any unsourced or conflicting statements.\\n\"\n            \"3) Tighten language; ensure citations map to evidence.\\n\"\n            \"4) Output FINAL_ANSWER with concise paragraphs and a 'Sources' list.\\n\"\n            \"5) If information is missing, state limitations explicitly.\"\n        ),\n        agent=reviewer,\n        expected_output=\"FINAL_ANSWER with short paragraphs and 'Sources' section.\",\n        context=[analyst_task]\n    )\n    return analyst_task, reviewer_task\n\ndef build_crew():\n    \"\"\"\n    Build the CrewAI crew with Analyst and Reviewer agents.\n\n    Returns:\n        Crew: CrewAI crew object.\n    \"\"\"\n    analyst, reviewer = build_agents()\n    analyst_task, reviewer_task = build_tasks(analyst, reviewer)\n    crew = Crew(\n        agents=[analyst, reviewer],\n        tasks=[analyst_task, reviewer_task],\n        process=Process.sequential,\n        verbose=True\n    )\n    return crew\n\nCREW = build_crew()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 5: Gradio chat interface\n\nWrap the crew in a Gradio chat interface. The chat handler summarizes conversation history and invokes the crew for each user message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n\ndef summarize_history(history: list) -> str:\n    \"\"\"\n    Summarize recent chat history for context.\n\n    Args:\n        history (list): List of (user, bot) message pairs.\n\n    Returns:\n        str: Summarized conversation string (last 5 exchanges, truncated to 2000 chars).\n    \"\"\"\n    lines = []\n    for user_msg, bot_msg in history[-5:]:\n        lines.append(f\"User: {user_msg}\")\n        lines.append(f\"Bot: {bot_msg[:400] if bot_msg else ''}\")\n    return \"\\n\".join(lines[-2000:])\n\ndef chat_predict(message, history):\n    \"\"\"\n    Gradio chat handler: process user message and return bot response.\n\n    Args:\n        message (str): User message.\n        history (list): Conversation history.\n\n    Returns:\n        str: Bot response.\n    \"\"\"\n    amb, clar_msg = is_ambiguous(message)\n    if amb:\n        return f\"{clar_msg}\"\n\n    conv_summary = summarize_history(history or [])\n    result = CREW.kickoff(inputs={\"user_query\": message, \"conv_summary\": conv_summary})\n    if isinstance(result, dict) and \"final_output\" in result:\n        return result[\"final_output\"]\n    return str(result)\n\nwith gr.Blocks(fill_height=True, theme=\"soft\") as demo:\n    gr.Markdown(\"# Multi-Agent RAG Chatbot\\nA two-stage Analyst â†’ Reviewer workflow with ChromaDB RAG.\")\n    examples = [\n        \"Summarize the Bank of Canada's latest inflation outlook and cite sources.\",\n        \"Compare Q2 2023 vs. Q2 2024 GDP growth figures with citations.\",\n        \"What risks did the report highlight about housing markets?\"\n    ]\n    chat = gr.ChatInterface(\n        fn=chat_predict,\n        title=\"Analyst + Reviewer Chatbot\",\n        description=\"Ask questions about your knowledge base. Answers include citations and reviewer verification.\",\n        examples=examples,\n        cache_examples=False,\n        submit_btn=\"Ask\",\n        retry_btn=\"Retry\",\n        undo_btn=\"Undo\",\n        clear_btn=\"Clear\"\n    )\n    gr.Markdown(\"Tip: Be specific (time period, metric). The system asks for clarification when needed.\")\n\ndemo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and validate\n\nLaunch the Gradio app and test with sample queries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test retrieval accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_queries = [\n    \"What is the Bank of Canada's inflation target?\",\n    \"Compare Q2 2023 vs. Q2 2024 GDP growth\",\n    \"What risks did the report highlight?\"\n]\n\nfor q in test_queries:\n    print(f\"\\nQuery: {q}\")\n    results = test_retrieval(col, emb, q, k=3, min_sim=0.68)\n    for r in results:\n        print(f\"  Similarity: {r['similarity']}, Source: {r['meta']['source']}, Section: {r['meta']['section']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test clarification path with a vague query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vague_query = \"Tell me more\"\namb, msg = is_ambiguous(vague_query)\nprint(f\"Ambiguous: {amb}, Message: {msg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning and trade-offs\n\n**Chunk size and overlap:**\n- Smaller chunks (600-800 chars) improve precision but may lose context.\n- Larger chunks (1500+ chars) retain context but dilute relevance.\n- Start with 1200 chars and 200 overlap; adjust based on your corpus.\n\n**Similarity threshold:**\n- Lower (0.5-0.6) increases recall but adds noise.\n- Higher (0.75+) improves precision but may miss relevant results.\n- 0.68 is a balanced default; tune based on retrieval tests.\n\n**Model choice:**\n- gpt-4o-mini is fast and cost-effective for most queries.\n- Upgrade to gpt-4o for complex reasoning or multi-hop questions.\n- Use gpt-3.5-turbo for high-volume, low-complexity workloads.\n\n**Recommended settings by corpus size:**\n- Small (< 50 docs): k=5, threshold=0.68, gpt-4o-mini\n- Medium (50-500 docs): k=7, threshold=0.70, gpt-4o-mini\n- Large (500+ docs): k=10, threshold=0.72, gpt-4o\n\n## Next steps\n\n**Add metadata filters:** Filter by date, author, or document type in search_kb to narrow results.\n\n**Implement retry logic:** Wrap crew.kickoff and embedding calls in retry/backoff for API timeouts.\n\n**Deploy with FastAPI:** Wrap chat_predict in a FastAPI endpoint for production serving.\n\n**Add observability:** Log queries, retrieval results, and agent outputs to track performance.\n\n**Swap providers:** Replace OpenAI with Azure OpenAI or OSS models by updating base_url and api_key.\n\n**Extend agents:** Add a third agent for summarization or a fourth for fact-checking external sources."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build a Multi-Agent Chatbot with CrewAI, ChromaDB, Gradio",
    "description": "Build a production-ready multi-agent chatbot with analyst and reviewer agents, ChromaDB RAG, CrewAI, and Gradio, delivering clearer, verified answers consistently.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}