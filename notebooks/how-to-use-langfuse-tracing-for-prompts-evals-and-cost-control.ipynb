{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Use Langfuse Tracing for Prompts, Evals, and Cost Control\n\n**Description:** Instrument calls with Langfuse tracing to see traces/spans, version prompts safely, A/B on datasets, and tie evaluations to p95 latency, token cost, and acceptance-rate improvements.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Observability Matters for GenAI Applications\n\nBuilding reliable, efficient GenAI systems requires more than just connecting to an LLM API. As you scale from prototype to production, you need visibility into latency, token usage, errors, and user feedback. Without observability, you're debugging blindâ€”unable to pinpoint bottlenecks, optimize costs, or measure quality.\n\nLangfuse is an open-source observability platform designed specifically for LLM applications. It captures traces, spans, and generations across your entire pipeline, giving you the data to improve performance, reduce costs, and iterate faster. This guide shows you how to instrument a Python application with Langfuse, from basic tracing to advanced scoring and feedback loops.\n\n## Setting Up Langfuse\n\nInstall the Langfuse SDK and set your credentials as environment variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install langfuse openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "export LANGFUSE_PUBLIC_KEY=\"your-public-key\"\nexport LANGFUSE_SECRET_KEY=\"your-secret-key\"\nexport LANGFUSE_HOST=\"https://cloud.langfuse.com\"  # or self-hosted URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize the client in your application:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom langfuse import Langfuse\n\nlangfuse = Langfuse(\n    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n    host=os.getenv(\"LANGFUSE_HOST\"),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This setup connects your app to Langfuse, enabling trace collection and analysis in the dashboard.\n\n## Creating Traces and Spans\n\nA **trace** represents a single user request end-to-end. A **span** is a timed operation within that traceâ€”like retrieval, generation, or tool execution.\n\nStart a trace with metadata for filtering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def start_trace(user_id: str, route: str, model: str):\n    trace = langfuse.trace(\n        name=\"chat_request\",\n        user_id=user_id,\n        metadata={\"route\": route, \"env\": os.getenv(\"APP_ENV\", \"dev\"), \"model\": model},\n    )\n    return trace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wrap operations in spans to measure latency:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n\ndef with_span(trace, name: str):\n    span = trace.span(name=name)\n    start = time.time()\n    try:\n        yield span\n    finally:\n        span.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use spans to isolate slow componentsâ€”retrieval, reranking, or generationâ€”and optimize where it counts.\n\n## Tracing LLM Generations\n\nCapture input, output, token usage, and latency for every LLM call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n\nclient = OpenAI()\n\ndef call_llm_with_tracing(trace, messages, model=\"gpt-4o-mini\"):\n    gen = trace.generation(\n        name=\"chat.completions\",\n        model=model,\n        metadata={\"provider\": \"openai\"},\n        input=messages,\n    )\n    t0 = time.time()\n    resp = client.chat.completions.create(model=model, messages=messages)\n    t1 = time.time()\n\n    content = resp.choices[0].message.content\n    usage = {\"input\": resp.usage.prompt_tokens, \"output\": resp.usage.completion_tokens}\n\n    gen.end(\n        output=content,\n        usage=usage,\n        metadata={\"elapsed_ms\": int((t1 - t0) * 1000)},\n    )\n    return content, usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This logs every generation with full context, making it easy to spot cost spikes or slow models in the Langfuse dashboard.\n\n## Logging Scores and Feedback\n\nScores quantify quality, cost, and user satisfaction. Log them directly to traces:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_feedback_and_scores(trace, accepted: bool, helpfulness: float, grounded: bool, notes: str = \"\"):\n    trace.score(name=\"accepted\", value=1.0 if accepted else 0.0)\n    trace.score(name=\"helpfulness\", value=helpfulness)\n    trace.score(name=\"groundedness\", value=1.0 if grounded else 0.0, comment=notes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Combine heuristic scores (e.g., groundedness checks) with user feedback (thumbs up/down) to track quality over time. Filter traces by score in the UI to debug low-quality outputs.\n\n## Analyzing Traces in Production\n\nEnd-to-end traces reveal bottlenecks across retrieval, generation, and tool calls. Each span shows latency and errors. Capturing token usage per generation exposes cost spikes by model and prompt. With this, you'll reduce p95 latency and cut tokens where it matters, not by hunch. For a comprehensive guide on building RAG systems, see our [5 essential steps to building agentic RAG systems with LangChain and ChromaDB](/blog/44830763/5-essential-steps-to-building-agentic-rag-systems-with-langchain-and-chromadb).\n\nUse the Langfuse dashboard to:\n\n- Filter traces by user, route, or model\n- Aggregate latency and token usage by span\n- Correlate scores with trace metadata to identify patterns\n\nThis data-driven approach replaces guesswork with actionable insights.\n\n## Complete Example\n\nHere's a full implementation combining tracing, spans, and scoring:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport time\nfrom langfuse import Langfuse\nfrom openai import OpenAI\n\nlangfuse = Langfuse(\n    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n    host=os.getenv(\"LANGFUSE_HOST\"),\n)\n\nclient = OpenAI()\n\ndef start_trace(user_id: str, route: str, model: str):\n    trace = langfuse.trace(\n        name=\"chat_request\",\n        user_id=user_id,\n        metadata={\"route\": route, \"env\": os.getenv(\"APP_ENV\", \"dev\"), \"model\": model},\n    )\n    return trace\n\ndef with_span(trace, name: str):\n    span = trace.span(name=name)\n    start = time.time()\n    try:\n        yield span\n    finally:\n        span.end()\n\ndef call_llm_with_tracing(trace, messages, model=\"gpt-4o-mini\"):\n    gen = trace.generation(\n        name=\"chat.completions\",\n        model=model,\n        metadata={\"provider\": \"openai\"},\n        input=messages,\n    )\n    t0 = time.time()\n    resp = client.chat.completions.create(model=model, messages=messages)\n    t1 = time.time()\n\n    content = resp.choices[0].message.content\n    usage = {\"input\": resp.usage.prompt_tokens, \"output\": resp.usage.completion_tokens}\n\n    gen.end(\n        output=content,\n        usage=usage,\n        metadata={\"elapsed_ms\": int((t1 - t0) * 1000)},\n    )\n    return content, usage\n\ndef openai_messages(user_input: str, system_prompt: str):\n    return [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_input},\n    ]\n\ndef log_feedback_and_scores(trace, accepted: bool, helpfulness: float, grounded: bool, notes: str = \"\"):\n    trace.score(name=\"accepted\", value=1.0 if accepted else 0.0)\n    trace.score(name=\"helpfulness\", value=helpfulness)\n    trace.score(name=\"groundedness\", value=1.0 if grounded else 0.0, comment=notes)\n\n# Example usage\ntrace = start_trace(user_id=\"user_123\", route=\"/chat\", model=\"gpt-4o-mini\")\nmessages = openai_messages(\"What is RAG?\", \"You are a helpful assistant.\")\ncontent, usage = call_llm_with_tracing(trace, messages)\nlog_feedback_and_scores(trace, accepted=True, helpfulness=0.9, grounded=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run this code, then open the Langfuse dashboard to explore traces, spans, and scores.\n\n## Next Steps\n\nStart by instrumenting one critical pathâ€”your main chat endpoint or RAG pipeline. Add spans for each operation, log token usage, and capture user feedback. As you collect data, filter by latency or cost to find optimization opportunities.\n\nLangfuse integrates with LangChain, LlamaIndex, and other frameworks, so you can extend tracing across your entire stack. For production deployments, consider self-hosting Langfuse to retain full control over your observability data."
      ]
    }
  ],
  "metadata": {
    "title": "How to Use Langfuse Tracing for Prompts, Evals, and Cost Control",
    "description": "Instrument calls with Langfuse tracing to see traces/spans, version prompts safely, A/B on datasets, and tie evaluations to p95 latency, token cost, and acceptance-rate improvements.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}