{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Use Langfuse Tracing for Prompts, Evals, and Cost Control\n\n**Description:** Instrument calls with Langfuse tracing to see traces/spans, version prompts safely, A/B on datasets, and tie evaluations to p95 latency, token cost, and acceptance-rate improvements.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Use Langfuse for This Problem\n\nWhen building LLM-powered applications, you need visibility into model calls, prompt performance, and user feedback to iterate quickly. Langfuse provides a unified workflow for observability and evaluation that reduces glue work compared to alternatives:\n\n- **Unified model of traces, generations, and prompts** â€“ Unlike OpenTelemetry + custom logging or Helicone, Langfuse natively understands LLM-specific concepts (generations, prompt versions, scores) in a single platform, eliminating the need to stitch together separate tools.\n- **Prompt version pinning and A/B testing** â€“ LangSmith and Phoenix offer tracing, but Langfuse's Prompt Library lets you version, pin, and A/B test prompts directly via SDK, streamlining experimentation without external config management.\n- **Dataset-driven evaluation workflows** â€“ Langfuse supports running experiments on datasets and logging scores per trace, enabling CI-gated quality checks and regression prevention with minimal custom code.\n\nThis guide shows you how to instrument a question-answering (QA) endpoint end-to-end with Langfuse, capturing traces, spans, generations, scores, and prompt versions. You'll walk away with:\n\n- A fully traced QA flow (input parsing, retrieval, generation, feedback)\n- Prompt versioning via Langfuse's Prompt Library\n- A/B experiments on two prompt versions\n- A dataset evaluation loop with automated scoring\n- A CI gating snippet to prevent regressions\n\nBy the end, you'll track p95 latency, tokens per request, and user acceptance, enabling you to baseline quality, A/B test changes, and roll out improvements confidently.\n\n## Core Concepts for This Use Case\n\nLangfuse organizes observability around four primitives, each tied to a step in your QA workflow:\n\n- **Trace** â€“ A single user request (e.g., \"What is the warranty period?\"). Contains metadata (user ID, environment) and aggregates all nested activity.\n- **Span** â€“ A logical step within a trace (e.g., input parsing, retrieval). Use spans to measure latency and isolate bottlenecks.\n- **Generation** â€“ A model call (e.g., OpenAI completion). Langfuse logs input, output, model name, token usage, and latency automatically.\n- **Score** â€“ A quality or performance metric attached to a trace (e.g., user acceptance, groundedness heuristic, LLM-as-judge helpfulness). Scores enable dataset evaluation and A/B comparison.\n- **Prompt (via Prompt Library)** â€“ A versioned, reusable template fetched by label and version. Pin a version in production or fetch the latest in dev to control rollout and enable A/B tests.\n\nThese primitives map directly to the tutorial steps: you'll create a trace per request, add spans for parsing and retrieval, log a generation for the model call, attach scores for feedback and heuristics, and fetch prompts by version to run experiments.\n\n## Setup\n\nRun the following cell to install dependencies and configure environment variables. If you're in Colab without a `.env` file, set keys inline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q langfuse openai python-dotenv\n\nimport os\nfrom dotenv import load_dotenv\n\n# Load from .env if present (local), otherwise set inline (Colab)\nload_dotenv()\n\nif \"LANGFUSE_PUBLIC_KEY\" not in os.environ:\n    os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"  # Replace with your key\nif \"LANGFUSE_SECRET_KEY\" not in os.environ:\n    os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"  # Replace with your key\nif \"LANGFUSE_HOST\" not in os.environ:\n    os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # Or your self-hosted URL\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Replace with your OpenAI key\n\n# Optional: pin a specific environment and model\nos.environ[\"ENV\"] = os.getenv(\"ENV\", \"dev\")\nos.environ[\"OPENAI_MODEL\"] = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** You'll need a Langfuse account (free at [langfuse.com](https://langfuse.com)) and an OpenAI API key. If `gpt-4o-mini` is unavailable, replace with `gpt-3.5-turbo` or another accessible model.\n\nNext, ensure the `qa-answer` prompt exists in your Langfuse Prompt Library. Run this cell once to create and publish it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n\nlangfuse = Langfuse()\n\ntry:\n    # Check if prompt exists\n    langfuse.get_prompt(\"qa-answer\")\n    print(\"Prompt 'qa-answer' already exists.\")\nexcept Exception:\n    # Create and publish a minimal prompt\n    prompt_text = \"\"\"Answer the question using only the context below. Be concise.\n\nContext:\n{{context}}\n\nQuestion: {{question}}\n\nAnswer:\"\"\"\n    \n    langfuse.create_prompt(\n        name=\"qa-answer\",\n        prompt=prompt_text,\n        is_active=True,  # Publish immediately\n    )\n    print(\"Created and published prompt 'qa-answer'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the prompt already exists, this cell will skip creation. You can now fetch it by label in the tutorial code.\n\n## Using the Tool in Practice\n\n### Step 1: Initialize Clients and Configure Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\nfrom openai import OpenAI\nfrom time import perf_counter\n\nlangfuse = Langfuse()\nopenai_client = OpenAI()\n\nENV = os.getenv(\"ENV\", \"dev\")\nDEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Define a Retrieval Function\n\nReplace this stub with your actual retriever (vector DB, keyword search, etc.). Keep it deterministic for reproducible tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_docs(query: str) -> list[str]:\n    \"\"\"Retrieve documents related to the query.\"\"\"\n    return [f\"Doc about: {query}\", \"Another relevant doc.\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Start a Trace with Parsing and Retrieval Spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def start_trace(user_id: str, question: str):\n    \"\"\"Create a trace and log input parsing + retrieval spans.\"\"\"\n    trace = langfuse.trace(\n        name=\"qa_request\",\n        user_id=user_id,\n        input={\"question\": question},\n        metadata={\"env\": ENV, \"component\": \"qa-service\"},\n    )\n    \n    # Span: parse input\n    parse = trace.span(name=\"parse_input\", input={\"raw\": question})\n    normalized = question.strip()\n    parse.end(output={\"normalized\": normalized})\n    \n    # Span: retrieve documents\n    ret = trace.span(name=\"retrieval\", input={\"query\": normalized})\n    docs = retrieve_docs(normalized)\n    ret.end(output={\"docs_count\": len(docs)})\n    \n    return trace, normalized, docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Generate an Answer and Log the Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(trace, model: str, prompt_text: str):\n    \"\"\"Call the model and log generation details.\"\"\"\n    gen_span = trace.span(name=\"generation.prepare_prompt\", input={\"prompt_preview\": prompt_text[:200]})\n    \n    t0 = perf_counter()\n    response = openai_client.chat.completions.create(\n        model=model,\n        temperature=0.2,\n        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n    )\n    latency_ms = (perf_counter() - t0) * 1000\n    gen_span.end(output={\"latency_ms\": latency_ms})\n    \n    content = response.choices[0].message.content\n    usage = getattr(response, \"usage\", None)\n    \n    generation = trace.generation(\n        name=\"answer_generation\",\n        model=response.model,\n        input=prompt_text,\n        output=content,\n        metadata={\"provider\": \"openai\", \"latency_ms\": latency_ms},\n        usage={\n            \"input_tokens\": getattr(usage, \"prompt_tokens\", None),\n            \"output_tokens\": getattr(usage, \"completion_tokens\", None),\n            \"total_tokens\": getattr(usage, \"total_tokens\", None),\n        },\n    )\n    generation.end()\n    trace.update(output={\"answer\": content})\n    return content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note on token costs:** If `usage` is missing (some providers don't return it), token counts will be `None`. For cost estimation, use a fallback tokenizer (e.g., `tiktoken`) or log a warning and track only latency.\n\n### Step 5: Record Feedback and Heuristic Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def record_feedback(trace, accepted: bool, question: str, answer: str, docs: list[str]):\n    \"\"\"Attach user feedback and heuristic scores to the trace.\"\"\"\n    trace.score(name=\"acceptance\", value=1.0 if accepted else 0.0, comment=\"user_feedback\")\n    \n    # Heuristic: does answer cite a document term?\n    hit = any(term.lower() in answer.lower() for term in set(\" \".join(docs).split()) if len(term) > 5)\n    trace.score(name=\"groundedness_heuristic\", value=1.0 if hit else 0.0)\n    \n    # Heuristic: brevity penalty\n    brevity = 1.0 if len(answer) < 800 else 0.0\n    trace.score(name=\"brevity_ok\", value=brevity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Fetch and Compile Prompts from the Prompt Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_compiled_prompt(label: str, variables: dict, pinned_version: str | None = None) -> str:\n    \"\"\"Fetch a prompt by label and version, then compile with variables.\"\"\"\n    if pinned_version:\n        prompt = langfuse.get_prompt(label, version=pinned_version)\n    else:\n        prompt = langfuse.get_prompt(label)  # Latest published\n    return prompt.compile(variables)\n\ndef build_prompt(question: str, docs: list[str]) -> str:\n    \"\"\"Build the QA prompt, optionally pinning a version.\"\"\"\n    version_pin = os.getenv(\"PROMPT_VERSION_PIN\") or None\n    return get_compiled_prompt(\n        \"qa-answer\",\n        {\"question\": question, \"context\": \"\\n\\n\".join(docs)},\n        pinned_version=version_pin,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Add LLM-as-Judge Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def judge_helpfulness(trace, question: str, answer: str):\n    \"\"\"Use an LLM to score answer helpfulness.\"\"\"\n    judge_prompt = f\"Rate helpfulness 0-1 for the answer given the question.\\nQuestion: {question}\\nAnswer: {answer}\\nRespond with a number.\"\n    \n    t0 = perf_counter()\n    judge_resp = openai_client.chat.completions.create(\n        model=DEFAULT_MODEL,\n        temperature=0,\n        messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n    )\n    latency_ms = (perf_counter() - t0) * 1000\n    judge_text = judge_resp.choices[0].message.content.strip()\n    \n    try:\n        score_val = float(judge_text.split()[0])\n    except Exception:\n        score_val = 0.0\n    \n    judge_gen = trace.generation(\n        name=\"judge_helpfulness\",\n        model=judge_resp.model,\n        input=judge_prompt,\n        output=judge_text,\n        metadata={\"provider\": \"openai\", \"latency_ms\": latency_ms},\n        usage={\n            \"input_tokens\": getattr(judge_resp.usage, \"prompt_tokens\", None),\n            \"output_tokens\": getattr(judge_resp.usage, \"completion_tokens\", None),\n        },\n    )\n    judge_gen.end()\n    trace.score(name=\"helpfulness_llm\", value=score_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8: Combine Steps into a Single Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer_and_score(user_id: str, question: str, accept: bool | None = None):\n    \"\"\"Generate an answer, record feedback, and judge helpfulness.\"\"\"\n    trace, normalized, docs = start_trace(user_id, question)\n    prompt_text = build_prompt(normalized, docs)\n    answer = generate_answer(trace, DEFAULT_MODEL, prompt_text)\n    record_feedback(trace, accepted=bool(accept), question=normalized, answer=answer, docs=docs)\n    judge_helpfulness(trace, question=normalized, answer=answer)\n    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Evaluate\n\n### Run the QA Flow\n\nExecute the flow a few times and inspect the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(answer_and_score(\"u1\", \"What is the warranty period for the Pro plan?\", accept=True))\nprint(answer_and_score(\"u2\", \"How do I reset my password?\", accept=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the Langfuse dashboard to see traces, spans, generations, and scores.\n\n### A/B Test Two Prompt Versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_ab(user_id: str, question: str, version_a: str, version_b: str):\n    \"\"\"Run an A/B test on two prompt versions.\"\"\"\n    results = []\n    for variant, v in [(\"A\", version_a), (\"B\", version_b)]:\n        trace, normalized, docs = start_trace(user_id, question)\n        trace.update(metadata={\"experiment\": \"qa_prompt_ab:v1\", \"variant\": variant})\n        prompt_text = get_compiled_prompt(\"qa-answer\", {\"question\": normalized, \"context\": \"\\n\\n\".join(docs)}, v)\n        answer = generate_answer(trace, DEFAULT_MODEL, prompt_text)\n        \n        # Simple auto-score: does answer contain first question keyword?\n        score = 1.0 if normalized.lower().split()[0] in answer.lower() else 0.0\n        trace.score(name=\"keyword_hit\", value=score)\n        results.append((variant, answer, trace))\n    return results\n\n# Run A/B test (replace \"3\" and \"4\" with actual prompt version numbers from your Prompt Library)\nab = run_ab(\"u3\", \"Do you support SSO?\", version_a=\"1\", version_b=\"1\")\nfor variant, ans, _ in ab:\n    print(f\"{variant}: {ans[:120]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** Create version 2 of the `qa-answer` prompt in the Langfuse UI by editing and publishing a new version, then update `version_a` and `version_b` accordingly.\n\n### Evaluate on a Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def exact_match(answer: str, ground_truth: str) -> float:\n    \"\"\"Check if answer exactly matches ground truth (case-insensitive).\"\"\"\n    return 1.0 if answer.strip().lower() == ground_truth.strip().lower() else 0.0\n\ndef contains_any(answer: str, ground_truth: str) -> float:\n    \"\"\"Check if answer contains any keyword from ground truth.\"\"\"\n    keywords = ground_truth.lower().split()\n    return 1.0 if any(kw in answer.lower() for kw in keywords) else 0.0\n\ndef evaluate_dataset(dataset: list[dict], version_a: str, version_b: str):\n    \"\"\"Evaluate a dataset using two prompt versions.\"\"\"\n    for item in dataset:\n        for variant, v in [(\"A\", version_a), (\"B\", version_b)]:\n            trace, normalized, docs = start_trace(user_id=item[\"id\"], question=item[\"question\"])\n            trace.update(metadata={\"experiment\": \"qa_dataset_eval:v1\", \"variant\": variant, \"dataset_id\": \"qa_sanity\"})\n            prompt_text = get_compiled_prompt(\"qa-answer\", {\"question\": normalized, \"context\": \"\\n\\n\".join(docs)}, v)\n            answer = generate_answer(trace, DEFAULT_MODEL, prompt_text)\n            trace.score(name=\"accuracy_exact\", value=exact_match(answer, item[\"ground_truth\"]))\n            trace.score(name=\"relevancy_contains\", value=contains_any(answer, item[\"ground_truth\"]))\n\n# Example dataset\ndataset = [\n    {\"id\": \"q1\", \"question\": \"What is the warranty period?\", \"ground_truth\": \"one year\"},\n    {\"id\": \"q2\", \"question\": \"How do I reset my password?\", \"ground_truth\": \"click forgot password\"},\n]\n\nevaluate_dataset(dataset, version_a=\"1\", version_b=\"1\")\nprint(\"Dataset evaluation complete. Check Langfuse for scores.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fetch and Aggregate Scores Locally (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flush pending traces to Langfuse\nlangfuse.flush()\n\n# Note: Fetching traces via API requires additional setup (API client, filtering by metadata).\n# For now, inspect aggregated scores in the Langfuse dashboard under Experiments or Datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With trace data, you can baseline quality and cost, A/B prompt versions on the same inputs, and roll out changes confidently. Iteration loops shrink: diagnose, tweak, re-run, compare. Expect smoother rollouts, lower p95 latency, and controlled token spend as you tighten the feedback loop. For a deeper understanding of how to develop Retrieval-Augmented Generation systems, you might find our guide on [building agentic RAG systems with LangChain and ChromaDB](/article/44830763/5-essential-steps-to-building-agentic-rag-systems-with-langchain-and-chromadb) helpful.\n\n## Production Considerations\n\n### Overhead and Sampling Strategy\n\nLangfuse batches trace data and flushes asynchronously, adding minimal latency (<10ms per trace in most cases). For high-throughput production services:\n\n- **Dev/staging:** Sample 100% of traces (`sample_rate=1.0`) to catch all issues.\n- **Production:** Sample 5â€“10% (`sample_rate=0.05`) to balance cost and visibility. Adjust based on traffic volume and budget.\n- **Flush on shutdown:** Call `langfuse.flush()` before process exit to ensure all traces are sent.\n\n### Error Handling and Retries\n\nWrap model calls in try/except blocks and log errors as spans:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n    response = openai_client.chat.completions.create(...)\nexcept Exception as e:\n    error_span = trace.span(name=\"generation_error\", metadata={\"error\": str(e)})\n    error_span.end()\n    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For transient failures (rate limits, timeouts), implement exponential backoff and log retry attempts as events:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n\nfor attempt in range(3):\n    try:\n        response = openai_client.chat.completions.create(...)\n        break\n    except Exception as e:\n        trace.event(name=\"retry\", metadata={\"attempt\": attempt, \"error\": str(e)})\n        time.sleep(2 ** attempt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CI Gating with Score Thresholds\n\nPrevent regressions by gating deployments on dataset evaluation scores. Example CI snippet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation and fetch mean acceptance score via Langfuse API\npython evaluate.py --dataset qa_sanity --version $NEW_VERSION\nSCORE=$(curl -X GET \"https://cloud.langfuse.com/api/public/scores?dataset=qa_sanity&version=$NEW_VERSION\" \\\n  -H \"Authorization: Bearer $LANGFUSE_SECRET_KEY\" | jq '.mean_acceptance')\n\nif (( $(echo \"$SCORE < 0.8\" | bc -l) )); then\n  echo \"Score $SCORE below threshold 0.8. Failing build.\"\n  exit 1\nfi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adjust thresholds based on your quality requirements.\n\n## Conclusion\n\nYou've built a fully instrumented QA endpoint with Langfuse, capturing traces, spans, generations, and scores at every step. You can now:\n\n- Track p95 latency, token usage, and user acceptance in real time\n- Version and A/B test prompts via the Prompt Library\n- Evaluate changes on datasets with automated scoring\n- Gate CI/CD pipelines on quality thresholds to prevent regressions\n\nThis workflow scales from prototyping to production, enabling you to iterate faster, ship confidently, and maintain quality as your LLM app grows.\n\n**Next steps:**\n\n- Explore advanced prompt engineering techniques and version multiple prompts for different use cases\n- Integrate Langfuse with LangChain or LlamaIndex for deeper callback instrumentation (covered in separate guides)\n- Set up alerts in Langfuse to notify your team when latency or acceptance drops below thresholds"
      ]
    }
  ],
  "metadata": {
    "title": "How to Use Langfuse Tracing for Prompts, Evals, and Cost Control",
    "description": "Instrument calls with Langfuse tracing to see traces/spans, version prompts safely, A/B on datasets, and tie evaluations to p95 latency, token cost, and acceptance-rate improvements.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}