{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Self-Host a Fast Llama 3 API Locally with vLLM and FastAPI\n\n**Description:** Ship a production-grade Llama 3 API: vLLM throughput with batching/KV cache, FastAPI gateway for auth, rate limits, streaming, and benchmarks.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What You'll Build\n\nA production-ready API gateway that sits in front of vLLM, exposing an OpenAI-compatible `/v1/chat/completions` endpoint with per-key rate limiting, Prometheus metrics, and streaming support. By the end, you'll be able to swap your OpenAI base URL, authenticate with a custom key, and stream tokens from a locally hosted Llama 3 modelâ€”while controlling cost and observability.\n\n**Prerequisites:**\n- CUDA-capable GPU (16GB+ VRAM recommended for Llama 3 8B)\n- Access to Meta Llama 3 on Hugging Face (gated model; request access at [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct))\n- Hugging Face token with read permissions (set as `HUGGING_FACE_HUB_TOKEN`)\n- Python 3.10+\n\n**Acceptance Criteria:**\n- `/v1/chat/completions` returns streaming or non-streaming responses matching OpenAI's schema\n- API keys enforce per-key rate limits (e.g., 10 req/min)\n- Prometheus `/metrics` endpoint exposes request counts, latencies, and token throughput\n- Clients can replace `openai.api_base` and `Authorization` header to use your gateway with zero code changes\n\n---\n\n## How It Works (High-Level Overview)\n\n**vLLM** serves the model with continuous batching and PagedAttention for high throughput. **FastAPI** wraps vLLM's HTTP API, adding:\n- **Authentication** via `Authorization: Bearer <key>` (with optional `x-api-key` fallback)\n- **Rate limiting** per key using Redis and SlowAPI\n- **Metrics** via Prometheus for request counts, latencies, and tokens/sec\n- **OpenAI schema compatibility** so existing clients work without modification\n\n**Flow:**\n1. Client sends POST to `/v1/chat/completions` with `Authorization: Bearer sk-abc123`\n2. Gateway validates key, checks rate limit in Redis\n3. If allowed, forwards request to vLLM's `/v1/chat/completions`\n4. Streams or returns JSON response, increments Prometheus counters\n5. Logs request ID, hashed key, and latency for traceability\n\n---\n\n## Setup & Installation\n\n### 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install vllm fastapi uvicorn[standard] httpx redis slowapi prometheus-client pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Start Redis (for rate limiting)\n\nIn a notebook or local environment, start Redis in the background:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\nimport time\n\nredis_proc = subprocess.Popen([\"redis-server\", \"--port\", \"6379\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\ntime.sleep(2)  # wait for Redis to start\nprint(\"Redis started on port 6379\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Seed API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import redis\nr = redis.Redis(host=\"localhost\", port=6379, decode_responses=True)\nr.sadd(\"valid_api_keys\", \"sk-test-key-1\")\nr.sadd(\"valid_api_keys\", \"sk-test-key-2\")\nprint(\"Seeded keys: sk-test-key-1, sk-test-key-2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Start vLLM Server\n\nEnsure `HUGGING_FACE_HUB_TOKEN` is set in your environment, then start vLLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nos.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"hf_...\"  # replace with your token\n\nvllm_proc = subprocess.Popen([\n    \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n    \"--model\", \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    \"--port\", \"8000\",\n    \"--max-model-len\", \"4096\"\n], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\ntime.sleep(30)  # wait for model to load\nprint(\"vLLM server started on port 8000\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step-by-Step Implementation\n\n### Step 1: Define Pydantic Schemas\n\nCreate `app/schemas.py` to validate incoming requests and allow unknown fields for full OpenAI compatibility:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field, ConfigDict\nfrom typing import List, Optional, Literal\nfrom enum import Enum\n\nclass Role(str, Enum):\n    system = \"system\"\n    user = \"user\"\n    assistant = \"assistant\"\n    tool = \"tool\"\n\nclass Message(BaseModel):\n    role: Role\n    content: str = Field(..., max_length=32000)\n\nclass ChatCompletionRequest(BaseModel):\n    model_config = ConfigDict(extra=\"allow\")  # pass unknown fields to vLLM\n    model: str\n    messages: List[Message]\n    temperature: Optional[float] = 0.7\n    max_tokens: Optional[int] = 512\n    stream: Optional[bool] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Build the FastAPI Gateway\n\nCreate `app/main.py`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import hashlib\nimport uuid\nimport httpx\nimport redis\nfrom fastapi import FastAPI, Request, Header, HTTPException, Depends\nfrom fastapi.responses import StreamingResponse, JSONResponse\nfrom slowapi import Limiter\nfrom slowapi.util import get_remote_address\nfrom slowapi.errors import RateLimitExceeded\nfrom prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST\nfrom app.schemas import ChatCompletionRequest\n\napp = FastAPI()\n\n# Redis client\nr = redis.Redis(host=\"localhost\", port=6379, decode_responses=True)\n\n# Rate limiter\nlimiter = Limiter(key_func=get_remote_address, storage_uri=\"redis://localhost:6379\")\napp.state.limiter = limiter\n\n@app.exception_handler(RateLimitExceeded)\nasync def rate_limit_handler(request: Request, exc: RateLimitExceeded):\n    return JSONResponse(\n        status_code=429,\n        content={\"error\": {\"message\": \"Rate limit exceeded\", \"type\": \"rate_limit_error\"}},\n        headers={\"Retry-After\": \"60\"}\n    )\n\n# Prometheus metrics\nREQUEST_COUNT = Counter(\"gateway_requests_total\", \"Total requests\", [\"endpoint\", \"status\"])\nREQUEST_LATENCY = Histogram(\"gateway_request_duration_seconds\", \"Request latency\", [\"endpoint\"])\nTOKENS_GENERATED = Counter(\"gateway_tokens_generated_total\", \"Total tokens generated\")\n\n# Auth dependency\nasync def verify_api_key(authorization: Optional[str] = Header(None), x_api_key: Optional[str] = Header(None)):\n    key = None\n    if authorization and authorization.startswith(\"Bearer \"):\n        key = authorization.split(\" \", 1)[1]\n    elif x_api_key:\n        key = x_api_key\n    if not key or not r.sismember(\"valid_api_keys\", key):\n        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n    return key\n\n@app.post(\"/v1/chat/completions\")\n@limiter.limit(\"10/minute\")\nasync def chat_completions(\n    request: Request,\n    body: ChatCompletionRequest,\n    api_key: str = Depends(verify_api_key)\n):\n    request_id = str(uuid.uuid4())\n    key_hash = hashlib.sha256(api_key.encode()).hexdigest()[:8]\n    \n    with REQUEST_LATENCY.labels(endpoint=\"/v1/chat/completions\").time():\n        try:\n            async with httpx.AsyncClient(timeout=60.0) as client:\n                if body.stream:\n                    upstream = await client.post(\n                        \"http://localhost:8000/v1/chat/completions\",\n                        json=body.model_dump(),\n                        headers={\"Content-Type\": \"application/json\"}\n                    )\n                    upstream.raise_for_status()\n                    \n                    async def stream_response():\n                        seen_done = False\n                        async for line in upstream.aiter_lines():\n                            if line.startswith(\"data: \"):\n                                chunk = line[6:]\n                                if chunk.strip() == \"[DONE]\":\n                                    seen_done = True\n                                yield line + \"\\n\\n\"\n                        if not seen_done:\n                            yield \"data: [DONE]\\n\\n\"\n                    \n                    REQUEST_COUNT.labels(endpoint=\"/v1/chat/completions\", status=200).inc()\n                    return StreamingResponse(\n                        stream_response(),\n                        media_type=\"text/event-stream\",\n                        headers={\n                            \"Cache-Control\": \"no-cache\",\n                            \"Connection\": \"keep-alive\",\n                            \"X-Accel-Buffering\": \"no\"\n                        }\n                    )\n                else:\n                    resp = await client.post(\n                        \"http://localhost:8000/v1/chat/completions\",\n                        json=body.model_dump(),\n                        headers={\"Content-Type\": \"application/json\"}\n                    )\n                    resp.raise_for_status()\n                    data = resp.json()\n                    tokens = data.get(\"usage\", {}).get(\"completion_tokens\", 0)\n                    TOKENS_GENERATED.inc(tokens)\n                    REQUEST_COUNT.labels(endpoint=\"/v1/chat/completions\", status=200).inc()\n                    print(f\"[{request_id}] key={key_hash} tokens={tokens}\")\n                    return data\n        except httpx.HTTPStatusError as e:\n            REQUEST_COUNT.labels(endpoint=\"/v1/chat/completions\", status=e.response.status_code).inc()\n            raise HTTPException(status_code=e.response.status_code, detail=e.response.text)\n\n@app.get(\"/v1/models\")\nasync def list_models(api_key: str = Depends(verify_api_key)):\n    async with httpx.AsyncClient() as client:\n        resp = await client.get(\"http://localhost:8000/v1/models\")\n        resp.raise_for_status()\n        return resp.json()\n\n@app.get(\"/healthz\")\nasync def health():\n    try:\n        async with httpx.AsyncClient(timeout=5.0) as client:\n            resp = await client.get(\"http://localhost:8000/v1/models\")\n            resp.raise_for_status()\n        r.ping()\n        return {\"status\": \"ok\"}\n    except Exception as e:\n        raise HTTPException(status_code=503, detail=str(e))\n\n@app.get(\"/metrics\")\nasync def metrics():\n    return generate_latest(), 200, {\"Content-Type\": CONTENT_TYPE_LATEST}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Start the Gateway"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uvicorn\nfrom threading import Thread\n\ndef run_gateway():\n    uvicorn.run(\"app.main:app\", host=\"0.0.0.0\", port=8001, workers=1)\n\ngateway_thread = Thread(target=run_gateway, daemon=True)\ngateway_thread.start()\ntime.sleep(3)\nprint(\"Gateway running on port 8001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Validation & Testing\n\n### Test Non-Streaming Request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import httpx\n\nasync def test_chat():\n    async with httpx.AsyncClient() as client:\n        resp = await client.post(\n            \"http://localhost:8001/v1/chat/completions\",\n            json={\n                \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Say hello\"}],\n                \"max_tokens\": 50\n            },\n            headers={\"Authorization\": \"Bearer sk-test-key-1\"}\n        )\n        print(resp.status_code, resp.json())\n\nimport asyncio\nasyncio.run(test_chat())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Streaming Request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def test_stream():\n    async with httpx.AsyncClient() as client:\n        async with client.stream(\n            \"POST\",\n            \"http://localhost:8001/v1/chat/completions\",\n            json={\n                \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Count to 5\"}],\n                \"stream\": True\n            },\n            headers={\"Authorization\": \"Bearer sk-test-key-1\"}\n        ) as resp:\n            async for line in resp.aiter_lines():\n                print(line)\n\nasyncio.run(test_stream())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check Prometheus Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def check_metrics():\n    async with httpx.AsyncClient() as client:\n        resp = await client.get(\"http://localhost:8001/metrics\")\n        print(resp.text)\n\nasyncio.run(check_metrics())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Benchmarking Throughput\n\nCreate `bench.py` to measure time-to-first-token (TTFT) and tokens/sec:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import httpx\nimport time\nimport asyncio\n\nasync def bench():\n    async with httpx.AsyncClient(timeout=60.0) as client:\n        start = time.time()\n        first_token_time = None\n        token_count = 0\n        \n        async with client.stream(\n            \"POST\",\n            \"http://localhost:8001/v1/chat/completions\",\n            json={\n                \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a 200-word essay on AI\"}],\n                \"stream\": True,\n                \"max_tokens\": 300\n            },\n            headers={\"Authorization\": \"Bearer sk-test-key-1\"}\n        ) as resp:\n            async for line in resp.aiter_lines():\n                if first_token_time is None:\n                    first_token_time = time.time() - start\n                if line.startswith(\"data: \") and line[6:].strip() != \"[DONE]\":\n                    token_count += 1\n        \n        total_time = time.time() - start\n        print(f\"TTFT: {first_token_time:.2f}s | Total: {total_time:.2f}s | Tokens/sec: {token_count/total_time:.1f}\")\n\nasyncio.run(bench())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example output:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TTFT: 0.32s | Total: 12.45s | Tokens/sec: 24.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Next Steps\n\n- **Multi-GPU scaling:** Use `--tensor-parallel-size 2` in vLLM for larger models\n- **Quantization:** Add `--quantization awq` to reduce memory and increase throughput\n- **Key management API:** Build a `/admin/keys` endpoint to create/revoke keys programmatically\n- **CORS for web clients:** Add `fastapi.middleware.cors.CORSMiddleware` with an allowlist\n- **TLS termination:** Deploy behind NGINX or Traefik with Let's Encrypt certificates\n- **Multiprocess metrics:** If using `workers > 1`, configure `prometheus_client` multiprocess mode\n\nYou now have a production-ready gateway that lets you replace OpenAI with a self-hosted model, control costs via rate limits, and monitor performance with Prometheusâ€”all while maintaining drop-in compatibility with existing clients."
      ]
    }
  ],
  "metadata": {
    "title": "How to Self-Host a Fast Llama 3 API Locally with vLLM and FastAPI",
    "description": "Ship a production-grade Llama 3 API: vLLM throughput with batching/KV cache, FastAPI gateway for auth, rate limits, streaming, and benchmarks.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}