{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build a Local Audio Meeting Copilot with Open-Source LLMs\n\n**Description:** Build a privacy-first meeting copilot that records audio, diarizes speakers, transcribes locally, then generates summaries and action items using open-source LLMs. Follow a runnable, end-to-end project that turns meeting audio into speaker-tagged transcripts, concise summaries, and task lists, no cloud, no data leakage.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Meetings generate valuable information, but capturing it without sending audio to third-party services is a real challenge. Teams in regulated industries, privacy-conscious organizations, or environments with sensitive discussions need a solution that keeps everything local. This guide walks you through building a privacy-first meeting copilot that records audio, identifies speakers, transcribes speech, and generates summaries and action items using only open-source models running on your own hardware.\n\nBy the end, you'll have a working system that processes live microphone input or recorded files, produces speaker-tagged transcripts, and extracts structured meeting outputs. You'll learn how to stream and segment audio with voice activity detection, run automatic speech recognition with word-level timestamps, perform speaker diarization, align speakers to transcript segments, and prompt a local LLM to generate summaries and action items with a strict JSON schema. You'll also understand the trade-offs in model selection, hardware requirements, and how to validate and extend the pipeline for production use.\n\n## What You'll Build and Why It Matters\n\nYou'll build a complete meeting copilot pipeline that runs entirely on your infrastructure. The system captures audio from a microphone or file, segments speech using voice activity detection, transcribes each segment with Whisper, labels speakers with Pyannote diarization, and sends the formatted transcript to a local LLM for summarization and task extraction. All artifacts, including audio, transcripts, and metadata, are saved as structured JSON for review and reproducibility.\n\nRunning locally means your raw audio never leaves your machine, and that is the big deal for sensitive meetings. It also means no per-minute fees and no vendor lock-in. The practical outcome is you can run this on developer workstations, on a small server, or on an isolated machine in regulated environments. For a step-by-step walkthrough on deploying your own language models securely, see our [practical guide to running a self-hosted LLM on your server](/article/how-to-run-a-self-hosted-llm-on-your-server-practical-guide-2025-2).\n\nThe end state is a Python pipeline that produces:\n\n- WAV files for each speech segment with unique IDs.\n- Speaker-tagged transcripts with timestamps.\n- Structured JSON output containing summary, decisions, and action items.\n- Full meeting artifacts saved for audit and review.\n\nYou'll be able to run this in two modes: live capture from a microphone or batch processing of recorded audio files. The pipeline is modular, so you can swap models, adjust parameters, and extend functionality without rewriting core logic.\n\n### Minimum Hardware and Dependencies\n\nYou need Python 3.8 or later, a working microphone or audio file, and enough compute to run inference. For CPU-only setups, use Whisper small with int8 quantization and expect slower-than-realtime transcription. For GPU setups, an NVIDIA GPU with 8GB VRAM will handle Whisper medium and diarization comfortably with float16 precision. Diarization benefits significantly from GPU acceleration.\n\nInstall dependencies with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install pyaudio webrtcvad faster-whisper pyannote.audio requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyAudio requires PortAudio system libraries. On Ubuntu, install with `apt-get install portaudio19-dev`. On macOS, use `brew install portaudio`. On Windows, PyAudio wheels are available via pip.\n\nPyannote diarization models are gated on Hugging Face and require a token for initial download. Accept the model terms at https://huggingface.co/pyannote/speaker-diarization and set your token with `export HF_TOKEN=your_token`. For air-gapped or offline environments, download model weights once and cache them locally by setting `HF_HOME` to a persistent directory.\n\nFor the LLM, install Ollama from https://ollama.ai/ and pull a model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ollama pull mistral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ollama runs a local API server at `http://localhost:11434`. You can swap models by changing the model name in the pipeline.\n\n### Core Privacy and Compliance Win\n\nAll processing happens on your hardware. Audio is never uploaded, transcripts are never sent to external APIs, and you control retention policies. This architecture supports GDPR, HIPAA, and other regulatory requirements by design. You can encrypt stored artifacts, configure data retention, and run the pipeline on isolated networks without internet access after initial model downloads.\n\n## System Architecture and Component Flow\n\nThe pipeline is organized into four stages with explicit inputs and outputs. Each stage is independent, making it easy to test, swap models, and optimize performance.\n\n**Stage 1: Audio Capture and Segmentation**\n\nInput: Live microphone stream or WAV file.\nOutput: PCM audio segments representing speech intervals.\n\nThe system reads audio frames and uses WebRTC VAD to detect speech. Non-speech frames are discarded. Speech frames are buffered with padding and yielded as segments when silence is detected or a maximum length is reached. This reduces downstream compute by processing only spoken audio.\n\n**Stage 2: Automatic Speech Recognition**\n\nInput: WAV file for each segment.\nOutput: Transcript with word-level timestamps and detected language.\n\nFaster-whisper runs Whisper models efficiently with CTranslate2. Each segment is transcribed independently, producing text and timing information. Word-level timestamps enable precise alignment with diarization.\n\n**Stage 3: Speaker Diarization and Alignment**\n\nInput: Same WAV file used for ASR.\nOutput: Speaker labels assigned to transcript segments.\n\nPyannote diarization identifies speaker turns with start and end times. The alignment function matches ASR segments to speaker turns based on maximum time overlap. Each transcript segment is tagged with a speaker ID.\n\n**Stage 4: LLM Post-Processing**\n\nInput: Full speaker-tagged transcript.\nOutput: Structured JSON with summary, decisions, and action items.\n\nThe transcript is formatted and sent to a local LLM via Ollama. A strict JSON schema enforces output structure. The LLM extracts key information and returns it as parseable JSON.\n\nTreat the copilot as four stages with explicit inputs and outputs. Audio capture produces PCM frames. VAD produces speech segments. ASR plus diarization produces a speaker-labeled transcript. LLM post-processing produces summary and tasks. Keeping these boundaries clean is what makes it easy to swap models and tune performance. If you're interested in customizing your models efficiently, our [hands-on guide to parameter-efficient fine-tuning with LoRA](/article/parameter-efficient-fine-tuning-peft-with-lora-2025-hands-on-guide-2) covers how to adapt large language models on a single GPU.\n\n## Step-by-Step Implementation\n\n### Step 1: Stream and Segment Audio with VAD\n\nThis code captures audio from the microphone and segments it into speech intervals using WebRTC VAD. The generator yields raw PCM bytes for each detected speech segment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import collections\nimport time\nimport wave\nimport pyaudio\nimport webrtcvad\n\nRATE = 16000\nCHANNELS = 1\nFORMAT = pyaudio.paInt16\nFRAME_MS = 30\nFRAME_SAMPLES = int(RATE * FRAME_MS / 1000)\nFRAME_BYTES = FRAME_SAMPLES * 2\n\nvad = webrtcvad.Vad(2)\n\ndef frames_from_mic():\n    \"\"\"\n    Generator that yields raw audio frames from the default microphone.\n\n    Yields:\n        bytes: Raw PCM audio frame of size FRAME_BYTES.\n    \"\"\"\n    pa = pyaudio.PyAudio()\n    stream = pa.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True,\n                     frames_per_buffer=FRAME_SAMPLES)\n    try:\n        while True:\n            data = stream.read(FRAME_SAMPLES, exception_on_overflow=False)\n            yield data\n    finally:\n        stream.stop_stream()\n        stream.close()\n        pa.terminate()\n\ndef segment_speech(frame_iter, padding_ms=300, max_segment_s=15):\n    \"\"\"\n    Segments incoming audio frames into speech segments using VAD.\n\n    Args:\n        frame_iter (iterable): Iterator yielding raw PCM audio frames.\n        padding_ms (int): Amount of padding (in ms) to include before/after speech.\n        max_segment_s (int): Maximum segment length in seconds to avoid long latency.\n\n    Yields:\n        bytes: Concatenated PCM bytes representing a speech segment.\n    \"\"\"\n    num_padding = int(padding_ms / FRAME_MS)\n    ring = collections.deque(maxlen=num_padding)\n    triggered = False\n    voiced = []\n    segment_start = time.time()\n\n    for frame in frame_iter:\n        is_speech = vad.is_speech(frame, RATE)\n\n        if not triggered:\n            ring.append((frame, is_speech))\n            if sum(1 for _, s in ring if s) > 0.8 * ring.maxlen:\n                triggered = True\n                voiced.extend([f for f, _ in ring])\n                ring.clear()\n                segment_start = time.time()\n        else:\n            voiced.append(frame)\n            ring.append((frame, is_speech))\n            segment_age = time.time() - segment_start\n            if segment_age > max_segment_s or sum(1 for _, s in ring if s) < 0.2 * ring.maxlen:\n                yield b\"\".join(voiced)\n                triggered = False\n                ring.clear()\n                voiced = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "VAD aggressiveness ranges from 0 to 3. Mode 2 balances sensitivity and false positives. Adjust `padding_ms` to include context around speech and `max_segment_s` to control latency. Shorter segments reduce wait time but increase processing overhead.\n\n### Step 2: Save Segments as WAV Files\n\nEach speech segment is saved as a uniquely named WAV file for downstream processing. This allows ASR and diarization to run on stable file inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport uuid\nimport wave\n\ndef write_wav(pcm_bytes, path, rate=16000):\n    \"\"\"\n    Write raw PCM bytes to a WAV file.\n\n    Args:\n        pcm_bytes (bytes): Raw PCM audio data.\n        path (str): Output file path.\n        rate (int): Sample rate in Hz.\n\n    Returns:\n        None\n    \"\"\"\n    with wave.open(path, \"wb\") as wf:\n        wf.setnchannels(1)\n        wf.setsampwidth(2)\n        wf.setframerate(rate)\n        wf.writeframes(pcm_bytes)\n\ndef save_segment(pcm_bytes, out_dir=\"segments\"):\n    \"\"\"\n    Save a PCM audio segment as a uniquely named WAV file.\n\n    Args:\n        pcm_bytes (bytes): Raw PCM audio data.\n        out_dir (str): Directory to save WAV files.\n\n    Returns:\n        tuple: (segment_id, file_path)\n    \"\"\"\n    os.makedirs(out_dir, exist_ok=True)\n    seg_id = str(uuid.uuid4())\n    path = os.path.join(out_dir, f\"{seg_id}.wav\")\n    write_wav(pcm_bytes, path)\n    return seg_id, path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unique IDs ensure segments don't overwrite each other and make it easy to trace outputs back to source audio.\n\n### Step 3: Transcribe with Faster-Whisper\n\nThis function transcribes a WAV file using faster-whisper and returns detailed segment and word-level timing information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from faster_whisper import WhisperModel\n\nasr_model = WhisperModel(\"small\", device=\"cuda\", compute_type=\"float16\")\n\ndef transcribe_wav(path):\n    \"\"\"\n    Transcribe a WAV file using faster-whisper, returning segment and word-level timestamps.\n\n    Args:\n        path (str): Path to the WAV file.\n\n    Returns:\n        dict: {\n            \"language\": Detected language code,\n            \"segments\": List of dicts with start, end, text, and word-level timing.\n        }\n    \"\"\"\n    segments, info = asr_model.transcribe(path, beam_size=3, word_timestamps=True)\n    out = []\n    for s in segments:\n        out.append({\n            \"start\": s.start,\n            \"end\": s.end,\n            \"text\": s.text.strip(),\n            \"words\": [{\"start\": w.start, \"end\": w.end, \"word\": w.word} for w in (s.words or [])]\n        })\n    return {\"language\": info.language, \"segments\": out}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use `device=\"cpu\"` and `compute_type=\"int8\"` for CPU inference. Beam size controls search quality. Larger beams improve accuracy but increase latency. Word timestamps are essential for aligning speakers to specific words.\n\n### Step 4: Diarize and Assign Speakers\n\nThis code runs speaker diarization on a WAV file and assigns speakers to ASR segments based on time overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyannote.audio import Pipeline\n\npipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n\ndef diarize_wav(path):\n    \"\"\"\n    Run speaker diarization on a WAV file.\n\n    Args:\n        path (str): Path to the WAV file.\n\n    Returns:\n        list: List of dicts with 'start', 'end', and 'speaker' keys.\n    \"\"\"\n    diar = pipeline(path)\n    turns = []\n    for turn, _, speaker in diar.itertracks(yield_label=True):\n        turns.append({\"start\": turn.start, \"end\": turn.end, \"speaker\": speaker})\n    return turns\n\ndef assign_speaker(asr_segments, speaker_turns):\n    \"\"\"\n    Assign speakers to ASR segments based on maximum time overlap with diarization turns.\n\n    Args:\n        asr_segments (list): List of ASR segment dicts with 'start' and 'end'.\n        speaker_turns (list): List of diarization dicts with 'start', 'end', 'speaker'.\n\n    Returns:\n        list: ASR segments with added 'speaker' key.\n    \"\"\"\n    def overlap(a0, a1, b0, b1):\n        return max(0.0, min(a1, b1) - max(a0, b0))\n\n    labeled = []\n    for seg in asr_segments:\n        best_spk, best_ov = None, 0.0\n        for t in speaker_turns:\n            ov = overlap(seg[\"start\"], seg[\"end\"], t[\"start\"], t[\"end\"])\n            if ov > best_ov:\n                best_ov, best_spk = ov, t[\"speaker\"]\n        labeled.append({**seg, \"speaker\": best_spk or \"UNKNOWN\"})\n    return labeled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Diarization produces speaker turns with timestamps. The assignment function matches each ASR segment to the speaker with the most overlapping time. If no overlap is found, the segment is labeled \"UNKNOWN\". This approach works well for clear turn-taking but may struggle with crosstalk or very short utterances.\n\n### Step 5: Summarize with a Local LLM\n\nThis function sends the formatted transcript to a local LLM via Ollama and extracts structured output using a strict JSON schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\nimport requests\n\nOLLAMA_URL = \"http://localhost:11434/api/generate\"\n\nSYSTEM_PROMPT = \"\"\"You are a meeting copilot. Produce concise, accurate outputs from the transcript.\nReturn ONLY valid JSON matching the provided schema. Do not include markdown or extra text.\"\"\"\n\nSCHEMA = {\n  \"type\": \"object\",\n  \"properties\": {\n    \"summary\": {\"type\": \"string\"},\n    \"decisions\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n    \"action_items\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"task\": {\"type\": \"string\"},\n          \"owner\": {\"type\": \"string\"},\n          \"due\": {\"type\": \"string\"},\n          \"source_quote\": {\"type\": \"string\"}\n        },\n        \"required\": [\"task\"]\n      }\n    }\n  },\n  \"required\": [\"summary\", \"action_items\"]\n}\n\ndef ollama_generate(model, prompt):\n    \"\"\"\n    Call the Ollama local LLM API to generate a response for a given prompt.\n\n    Args:\n        model (str): Model name (e.g., \"mistral\").\n        prompt (str): Prompt string.\n\n    Returns:\n        str: Model's raw response (expected to be JSON).\n    Raises:\n        requests.HTTPError: If the API call fails.\n    \"\"\"\n    payload = {\"model\": model, \"prompt\": prompt, \"stream\": False}\n    r = requests.post(OLLAMA_URL, json=payload, timeout=600)\n    r.raise_for_status()\n    return r.json()[\"response\"]\n\ndef build_prompt(transcript_text):\n    \"\"\"\n    Build a prompt for the LLM including the system instruction, JSON schema, and transcript.\n\n    Args:\n        transcript_text (str): The formatted transcript.\n\n    Returns:\n        str: The full prompt string.\n    \"\"\"\n    return f\"\"\"{SYSTEM_PROMPT}\n\nJSON schema:\n{json.dumps(SCHEMA)}\n\nTranscript:\n{transcript_text}\n\"\"\"\n\ndef summarize_transcript(transcript_text, model=\"mistral\"):\n    \"\"\"\n    Summarize a transcript and extract action items using the specified LLM model.\n\n    Args:\n        transcript_text (str): The formatted transcript.\n        model (str): LLM model name.\n\n    Returns:\n        dict: Parsed JSON output from the LLM.\n    Raises:\n        json.JSONDecodeError: If the LLM output is not valid JSON.\n    \"\"\"\n    raw = ollama_generate(model, build_prompt(transcript_text))\n    return json.loads(raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The schema enforces structure. The LLM must return valid JSON with a summary, decisions, and action items. If the model produces invalid JSON, wrap the call in a try-except block and retry with a repair prompt or use regex to extract the first JSON object from the response.\n\nFor meeting summaries, you want strong instruction following and enough context. Mistral models are a common default, official site is https://mistral.ai/ and model releases are often distributed via official channels and partners. If you need a smaller model, choose something 7B to 8B and keep transcript chunks small. To get the most out of your LLM prompts and context window, check out our [guide to in-context learning and prompt engineering](/article/the-magic-of-in-context-learning-teach-your-llm-on-the-fly-3).\n\n### Step 6: Save Meeting Artifacts\n\nThis function saves all meeting data, including audio paths, transcripts, and summaries, as a timestamped JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\nimport os\nfrom datetime import datetime\n\ndef save_meeting_artifacts(out_dir, meeting):\n    \"\"\"\n    Save meeting artifacts to a timestamped JSON file.\n\n    Args:\n        out_dir (str): Output directory.\n        meeting (dict): Meeting data including config, segments, transcript, summary, etc.\n\n    Returns:\n        str: Path to the saved JSON file.\n    \"\"\"\n    os.makedirs(out_dir, exist_ok=True)\n    ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n    path = os.path.join(out_dir, f\"meeting_{ts}.json\")\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(meeting, f, ensure_ascii=False, indent=2)\n    return path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Storing artifacts as JSON makes it easy to review, audit, and reprocess meetings. You can load the JSON, inspect transcripts, and regenerate summaries with different prompts or models.\n\n### Step 7: Run the Full Pipeline\n\nThis function ties everything together, running the live meeting pipeline from microphone capture to final output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_transcript(labeled_segments, offset_s=0.0):\n    \"\"\"\n    Format labeled transcript segments for LLM input.\n\n    Args:\n        labeled_segments (list): List of dicts with 'start', 'end', 'speaker', 'text'.\n        offset_s (float): Time offset in seconds to adjust timestamps.\n\n    Returns:\n        str: Formatted transcript string.\n    \"\"\"\n    lines = []\n    for s in labeled_segments:\n        a = s[\"start\"] + offset_s\n        b = s[\"end\"] + offset_s\n        lines.append(f\"[{a:06.1f}-{b:06.1f}] {s['speaker']}: {s['text']}\")\n    return \"\\n\".join(lines)\n\ndef run_pipeline_live(out_dir=\"out\", llm_model=\"mistral\"):\n    \"\"\"\n    Run the full live meeting pipeline: capture, segment, transcribe, diarize, summarize, and save artifacts.\n\n    Args:\n        out_dir (str): Output directory for artifacts.\n        llm_model (str): LLM model name for summarization.\n\n    Returns:\n        str: Path to the saved meeting JSON file.\n    \"\"\"\n    meeting = {\n        \"config\": {\n            \"rate\": RATE,\n            \"vad_mode\": 2,\n            \"asr_model\": \"small\",\n            \"llm_model\": llm_model\n        },\n        \"segments\": [],\n        \"transcript\": \"\",\n        \"summary\": None\n    }\n\n    timeline_s = 0.0\n    for pcm in segment_speech(frames_from_mic()):\n        seg_id, wav_path = save_segment(pcm)\n        asr = transcribe_wav(wav_path)\n        turns = diarize_wav(wav_path)\n        labeled = assign_speaker(asr[\"segments\"], turns)\n\n        text = format_transcript(labeled, offset_s=timeline_s)\n        meeting[\"segments\"].append({\n            \"id\": seg_id,\n            \"wav_path\": wav_path,\n            \"asr\": asr,\n            \"diarization\": turns,\n            \"labeled\": labeled\n        })\n        meeting[\"transcript\"] += text + \"\\n\"\n        timeline_s += max((s[\"end\"] for s in asr[\"segments\"]), default=0.0)\n\n        if len(meeting[\"segments\"]) >= 10:\n            break\n\n    meeting[\"summary\"] = summarize_transcript(meeting[\"transcript\"], model=llm_model)\n    path = save_meeting_artifacts(out_dir, meeting)\n    return path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This pipeline processes segments sequentially. For production use, consider running ASR and diarization in parallel threads or async tasks to reduce latency. You can also queue segments and process them in batches.\n\n## Model Selection and Trade-Offs\n\nWhisper comes in multiple sizes. Tiny and base are fast but less accurate. Small is a good balance for most use cases. Medium and large improve accuracy but require more compute. Use faster-whisper for efficient inference. It supports quantization and runs significantly faster than the original Whisper implementation.\n\nFor diarization, Pyannote is the most mature open-source option. It requires GPU for reasonable speed. If you need CPU-only diarization, expect slower processing. Alternative approaches include clustering speaker embeddings or using simpler heuristics based on pause length, but Pyannote delivers better accuracy out of the box.\n\nFor the LLM, Mistral 7B is a strong default. It handles instruction following well and fits in 8GB VRAM with float16. Llama 3 8B is another solid choice. For smaller models, Phi-3 or Gemma 2B can work but may struggle with complex extraction tasks. Test your prompts and schema with your chosen model to ensure reliable JSON output.\n\n## Run, Validate, and Next Steps\n\nTo run the pipeline, execute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    meeting_path = run_pipeline_live(out_dir=\"meetings\", llm_model=\"mistral\")\n    print(f\"Meeting saved to {meeting_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Open the saved JSON file and verify:\n\n- Transcript segments have correct timestamps and speaker labels.\n- Summary captures key points from the meeting.\n- Action items are extracted with tasks and owners when mentioned.\n\nMeasure latency by timing each stage. VAD and segmentation should be near real-time. ASR and diarization will lag depending on hardware. LLM summarization runs after the meeting ends, so latency is less critical.\n\nTo validate accuracy, record a test meeting with known speakers and content. Compare the transcript to a manual transcription. Check speaker labels for consistency. Review the summary and action items for completeness and correctness.\n\nFor production use, add error handling, logging, and monitoring. Implement retry logic for LLM calls. Store raw LLM responses alongside parsed JSON to debug failures. Add a CLI or web UI to review transcripts, edit speaker labels, and regenerate summaries.\n\nConsider deploying on edge devices for distributed capture. A Raspberry Pi can handle audio capture and VAD, then send segments to a central server for ASR and diarization. This reduces hardware requirements per meeting room while keeping processing local to your network.\n\nExtend the pipeline by adding real-time streaming, multi-language support, or custom entity extraction. You can also fine-tune Whisper on domain-specific audio or adapt the LLM for your meeting format and terminology."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build a Local Audio Meeting Copilot with Open-Source LLMs",
    "description": "Build a privacy-first meeting copilot that records audio, diarizes speakers, transcribes locally, then generates summaries and action items using open-source LLMs. Follow a runnable, end-to-end project that turns meeting audio into speaker-tagged transcripts, concise summaries, and task lists, no cloud, no data leakage.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}