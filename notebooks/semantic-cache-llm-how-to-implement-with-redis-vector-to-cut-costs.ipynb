{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQhv5b9Vj0vw"
      },
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n",
        "\n",
        "**Title:** Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs\n",
        "\n",
        "**Description:** Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.\n",
        "\n",
        "---\n",
        "\n",
        "*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFnUUE6Bj0vx"
      },
      "source": [
        "Most LLM applications waste money and time answering the same question phrased slightly differently. A semantic cache solves this by recognizing when a new query is semantically similar to a previous one and returning the cached response instantly, eliminating the need for an LLM call.\n",
        "\n",
        "This guide walks you through building a production-grade semantic cache using embeddings and Redis Vector. You'll create a Redis-backed semantic cache, complete with thresholds, TTLs, and metrics. By the end, you'll have working code, a tunable architecture, and a clear path to immediate latency and cost reductions.\n",
        "\n",
        "**What you'll build:**\n",
        "\n",
        "<ul>\n",
        "<li>A Redis HNSW vector index for semantic similarity search\n",
        "</li>\n",
        "<li>A cache layer that normalizes queries, generates embeddings, and retrieves cached responses\n",
        "</li>\n",
        "<li>A demo script to validate cache hit rates and latency improvements\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "<ul>\n",
        "<li>Python 3.9+\n",
        "</li>\n",
        "<li>Redis Stack (local via Docker or managed Redis Cloud)\n",
        "</li>\n",
        "<li>OpenAI API key\n",
        "</li>\n",
        "<li>Basic familiarity with embeddings and vector search\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "If you're using Google Colab or a cloud notebook, connect to a managed Redis Stack instance (e.g., Redis Cloud) instead of running Docker locally.\n",
        "\n",
        "For a deeper understanding of how LLMs manage memory and the concept of context rot, see our article on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/context-rot-why-llms-forget-as-their-memory-grows\">why LLMs \"forget\" as their memory grows</a>.\n",
        "\n",
        "<hr>\n",
        "\n",
        "## How It Works (High-Level Overview)\n",
        "\n",
        "**The paraphrase problem:** Users ask the same question in many ways. \"What's your refund policy?\" and \"Can I get my money back?\" are semantically identical, but traditional caching treats them as different keys.\n",
        "\n",
        "**The embedding advantage:** Embeddings map text into a high-dimensional vector space where semantically similar phrases cluster together. By comparing query embeddings using cosine similarity, you can detect paraphrases and return cached responses.\n",
        "\n",
        "**Why Redis Vector:** Redis Stack provides HNSW (Hierarchical Navigable Small World) indexing for fast approximate nearest neighbor search. It combines low-latency vector search with Redis's native TTL, tagging, and filtering capabilities, making it ideal for production caching.\n",
        "\n",
        "**Architecture:**\n",
        "\n",
        "<ol>\n",
        "<li>Normalize the user query (lowercase, strip volatile patterns like timestamps)\n",
        "</li>\n",
        "<li>Generate an embedding for the normalized query\n",
        "</li>\n",
        "<li>Search the Redis HNSW index for the nearest cached embedding\n",
        "</li>\n",
        "<li>If distance < threshold and metadata matches (model, temperature, system prompt hash), return the cached response\n",
        "</li>\n",
        "<li>Otherwise, call the LLM, cache the new response with its embedding, and return it\n",
        "</li>\n",
        "</ol>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Setup & Installation\n",
        "\n",
        "### Option 1: Managed Redis (Recommended for Notebooks)\n",
        "\n",
        "Sign up for a free Redis Cloud account at <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://redis.com/try-free\">redis.com/try-free</a> and create a Redis Stack database. Copy the connection URL.\n",
        "\n",
        "In your notebook or terminal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3L-n9TWyj0vz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e3b854d-ac15-4515-f439-36b8952e5b08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: redis in /usr/local/lib/python3.12/dist-packages (6.4.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjHOIKHSj0v0"
      },
      "source": [
        "Set environment variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6NP21I1bj0v0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"REDIS_URL\"] = \"redis://default:password@your-redis-host:port\"\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "os.environ[\"EMBEDDING_MODEL\"] = \"text-embedding-3-small\"\n",
        "os.environ[\"CHAT_MODEL\"] = \"gpt-4o-mini\"\n",
        "os.environ[\"SIMILARITY_THRESHOLD\"] = \"0.30\"\n",
        "os.environ[\"TOP_K\"] = \"5\"\n",
        "os.environ[\"CACHE_TTL_SECONDS\"] = \"86400\"\n",
        "os.environ[\"CACHE_NAMESPACE\"] = \"sc:v1:\"\n",
        "os.environ[\"CORPUS_VERSION\"] = \"v1\"\n",
        "os.environ[\"TEMPERATURE\"] = \"0.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QktgskEcj0v1"
      },
      "source": [
        "### Option 2: Local Redis with Docker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbHhqS92j0v1"
      },
      "outputs": [],
      "source": [
        "docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J56cAYaUj0v2"
      },
      "source": [
        "Create a `.env` file:\n",
        "\n",
        "<pre><code>REDIS_URL=redis://localhost:6379\n",
        "OPENAI_API_KEY=sk-...\n",
        "EMBEDDING_MODEL=text-embedding-3-small\n",
        "CHAT_MODEL=gpt-4o-mini\n",
        "SIMILARITY_THRESHOLD=0.10\n",
        "TOP_K=5\n",
        "CACHE_TTL_SECONDS=86400\n",
        "CACHE_NAMESPACE=sc:v1:\n",
        "CORPUS_VERSION=v1\n",
        "TEMPERATURE=0.2\n",
        "</code></pre>\n",
        "\n",
        "Install dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc7pGvZlj0v2"
      },
      "outputs": [],
      "source": [
        "pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3zXkp7yj0v3"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Step-by-Step Implementation\n",
        "\n",
        "### Step 1: Create the Redis HNSW Index\n",
        "\n",
        "The index stores embeddings and metadata for cached responses. We use HNSW for fast approximate nearest neighbor search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46FV0WrQj0v3",
        "outputId": "653deac1-9591-462a-94b6-02fba2da54f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using index name: sc_index\n",
            "Dropped existing index 'sc_index' including documents.\n",
            "Index 'sc_index' created.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import redis\n",
        "import time\n",
        "\n",
        "r = redis.Redis.from_url(os.getenv(\"REDIS_URL\"))\n",
        "\n",
        "INDEX = \"sc_index\" # Make sure to update this variable if you want a different index name\n",
        "PREFIX = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "DIM = 1536  # Dimension for text-embedding-3-small\n",
        "M = 16  # HNSW graph connectivity\n",
        "EF_CONSTRUCTION = 200  # HNSW construction quality\n",
        "\n",
        "def create_index():\n",
        "    print(f\"Using index name: {INDEX}\") # Print the index name being used\n",
        "\n",
        "    # Drop index if it exists, and delete associated documents (DD)\n",
        "    try:\n",
        "        r.execute_command(\"FT.DROPINDEX\", INDEX, \"DD\")\n",
        "        print(f\"Dropped existing index '{INDEX}' including documents.\")\n",
        "    except redis.ResponseError:\n",
        "        print(f\"Index '{INDEX}' did not exist, proceeding with creation.\")\n",
        "        pass # Index does not exist, safe to ignore\n",
        "\n",
        "    # Create index with vector field and metadata tags\n",
        "    cmd = [\n",
        "        \"FT.CREATE\", INDEX,  # Command to create a full-text search index with the given name\n",
        "        \"ON\", \"HASH\",  # Index applies to Redis Hash data structures\n",
        "        \"PREFIX\", \"1\", PREFIX,  # Only index keys starting with the defined prefix\n",
        "        \"SCHEMA\",  # Define the schema of the index\n",
        "        \"prompt_hash\", \"TAG\",  # Tag field for hashing the canonicalized prompt\n",
        "        \"model\", \"TAG\",  # Tag field for the LLM model used\n",
        "        \"sys_hash\", \"TAG\",  # Tag field for hashing the system prompt\n",
        "        \"corpus_version\", \"TAG\",  # Tag field for tracking the version of the underlying corpus\n",
        "        \"temperature\", \"NUMERIC\",  # Numeric field for the temperature parameter used by the LLM\n",
        "        \"created_at\", \"NUMERIC\",  # Numeric field for the creation timestamp\n",
        "        \"last_hit_at\", \"NUMERIC\",  # Numeric field for the timestamp of the last cache hit\n",
        "        \"response\", \"TEXT\",  # Text field for the LLM's response\n",
        "        \"user_question\", \"TEXT\", # Text field for the original user question\n",
        "        \"vector\", \"VECTOR\", \"HNSW\", \"10\",  # Define a vector field named \"vector\" using the HNSW algorithm. \"10\" specifies the number of pairs for the HNSW vector definition.\n",
        "        \"TYPE\", \"FLOAT32\",  # Specify the data type of the vector embeddings\n",
        "        \"DIM\", str(DIM),  # Specify the dimension of the vector embeddings\n",
        "        \"DISTANCE_METRIC\", \"COSINE\",  # Specify the distance metric to use for vector similarity search\n",
        "        \"M\", str(M),  # HNSW parameter: number of established connections for each element during graph construction\n",
        "        \"EF_CONSTRUCTION\", str(EF_CONSTRUCTION),  # HNSW parameter: size of the dynamic list for heuristic search during graph construction\n",
        "    ]\n",
        "    r.execute_command(*cmd)\n",
        "    print(f\"Index '{INDEX}' created.\")\n",
        "\n",
        "create_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnf6RGpNj0v4"
      },
      "source": [
        "**Validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCqqWYtRj0v4",
        "outputId": "8ae2a985-ad15-41d4-d2d8-ed7172a0ba9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index Info:\n",
            "  index_name: sc_index\n",
            "  num_docs: 0\n"
          ]
        }
      ],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n",
        "\n",
        "# Helper function to decode bytes to string\n",
        "def decode_bytes(item):\n",
        "    if isinstance(item, bytes):\n",
        "        return item.decode()\n",
        "    return item\n",
        "\n",
        "# Parse the info output for better readability\n",
        "parsed_info = {}\n",
        "for i in range(0, len(info), 2):\n",
        "    key = decode_bytes(info[i])\n",
        "    value = info[i+1]\n",
        "    if isinstance(value, list):\n",
        "        # Decode lists of bytes\n",
        "        parsed_info[key] = [decode_bytes(item) for item in value]\n",
        "    else:\n",
        "        parsed_info[key] = decode_bytes(value)\n",
        "\n",
        "print(\"Index Info:\")\n",
        "print(f\"  index_name: {parsed_info.get('index_name')}\")\n",
        "print(f\"  num_docs: {parsed_info.get('num_docs')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkYR0jDUj0v4"
      },
      "source": [
        "You should see `num_docs: 0` initially.\n",
        "\n",
        "<hr>\n",
        "\n",
        "### Step 2: Normalize Queries for Stable Cache Keys\n",
        "\n",
        "Canonicalization removes volatile elements (timestamps, UUIDs, IDs) and normalizes whitespace to ensure paraphrases map to the same cache key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Reod8eyHj0v5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import hashlib\n",
        "\n",
        "# Note: Normalization adequacy depends on expected query variations and embedding model robustness.\n",
        "VOLATILE_PATTERNS = [\n",
        "    # ISO timestamps and variations\n",
        "    r\"\\b\\d{4}-\\d{2}-\\d{2}(T|\\s)\\d{2}:\\d{2}(:\\d{2})?(Z|[+-]\\d{2}:\\d{2})?\\b\",\n",
        "    # Common date formats (MM/DD/YYYY, DD/MM/YYYY, YYYY/MM/DD, YYYY-MM-DD)\n",
        "    r\"\\b\\d{1,4}[-/.]?\\d{1,2}[-/.]?\\d{2,4}\\b\", # Updated to be more flexible with separators and year length\n",
        "    # UUID v4\n",
        "    r\"\\b[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\\b\",\n",
        "    # Long IDs (6+ digits)\n",
        "    r\"\\b\\d{6,}\\b\",\n",
        "    # Email addresses (often contain volatile parts or personally identifiable info)\n",
        "    r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\",\n",
        "]\n",
        "\n",
        "def canonicalize(text: str) -> str:\n",
        "    # Removes volatile patterns (like dates, IDs) and standardizes whitespace\n",
        "    # to create a consistent representation of the query for caching.\n",
        "    t = text.strip().lower()\n",
        "    for pat in VOLATILE_PATTERNS:\n",
        "        t = re.sub(pat, \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def sha256(s: str) -> str:\n",
        "    # Generates a SHA256 hash of a string. Used for creating stable identifiers\n",
        "    # for prompts and system prompts.\n",
        "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def scope_hash(prompt_norm: str, model: str, sys_hash: str, temperature: float, corpus_version: str) -> str:\n",
        "    # Creates a unique hash that defines the scope of a cache entry.\n",
        "    # This ensures that a cache hit is only valid if all relevant parameters\n",
        "    # (normalized prompt, model, system prompt hash, temperature, corpus version) match.\n",
        "    payload = f\"{prompt_norm}|{model}|{sys_hash}|{temperature}|{corpus_version}\"\n",
        "    return sha256(payload)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fycERspj0v5"
      },
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um2EQfnej0v5",
        "outputId": "01961873-81d9-4499-9777-b2b695f0f806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is our refund policy on ?\n",
            "what is our refund policy on ?\n"
          ]
        }
      ],
      "source": [
        "q1 = \"What is our refund policy on 2025-01-15?\"\n",
        "q2 = \"what is our refund policy on 2025-01-20?\"\n",
        "print(canonicalize(q1))\n",
        "print(canonicalize(q2))\n",
        "# Both should output: \"what is our refund policy on\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilAUmXFkj0v6"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 3: Initialize Clients and Embedding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Kk9WtNFoj0v6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "EMBED_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
        "CHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")\n",
        "THRESH = float(os.getenv(\"SIMILARITY_THRESHOLD\", 0.10))\n",
        "TOP_K = int(os.getenv(\"TOP_K\", 5))\n",
        "TTL = int(os.getenv(\"CACHE_TTL_SECONDS\", 86400))\n",
        "NS = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "CORPUS_VERSION = os.getenv(\"CORPUS_VERSION\", \"v1\")\n",
        "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.2))\n",
        "\n",
        "def embed(text: str) -> np.ndarray:\n",
        "    # Generates a vector embedding for the input text using the specified embedding model.\n",
        "    # The vector is then L2 normalized, which is standard practice for cosine\n",
        "    # similarity search (But optional as it's already handled by Redis)\n",
        "    e = client.embeddings.create(model=EMBED_MODEL, input=text)\n",
        "    vec = np.array(e.data[0].embedding, dtype=np.float32)\n",
        "    norm = np.linalg.norm(vec)\n",
        "    return vec / max(norm, 1e-12) # L2 normalization\n",
        "\n",
        "def to_bytes(vec: np.ndarray) -> bytes:\n",
        "    # Converts a NumPy array (the vector embedding) into bytes.\n",
        "    # This is necessary for storing the vector data in Redis, as Redis\n",
        "    # stores data as bytes.\n",
        "    return vec.astype(np.float32).tobytes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywGrUE6Lj0v6"
      },
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCa7lNNcj0v7",
        "outputId": "5d7f3627-ae3f-48cd-c8ff-88ba694d1cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: (1536,), norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "test_vec = embed(\"hello world\")\n",
        "print(f\"Embedding shape: {test_vec.shape}, norm: {np.linalg.norm(test_vec):.4f}\")\n",
        "# Should output shape (1536,) and norm ~1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QzDCsOXj0v7"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 4: Implement Vector Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "A_DoW81jj0v7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "def vector_search(query_vec, ef_runtime: int = 100, threshold: float = THRESH) -> Optional[Tuple[str, Dict[str, Any], float]]:\n",
        "    # Performs a vector similarity search in the Redis HNSW index.\n",
        "    # It searches for the nearest neighbor(s) to the query vector and\n",
        "    # returns the document(s) that are within the specified distance threshold.\n",
        "    # Perform KNN search with EF_RUNTIME parameter\n",
        "    # Define the parameters for the search query\n",
        "    params = [\"vec\", to_bytes(query_vec), \"ef_runtime\", ef_runtime]\n",
        "    # Define the search query using RediSearch's query syntax\n",
        "    # * => search all documents\n",
        "    # [KNN {TOP_K} @vector $vec => search for KNN of the vector parameter named \"vec\"\n",
        "    # AS score => return the score (distance) as \"score\"\n",
        "    # EF_RUNTIME $ef_runtime => specify the ef_runtime parameter for HNSW search\n",
        "    q = f\"*=>[KNN {TOP_K} @vector $vec AS score]\"\n",
        "    try:\n",
        "        # Execute the RediSearch query\n",
        "        res = r.execute_command(\n",
        "            \"FT.SEARCH\", INDEX, # Index name\n",
        "            q, \"PARAMS\", str(len(params)), *params, # Query and parameters\n",
        "            \"SORTBY\", \"score\", \"ASC\", # Sort results by score in ascending order (smaller distance is better)\n",
        "            \"RETURN\", \"8\", \"response\", \"model\", \"sys_hash\", \"corpus_version\", \"temperature\", \"prompt_hash\", \"score\", \"user_question\", # Return these fields, added \"user_question\"\n",
        "            \"DIALECT\", \"2\" # Use dialect 2 for parameters\n",
        "        )\n",
        "    except redis.RedisError as e:\n",
        "        # Handle Redis errors during search\n",
        "        print(f\"Redis search error: {e}\") # Modified to print the exception\n",
        "        return None\n",
        "\n",
        "    # Process the search results\n",
        "    total = res[0] if res else 0 # Total number of results (should be 1 if a match is found)\n",
        "    if total < 1:\n",
        "        # No results found\n",
        "        return None\n",
        "\n",
        "    # Extract document id and fields from the result\n",
        "    doc_id = res[1]\n",
        "    fields = res[2]\n",
        "    # Convert field names and values from bytes to strings\n",
        "    f = {fields[i].decode() if isinstance(fields[i], bytes) else fields[i]:\n",
        "         fields[i+1].decode() if isinstance(fields[i+1], bytes) else fields[i+1]\n",
        "         for i in range(0, len(fields), 2)}\n",
        "\n",
        "    try:\n",
        "        # Extract the score (distance)\n",
        "        distance = float(f[\"score\"])\n",
        "    except Exception:\n",
        "        # Handle error in extracting score\n",
        "        print(\"Error extracting score\") # Added error print for debugging\n",
        "        distance = 1.0\n",
        "\n",
        "    # Return the document id, fields, and distance\n",
        "    return doc_id.decode() if isinstance(doc_id, bytes) else doc_id, f, distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhzCtHhyj0v7"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 5: Build the Cache Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DgRuimkRj0v8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "def sys_hash(system_prompt: str) -> str:\n",
        "    # Generates a SHA256 hash of the system prompt\n",
        "    return sha256(system_prompt.strip())\n",
        "\n",
        "def key(doc_id_hash: str) -> str:\n",
        "    # Creates a Redis key with a namespace prefix\n",
        "    return f\"{NS}{doc_id_hash}\"\n",
        "\n",
        "def metadata_matches(f: Dict[str, Any], model: str, sys_h: str, temp: float, corpus: str) -> bool:\n",
        "    # Checks if the metadata from a cached document matches the current query parameters\n",
        "    try:\n",
        "        if f.get(\"model\") != model: return False\n",
        "        if f.get(\"sys_hash\") != sys_h: return False\n",
        "        # Compare temperatures with a tolerance for floating point precision\n",
        "        if abs(float(f.get(\"temperature\", temp)) - temp) > 1e-6: return False\n",
        "        if f.get(\"corpus_version\") != corpus: return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        # Return False if there's an error during metadata comparison\n",
        "        return False\n",
        "\n",
        "def chat_call(system_prompt: str, user_prompt: str):\n",
        "    # Calls the OpenAI chat completion API\n",
        "    t0 = time.perf_counter()\n",
        "    resp = client.chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        temperature=TEMPERATURE,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "    )\n",
        "    latency_ms = (time.perf_counter() - t0) * 1000\n",
        "    content = resp.choices[0].message.content\n",
        "    usage = getattr(resp, \"usage\", None)\n",
        "    return content, latency_ms, usage\n",
        "\n",
        "def cache_get_or_generate(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH, add_to_cache: bool = True):\n",
        "    # Attempts to retrieve a response from the cache; if not found, calls the LLM and caches the response (optionally)\n",
        "    t0 = time.perf_counter()\n",
        "    sp_hash = sys_hash(system_prompt)\n",
        "    prompt_norm = canonicalize(user_prompt)\n",
        "    p_hash = sha256(prompt_norm)\n",
        "\n",
        "    qvec = embed(prompt_norm)\n",
        "\n",
        "    # --- Cache Lookup ---\n",
        "    res = vector_search(qvec, ef_runtime=ef_runtime, threshold=threshold)\n",
        "\n",
        "    # Check if a cached response was found and if its metadata matches\n",
        "    if res:\n",
        "        doc_id, fields, distance = res\n",
        "        if distance < threshold and metadata_matches(fields, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION):\n",
        "            try:\n",
        "                # Update the last hit timestamp for cache freshness\n",
        "                r.hset(doc_id, mapping={\"last_hit_at\": time.time()})\n",
        "            except redis.RedisError:\n",
        "                # Handle potential Redis errors during hset\n",
        "                pass\n",
        "            # Return the cached response details\n",
        "            return {\n",
        "                \"source\": \"cache\",\n",
        "                \"response\": fields[\"response\"],\n",
        "                \"user_question\": fields[\"user_question\"], # Include user_question for cache hits\n",
        "                \"distance\": distance,\n",
        "                \"latency_ms\": (time.perf_counter() - t0) * 1000,\n",
        "                \"closest_match_before_llm\": None # No pre-LLM closest match info on a cache hit\n",
        "            }\n",
        "\n",
        "    # --- Cache Miss - Call LLM and Cache (Optionally) ---\n",
        "\n",
        "    # If no cache hit, perform a debugging search for the closest match *before* adding the new item\n",
        "    closest_res_before_llm = vector_search(qvec, ef_runtime=ef_runtime, threshold=1.0) # Use high threshold to find closest regardless of match\n",
        "\n",
        "    content, llm_latency_ms, usage = chat_call(system_prompt, user_prompt)\n",
        "\n",
        "    # Only add to cache if add_to_cache is True\n",
        "    if add_to_cache:\n",
        "        # Generate a unique key for the new cache entry\n",
        "        doc_scope = scope_hash(prompt_norm, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION)\n",
        "        redis_key = key(doc_scope)\n",
        "\n",
        "        try:\n",
        "            # Prepare data to be stored in Redis Hash\n",
        "            mapping = {\n",
        "                \"prompt_hash\": p_hash,\n",
        "                \"model\": CHAT_MODEL,\n",
        "                \"sys_hash\": sp_hash,\n",
        "                \"corpus_version\": CORPUS_VERSION,\n",
        "                \"temperature\": TEMPERATURE,\n",
        "                \"created_at\": time.time(),\n",
        "                \"last_hit_at\": time.time(),\n",
        "                \"response\": content,\n",
        "                \"user_question\": user_prompt,\n",
        "                \"vector\": to_bytes(qvec), # Store the embedding as bytes\n",
        "            }\n",
        "            # Use a pipeline for atomic HSET and EXPIRE operations\n",
        "            pipe = r.pipeline(transaction=True)\n",
        "            pipe.hset(redis_key, mapping=mapping)\n",
        "            pipe.expire(redis_key, int(TTL)) # Set the time-to-live for the cache entry\n",
        "            pipe.execute()\n",
        "        except redis.RedisError:\n",
        "            # Handle potential Redis errors during caching\n",
        "            pass\n",
        "\n",
        "    # Prepare closest match info for the return dictionary\n",
        "    closest_match_info = None\n",
        "    if closest_res_before_llm:\n",
        "         doc_id, fields, distance = closest_res_before_llm\n",
        "         closest_match_info = {\n",
        "             \"user_question\": fields.get('user_question'),\n",
        "             \"distance\": distance\n",
        "         }\n",
        "\n",
        "\n",
        "    # Return the LLM response details\n",
        "    return {\n",
        "        \"source\": \"llm\",\n",
        "        \"response\": content,\n",
        "        \"user_question\": user_prompt, # Include user_question for LLM responses\n",
        "        \"distance\": None, # No distance for an LLM response\n",
        "        \"latency_ms\": llm_latency_ms,\n",
        "        \"usage\": {\n",
        "            \"prompt_tokens\": getattr(usage, \"prompt_tokens\", None) if usage else None,\n",
        "            \"completion_tokens\": getattr(usage, \"completion_tokens\", None) if usage else None,\n",
        "            \"total_tokens\": getattr(usage, \"total_tokens\", None) if usage else None,\n",
        "        },\n",
        "        \"closest_match_before_llm\": closest_match_info # Include closest match info before LLM call\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEMFjc1Nj0v8"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 6: Add Metrics Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xJCdv9l0j0v8"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        # Initialize counters for cache hits and misses\n",
        "        self.hits = 0\n",
        "        self.misses = 0\n",
        "        # Lists to store latencies for cache hits and LLM calls\n",
        "        self.cache_latencies = []\n",
        "        self.llm_latencies = []\n",
        "\n",
        "    def record(self, result):\n",
        "        # Record metrics based on the source of the response (cache or LLM)\n",
        "        if result[\"source\"] == \"cache\":\n",
        "            self.hits += 1\n",
        "            self.cache_latencies.append(result[\"latency_ms\"])\n",
        "        else:\n",
        "            self.misses += 1\n",
        "            self.llm_latencies.append(result[\"latency_ms\"])\n",
        "\n",
        "    def snapshot(self):\n",
        "        # Calculate and return a snapshot of the current metrics\n",
        "        def safe_percentile(vals, p):\n",
        "            # Helper function to calculate percentiles safely\n",
        "            if not vals:\n",
        "                return None\n",
        "            sorted_vals = sorted(vals)\n",
        "            idx = int(len(sorted_vals) * p / 100) - 1\n",
        "            return sorted_vals[max(0, idx)]\n",
        "\n",
        "        return {\n",
        "            # Calculate the cache hit rate\n",
        "            \"hit_rate\": self.hits / max(self.hits + self.misses, 1),\n",
        "            # Calculate the median and 95th percentile latency for cache hits\n",
        "            \"p50_cache_ms\": statistics.median(self.cache_latencies) if self.cache_latencies else None,\n",
        "            \"p95_cache_ms\": safe_percentile(self.cache_latencies, 95),\n",
        "            # Calculate the median and 95th percentile latency for LLM calls\n",
        "            \"p50_llm_ms\": statistics.median(self.llm_latencies) if self.llm_latencies else None,\n",
        "            \"p95_llm_ms\": safe_percentile(self.llm_latencies, 95),\n",
        "        }\n",
        "\n",
        "metrics = Metrics()\n",
        "\n",
        "# Modify the answer function to accept add_to_cache and pass it down\n",
        "def answer(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH, add_to_cache: bool = True):\n",
        "    # Main function to get an answer, using the cache or calling the LLM\n",
        "    # Pass the add_to_cache parameter to cache_get_or_generate\n",
        "    res = cache_get_or_generate(system_prompt, user_prompt, ef_runtime=ef_runtime, threshold=threshold, add_to_cache=add_to_cache)\n",
        "    # Record the result in the metrics tracker\n",
        "    metrics.record(res)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llYkrzInj0v9"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Run and Validate\n",
        "\n",
        "### Warm the Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbMRxWUrj0v9",
        "outputId": "0253a48d-b4a7-43f8-bc20-abce86ac4007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warming cache...\n",
            "llm 2152.2ms\n",
            "llm 663.0ms\n",
            "llm 1516.1ms\n"
          ]
        }
      ],
      "source": [
        "SYSTEM_PROMPT = \"You are a concise support assistant for ACME Corp. Use internal policy v1 for refunds and returns.\"\n",
        "seed_prompts = [\n",
        "    \"What is your refund policy?\",\n",
        "    \"How long is the return window?\",\n",
        "    \"Do you offer exchanges?\",\n",
        "]\n",
        "\n",
        "print(\"Warming cache...\")\n",
        "for p in seed_prompts:\n",
        "    res = answer(SYSTEM_PROMPT, p, add_to_cache=True)\n",
        "    print(f\"{res['source']} {res['latency_ms']:.1f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KRrSFDDj0v9"
      },
      "source": [
        "### Test Paraphrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rTSaG0Nj0v9",
        "outputId": "51210f39-aa3f-47e4-ef1c-afcf957b9628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing paraphrases...\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Could you explain your refund policy?\n",
            "Canonicalized: could you explain your refund policy?\n",
            "Result: CACHE HIT\n",
            "  Cached Question: What is your refund policy?\n",
            "  Distance: 0.09\n",
            "  Latency: 180.4ms\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Can you tell me how refunds work?\n",
            "Canonicalized: can you tell me how refunds work?\n",
            "Result: CACHE HIT\n",
            "  Cached Question: What is your refund policy?\n",
            "  Distance: 0.30\n",
            "  Latency: 374.3ms\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: How do I request a refund?\n",
            "Canonicalized: how do i request a refund?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 2806.0ms\n",
            "  Token Usage: Prompt=39, Completion=92, Total=131\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What is your refund policy?'\n",
            "    Distance: 0.35\n",
            "    Current THRESHOLD: 0.3000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Do you offer refunds if I'm not satisfied?\n",
            "Canonicalized: do you offer refunds if i'm not satisfied?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1777.5ms\n",
            "  Token Usage: Prompt=41, Completion=51, Total=92\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What is your refund policy?'\n",
            "    Distance: 0.33\n",
            "    Current THRESHOLD: 0.3000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: How can I get my money back after a purchase?\n",
            "Canonicalized: how can i get my money back after a purchase?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 2202.9ms\n",
            "  Token Usage: Prompt=43, Completion=96, Total=139\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What is your refund policy?'\n",
            "    Distance: 0.51\n",
            "    Current THRESHOLD: 0.3000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: What is the timeframe for returns?\n",
            "Canonicalized: what is the timeframe for returns?\n",
            "Result: CACHE HIT\n",
            "  Cached Question: How long is the return window?\n",
            "  Distance: 0.24\n",
            "  Latency: 195.4ms\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: How long is the return window?\n",
            "Canonicalized: how long is the return window?\n",
            "Result: CACHE HIT\n",
            "  Cached Question: How long is the return window?\n",
            "  Distance: 0.00\n",
            "  Latency: 190.3ms\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Whatâ€™s the time limit to send something back?\n",
            "Canonicalized: whatâ€™s the time limit to send something back?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 662.2ms\n",
            "  Token Usage: Prompt=42, Completion=17, Total=59\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.38\n",
            "    Current THRESHOLD: 0.3000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: When does the return period expire?\n",
            "Canonicalized: when does the return period expire?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1383.2ms\n",
            "  Token Usage: Prompt=39, Completion=18, Total=57\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.34\n",
            "    Current THRESHOLD: 0.3000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: How many days do I have to return an item?\n",
            "Canonicalized: how many days do i have to return an item?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 760.1ms\n",
            "  Token Usage: Prompt=43, Completion=15, Total=58\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.33\n",
            "    Current THRESHOLD: 0.3000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Do you permit exchanges instead of refunds?\n",
            "Canonicalized: do you permit exchanges instead of refunds?\n",
            "Result: CACHE HIT\n",
            "  Cached Question: Do you offer exchanges?\n",
            "  Distance: 0.22\n",
            "  Latency: 534.3ms\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Can I exchange a product I bought?\n",
            "Canonicalized: can i exchange a product i bought?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1496.2ms\n",
            "  Token Usage: Prompt=40, Completion=39, Total=79\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'Do you offer exchanges?'\n",
            "    Distance: 0.34\n",
            "    Current THRESHOLD: 0.3000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Is it possible to swap an item for another?\n",
            "Canonicalized: is it possible to swap an item for another?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1187.3ms\n",
            "  Token Usage: Prompt=42, Completion=46, Total=88\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'Do you offer exchanges?'\n",
            "    Distance: 0.45\n",
            "    Current THRESHOLD: 0.3000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Do you allow exchanges for different sizes or colors?\n",
            "Canonicalized: do you allow exchanges for different sizes or colors?\n",
            "Result: CACHE HIT\n",
            "  Cached Question: Do you offer exchanges?\n",
            "  Distance: 0.28\n",
            "  Latency: 293.1ms\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: How do exchanges work in your store?\n",
            "Canonicalized: how do exchanges work in your store?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 2743.0ms\n",
            "  Token Usage: Prompt=40, Completion=92, Total=132\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'Do you offer exchanges?'\n",
            "    Distance: 0.36\n",
            "    Current THRESHOLD: 0.3000\n"
          ]
        }
      ],
      "source": [
        "paraphrases = [\n",
        "    # Refunds\n",
        "    \"Could you explain your refund policy?\",\n",
        "    \"Can you tell me how refunds work?\",\n",
        "    \"How do I request a refund?\",\n",
        "    \"Do you offer refunds if I'm not satisfied?\",\n",
        "    \"How can I get my money back after a purchase?\",\n",
        "\n",
        "    # Returns\n",
        "    \"What is the timeframe for returns?\",\n",
        "    \"How long is the return window?\",\n",
        "    \"Whatâ€™s the time limit to send something back?\",\n",
        "    \"When does the return period expire?\",\n",
        "    \"How many days do I have to return an item?\",\n",
        "\n",
        "    # Exchanges\n",
        "    \"Do you permit exchanges instead of refunds?\",\n",
        "    \"Can I exchange a product I bought?\",\n",
        "    \"Is it possible to swap an item for another?\",\n",
        "    \"Do you allow exchanges for different sizes or colors?\",\n",
        "    \"How do exchanges work in your store?\",\n",
        "]\n",
        "\n",
        "print(\"\\nTesting paraphrases...\")\n",
        "for p in paraphrases:\n",
        "    print(f\"\\n--- Testing Paraphrase ---\")\n",
        "    print(f\"Original: {p}\")\n",
        "    canonical_p = canonicalize(p)\n",
        "    print(f\"Canonicalized: {canonical_p}\")\n",
        "\n",
        "    # We don't want to polute the cache while testing\n",
        "    res = answer(SYSTEM_PROMPT, p, add_to_cache=False)\n",
        "\n",
        "    if res['source'] == 'cache':\n",
        "        print(f\"Result: CACHE HIT\")\n",
        "        print(f\"  Cached Question: {res.get('user_question')}\")\n",
        "        print(f\"  Distance: {res.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "    else: # res['source'] == 'llm'\n",
        "        print(f\"Result: CACHE MISS (LLM Call)\")\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "        if res.get('usage'):\n",
        "             print(f\"  Token Usage: Prompt={res['usage'].get('prompt_tokens')}, Completion={res['usage'].get('completion_tokens')}, Total={res['usage'].get('total_tokens')}\")\n",
        "\n",
        "        # Display closest match information found *before* the LLM call\n",
        "        closest_info = res.get('closest_match_before_llm')\n",
        "        if closest_info:\n",
        "            print(f\"  Closest match in cache (before LLM call):\")\n",
        "            print(f\"    Original Cached Q: '{closest_info.get('user_question')}'\")\n",
        "            print(f\"    Distance: {closest_info.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "            print(f\"    Current THRESHOLD: {THRESH:.4f}\")\n",
        "        else:\n",
        "            print(f\"  No close match found in cache (even with high threshold) before LLM call.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qv5LJEaj0v9"
      },
      "source": [
        "### Print Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tog6pXSXj0v-",
        "outputId": "89ff5fb4-552c-4d06-fd52-5f00e61a1805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics: {'hit_rate': 0.3333333333333333, 'p50_cache_ms': 244.25534700003482, 'p95_cache_ms': 374.27036100052646, 'p50_llm_ms': 1506.1479044998123, 'p95_llm_ms': 2742.960528000367}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nMetrics:\", metrics.snapshot())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "f4cbc777",
        "outputId": "771c89ba-803a-421f-d729-c175a11c9b91"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get the snapshot of the metrics\n",
        "metrics_snapshot = metrics.snapshot()\n",
        "\n",
        "# Extract data for plotting\n",
        "labels = ['Cache (P50)', 'Cache (P95)', 'LLM (P50)', 'LLM (P95)']\n",
        "latency_values = [\n",
        "    metrics_snapshot.get('p50_cache_ms'),\n",
        "    metrics_snapshot.get('p95_cache_ms'),\n",
        "    metrics_snapshot.get('p50_llm_ms'),\n",
        "    metrics_snapshot.get('p95_llm_ms')\n",
        "]\n",
        "\n",
        "# Filter out None values if no cache hits or LLM calls occurred\n",
        "filtered_labels = [labels[i] for i in range(len(latency_values)) if latency_values[i] is not None]\n",
        "filtered_values = [value for value in latency_values if value is not None]\n",
        "\n",
        "if not filtered_values:\n",
        "    print(\"No latency data available to plot.\")\n",
        "else:\n",
        "    # Create the bar chart\n",
        "    x = np.arange(len(filtered_labels))\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    bars = ax.bar(x, filtered_values, color=['skyblue', 'deepskyblue', 'lightcoral', 'indianred'])\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_ylabel('Latency (ms)')\n",
        "    ax.set_title('Cache vs. LLM Latency (P50 and P95)')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(filtered_labels)\n",
        "    ax.set_ylim(0, max(filtered_values) * 1.2) # Set y-axis limit\n",
        "\n",
        "    # Add value labels on top of the bars\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval + 5, f'{yval:.1f}', ha='center', va='bottom')\n",
        "\n",
        "    # Display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Optionally, print the hit rate separately\n",
        "print(f\"\\nCache Hit Rate: {metrics_snapshot.get('hit_rate', 0.0):.2f}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cache Hit Rate: 0.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJZeOZFjj0v-"
      },
      "source": [
        "## Tuning the Similarity Threshold\n",
        "\n",
        "The threshold controls cache precision. Lower = stricter (fewer false hits), higher = more lenient (more hits, risk of incorrect matches)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbmSSSBbj0v-"
      },
      "outputs": [],
      "source": [
        "def sweep_thresholds(thresholds):\n",
        "    for t in thresholds:\n",
        "        print(f\"\\nThreshold={t}\")\n",
        "        for p in paraphrases:\n",
        "            res = cache_get_or_generate(SYSTEM_PROMPT, p, ef_runtime=150, threshold=t, add_to_cache=False)\n",
        "            distance_str = f\"{res.get('distance'):.2f}\" if res.get('distance') is not None else 'N/A'\n",
        "            cached_question_str = f\" (Cached: {res.get('user_question')})\" if res['source'] == 'cache' else ''\n",
        "            print(f\"{p} => {res['source']} dist={distance_str}{cached_question_str}\")\n",
        "\n",
        "# Assuming 'paraphrases' list is defined earlier in the notebook\n",
        "sweep_thresholds([0.06, 0.08, 0.10, 0.12, 0.14])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8x6YDIxj0v-"
      },
      "source": [
        "Start with 0.10 and adjust based on false positive rate.\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Inspect the Cache\n",
        "\n",
        "**Count indexed documents:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDJ3Iy_Jj0v_",
        "outputId": "f9ae62f8-f4d1-4c02-8bd4-9fa594166aa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cached documents: 3\n"
          ]
        }
      ],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n",
        "num_docs = info[info.index(b'num_docs') + 1]\n",
        "print(f\"Cached documents: {num_docs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWjbEHpkj0v_"
      },
      "source": [
        "**Inspect a document:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zebeV7cDj0v_",
        "outputId": "7546d51b-4e5d-40e7-b9b6-c9cf1acccd98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 documents:\n",
            "\n",
            "--- Document 1 (Key: sc:v1:71f8a352abd2593556cf47089f0618ed0e4f184e490218e7545127147c68eb99) ---\n",
            "  created_at: 1761058491.2114646\n",
            "  user_question: What is your refund policy?\n",
            "  corpus_version: v1\n",
            "  response: ACME Corp's refund policy allows customers to request a refund within 30 days of purchase. Items must be in original condition and packaging. To initiate a return, please contact our customer service team for instructions. Refunds will be processed within 7-10 business days after receiving the returned item.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: b92b567fdfe4b52a6f2f93624b704f18c64c3212e2ee688b7dd2f68f10a81a50\n",
            "  last_hit_at: 1761058500.0539434\n",
            "\n",
            "--- Document 2 (Key: sc:v1:fd1a249118ff83ede5d8f4d14d19dc6720c498176a7f9a0a08636f9841a0f0fa) ---\n",
            "  created_at: 1761058493.9928854\n",
            "  user_question: Do you offer exchanges?\n",
            "  corpus_version: v1\n",
            "  response: ACME Corp. does not offer exchanges. If you need a different item, please return the original item for a refund and place a new order.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: fe8a40968fc5fd2fb51084f84b99daac2e538f86fff748eee50a458729742905\n",
            "  last_hit_at: 1761058515.065673\n",
            "\n",
            "--- Document 3 (Key: sc:v1:493173b9d91727a583115a82e58560182ee3f8ba4aa15b878f637e774b5df763) ---\n",
            "  created_at: 1761058492.107014\n",
            "  user_question: How long is the return window?\n",
            "  corpus_version: v1\n",
            "  response: The return window is 30 days from the date of purchase.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 159d9f1ce048fad930a43a6b337a5476ac857e573dceed1cf53c36ed8140c193\n",
            "  last_hit_at: 1761058507.7109456\n"
          ]
        }
      ],
      "source": [
        "def print_cached_documents(max_docs: int = None):\n",
        "    keys = r.keys(f\"{NS}*\")\n",
        "    if keys:\n",
        "        print(f\"Found {len(keys)} documents:\")\n",
        "        # Limit the keys to iterate if max_docs is specified\n",
        "        keys_to_print = keys[:max_docs] if max_docs is not None else keys\n",
        "\n",
        "        for i, key in enumerate(keys_to_print):\n",
        "            print(f\"\\n--- Document {i+1} (Key: {key.decode()}) ---\")\n",
        "            doc = r.hgetall(key)\n",
        "            # Decode bytes to string for all fields except 'vector' and print them\n",
        "            for k, v in doc.items():\n",
        "                decoded_key = k.decode()\n",
        "                if decoded_key == \"vector\":\n",
        "                    # Skip printing the vector field\n",
        "                    continue\n",
        "                else:\n",
        "                    # Decode other fields and print with a label\n",
        "                    decoded_value = v.decode() if isinstance(v, bytes) else v\n",
        "                    print(f\"  {decoded_key}: {decoded_value}\")\n",
        "        if max_docs is not None and len(keys) > max_docs:\n",
        "            print(f\"\\n... and {len(keys) - max_docs} more documents (showing first {max_docs})\")\n",
        "    else:\n",
        "        print(\"No documents found in the index.\")\n",
        "\n",
        "# Call the function to print documents (prints all by default)\n",
        "print_cached_documents()\n",
        "\n",
        "# Example of printing only the first 3 documents:\n",
        "# print_cached_documents(max_docs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1n83Loqj0wE"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "You've built a production-grade semantic cache with Redis Vector. The system normalizes queries, generates embeddings, performs fast vector search, and returns cached responses when similarity is high. This approach cuts latency by 10â€“20x and reduces LLM costs by 60â€“80% for repeated queries.\n",
        "\n",
        "**Key design decisions:**\n",
        "\n",
        "<ul>\n",
        "<li>**Canonicalization** stabilizes cache keys across paraphrases\n",
        "</li>\n",
        "<li>**HNSW indexing** enables sub-50ms vector search at scale\n",
        "</li>\n",
        "<li>**Metadata gating** ensures cache hits respect model, temperature, and system prompt changes\n",
        "</li>\n",
        "<li>**TTL and namespace versioning** provide safe invalidation paths\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "**Next steps:**\n",
        "\n",
        "<ul>\n",
        "<li>Add query-side metadata filters in `FT.SEARCH` to reduce false candidates (e.g., `@model:{gpt-4o-mini} @sys_hash:{<hash>}`)\n",
        "</li>\n",
        "<li>Integrate Prometheus and Grafana for observability (track hit rate, p95 latency, cache size)\n",
        "</li>\n",
        "<li>Implement LRU eviction or score-based pruning for long-running caches\n",
        "</li>\n",
        "<li>Explore quantization (FLOAT16) to reduce memory footprint\n",
        "</li>\n",
        "<li>Scale with Redis Cluster for multi-tenant or high-throughput workloads\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "For more on building intelligent systems, see our guides on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/build-rag-pipeline\">building a RAG pipeline</a> and <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/optimize-llm-context\">optimizing LLM context windows</a>."
      ]
    }
  ],
  "metadata": {
    "title": "Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs",
    "description": "Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}