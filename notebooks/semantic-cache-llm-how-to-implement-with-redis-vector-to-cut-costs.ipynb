{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQhv5b9Vj0vw"
      },
      "source": [
        "# üìì The GenAI Revolution Cookbook\n",
        "\n",
        "**Title:** Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs\n",
        "\n",
        "**Description:** Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.\n",
        "\n",
        "---\n",
        "\n",
        "*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFnUUE6Bj0vx"
      },
      "source": [
        "Most LLM applications waste money and time answering the same question phrased slightly differently. A semantic cache solves this by recognizing when a new query is semantically similar to a previous one and returning the cached response instantly, eliminating the need for an LLM call.\n",
        "\n",
        "This guide walks you through building a production-grade semantic cache using embeddings and Redis Vector. You'll create a Redis-backed semantic cache, complete with thresholds, TTLs, and metrics. By the end, you'll have working code, a tunable architecture, and a clear path to immediate latency and cost reductions.\n",
        "\n",
        "**What you'll build:**\n",
        "\n",
        "<ul>\n",
        "<li>A Redis HNSW vector index for semantic similarity search\n",
        "</li>\n",
        "<li>A cache layer that normalizes queries, generates embeddings, and retrieves cached responses\n",
        "</li>\n",
        "<li>A demo script to validate cache hit rates and latency improvements\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "<ul>\n",
        "<li>Python 3.9+\n",
        "</li>\n",
        "<li>Redis Stack (local via Docker or managed Redis Cloud)\n",
        "</li>\n",
        "<li>OpenAI API key\n",
        "</li>\n",
        "<li>Basic familiarity with embeddings and vector search\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "If you're using Google Colab or a cloud notebook, connect to a managed Redis Stack instance (e.g., Redis Cloud) instead of running Docker locally.\n",
        "\n",
        "For a deeper understanding of how LLMs manage memory and the concept of context rot, see our article on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/context-rot-why-llms-forget-as-their-memory-grows\">why LLMs \"forget\" as their memory grows</a>.\n",
        "\n",
        "<hr>\n",
        "\n",
        "## How It Works (High-Level Overview)\n",
        "\n",
        "**The paraphrase problem:** Users ask the same question in many ways. \"What's your refund policy?\" and \"Can I get my money back?\" are semantically identical, but traditional caching treats them as different keys.\n",
        "\n",
        "**The embedding advantage:** Embeddings map text into a high-dimensional vector space where semantically similar phrases cluster together. By comparing query embeddings using cosine similarity, you can detect paraphrases and return cached responses.\n",
        "\n",
        "**Why Redis Vector:** Redis Stack provides HNSW (Hierarchical Navigable Small World) indexing for fast approximate nearest neighbor search. It combines low-latency vector search with Redis's native TTL, tagging, and filtering capabilities, making it ideal for production caching.\n",
        "\n",
        "**Architecture:**\n",
        "\n",
        "<ol>\n",
        "<li>Normalize the user query (lowercase, strip volatile patterns like timestamps)\n",
        "</li>\n",
        "<li>Generate an embedding for the normalized query\n",
        "</li>\n",
        "<li>Search the Redis HNSW index for the nearest cached embedding\n",
        "</li>\n",
        "<li>If distance < threshold and metadata matches (model, temperature, system prompt hash), return the cached response\n",
        "</li>\n",
        "<li>Otherwise, call the LLM, cache the new response with its embedding, and return it\n",
        "</li>\n",
        "</ol>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Setup & Installation\n",
        "\n",
        "### Option 1: Managed Redis (Recommended for Notebooks)\n",
        "\n",
        "Sign up for a free Redis Cloud account at <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://redis.com/try-free\">redis.com/try-free</a> and create a Redis Stack database. Copy the connection URL.\n",
        "\n",
        "In your notebook or terminal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L-n9TWyj0vz",
        "outputId": "6d8b4a8f-e4e4-4fd7-b47e-cb4bbe13ff70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting redis\n",
            "  Downloading redis-6.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Downloading redis-6.4.0-py3-none-any.whl (279 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m279.8/279.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: redis\n",
            "Successfully installed redis-6.4.0\n"
          ]
        }
      ],
      "source": [
        "%pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjHOIKHSj0v0"
      },
      "source": [
        "Set environment variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6NP21I1bj0v0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"REDIS_URL\"] = \"redis://default:password@your-redis-host:port\"\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "os.environ[\"EMBEDDING_MODEL\"] = \"text-embedding-3-small\"\n",
        "os.environ[\"CHAT_MODEL\"] = \"gpt-4o-mini\"\n",
        "os.environ[\"SIMILARITY_THRESHOLD\"] = \"0.10\"\n",
        "os.environ[\"TOP_K\"] = \"5\"\n",
        "os.environ[\"CACHE_TTL_SECONDS\"] = \"86400\"\n",
        "os.environ[\"CACHE_NAMESPACE\"] = \"sc:v1:\"\n",
        "os.environ[\"CORPUS_VERSION\"] = \"v1\"\n",
        "os.environ[\"TEMPERATURE\"] = \"0.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QktgskEcj0v1"
      },
      "source": [
        "### Option 2: Local Redis with Docker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbHhqS92j0v1"
      },
      "outputs": [],
      "source": [
        "docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J56cAYaUj0v2"
      },
      "source": [
        "Create a `.env` file:\n",
        "\n",
        "<pre><code>REDIS_URL=redis://localhost:6379\n",
        "OPENAI_API_KEY=sk-...\n",
        "EMBEDDING_MODEL=text-embedding-3-small\n",
        "CHAT_MODEL=gpt-4o-mini\n",
        "SIMILARITY_THRESHOLD=0.10\n",
        "TOP_K=5\n",
        "CACHE_TTL_SECONDS=86400\n",
        "CACHE_NAMESPACE=sc:v1:\n",
        "CORPUS_VERSION=v1\n",
        "TEMPERATURE=0.2\n",
        "</code></pre>\n",
        "\n",
        "Install dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc7pGvZlj0v2"
      },
      "outputs": [],
      "source": [
        "pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3zXkp7yj0v3"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Step-by-Step Implementation\n",
        "\n",
        "### Step 1: Create the Redis HNSW Index\n",
        "\n",
        "The index stores embeddings and metadata for cached responses. We use HNSW for fast approximate nearest neighbor search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46FV0WrQj0v3",
        "outputId": "9961b572-0f0c-4811-9934-10f888c947fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index already exists.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import redis\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "r = redis.Redis.from_url(os.getenv(\"REDIS_URL\"))\n",
        "\n",
        "INDEX = \"sc_index_demo\"\n",
        "PREFIX = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "DIM = 1536  # Dimension for text-embedding-3-small\n",
        "M = 16  # HNSW graph connectivity\n",
        "EF_CONSTRUCTION = 200  # HNSW construction quality\n",
        "\n",
        "def create_index():\n",
        "    try:\n",
        "        r.execute_command(\"FT.INFO\", INDEX)\n",
        "        print(\"Index already exists.\")\n",
        "        return\n",
        "    except redis.ResponseError:\n",
        "        pass\n",
        "\n",
        "    # Create index with vector field and metadata tags\n",
        "    cmd = [\n",
        "        \"FT.CREATE\", INDEX,  # Command to create a full-text search index with the given name\n",
        "        \"ON\", \"HASH\",  # Index applies to Redis Hash data structures\n",
        "        \"PREFIX\", \"1\", PREFIX,  # Only index keys starting with the defined prefix\n",
        "        \"SCHEMA\",  # Define the schema of the index\n",
        "        \"prompt_hash\", \"TAG\",  # Tag field for hashing the canonicalized prompt\n",
        "        \"model\", \"TAG\",  # Tag field for the LLM model used\n",
        "        \"sys_hash\", \"TAG\",  # Tag field for hashing the system prompt\n",
        "        \"corpus_version\", \"TAG\",  # Tag field for tracking the version of the underlying corpus\n",
        "        \"temperature\", \"NUMERIC\",  # Numeric field for the temperature parameter used by the LLM\n",
        "        \"created_at\", \"NUMERIC\",  # Numeric field for the creation timestamp\n",
        "        \"last_hit_at\", \"NUMERIC\",  # Numeric field for the timestamp of the last cache hit\n",
        "        \"response\", \"TEXT\",  # Text field for the LLM's response\n",
        "        \"user_question\", \"TEXT\", # Text field for the original user question\n",
        "        \"vector\", \"VECTOR\", \"HNSW\", \"10\",  # Define a vector field named \"vector\" using the HNSW algorithm. \"10\" specifies the number of pairs for the HNSW vector definition.\n",
        "        \"TYPE\", \"FLOAT32\",  # Specify the data type of the vector embeddings\n",
        "        \"DIM\", str(DIM),  # Specify the dimension of the vector embeddings\n",
        "        \"DISTANCE_METRIC\", \"COSINE\",  # Specify the distance metric to use for vector similarity search\n",
        "        \"M\", str(M),  # HNSW parameter: number of established connections for each element during graph construction\n",
        "        \"EF_CONSTRUCTION\", str(EF_CONSTRUCTION),  # HNSW parameter: size of the dynamic list for heuristic search during graph construction\n",
        "    ]\n",
        "    r.execute_command(*cmd)\n",
        "    print(\"Index created.\")\n",
        "\n",
        "create_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnf6RGpNj0v4"
      },
      "source": [
        "**Validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCqqWYtRj0v4",
        "outputId": "9608ddbe-959c-4654-d02d-1f7b96529de2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index Info:\n",
            "  index_name: sc_index\n",
            "  num_docs: 9\n"
          ]
        }
      ],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n",
        "\n",
        "# Helper function to decode bytes to string\n",
        "def decode_bytes(item):\n",
        "    if isinstance(item, bytes):\n",
        "        return item.decode()\n",
        "    return item\n",
        "\n",
        "# Parse the info output for better readability\n",
        "parsed_info = {}\n",
        "for i in range(0, len(info), 2):\n",
        "    key = decode_bytes(info[i])\n",
        "    value = info[i+1]\n",
        "    if isinstance(value, list):\n",
        "        # Decode lists of bytes\n",
        "        parsed_info[key] = [decode_bytes(item) for item in value]\n",
        "    else:\n",
        "        parsed_info[key] = decode_bytes(value)\n",
        "\n",
        "print(\"Index Info:\")\n",
        "print(f\"  index_name: {parsed_info.get('index_name')}\")\n",
        "print(f\"  num_docs: {parsed_info.get('num_docs')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkYR0jDUj0v4"
      },
      "source": [
        "You should see `num_docs: 0` initially.\n",
        "\n",
        "<hr>\n",
        "\n",
        "### Step 2: Normalize Queries for Stable Cache Keys\n",
        "\n",
        "Canonicalization removes volatile elements (timestamps, UUIDs, IDs) and normalizes whitespace to ensure paraphrases map to the same cache key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Reod8eyHj0v5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import hashlib\n",
        "\n",
        "# Note: Normalization adequacy depends on expected query variations and embedding model robustness.\n",
        "VOLATILE_PATTERNS = [\n",
        "    # ISO timestamps and variations\n",
        "    r\"\\b\\d{4}-\\d{2}-\\d{2}(T|\\s)\\d{2}:\\d{2}(:\\d{2})?(Z|[+-]\\d{2}:\\d{2})?\\b\",\n",
        "    # Common date formats (MM/DD/YYYY, DD/MM/YYYY, YYYY/MM/DD, YYYY-MM-DD)\n",
        "    r\"\\b\\d{1,4}[-/.]?\\d{1,2}[-/.]?\\d{2,4}\\b\", # Updated to be more flexible with separators and year length\n",
        "    # UUID v4\n",
        "    r\"\\b[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\\b\",\n",
        "    # Long IDs (6+ digits)\n",
        "    r\"\\b\\d{6,}\\b\",\n",
        "    # Email addresses (often contain volatile parts or personally identifiable info)\n",
        "    r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\",\n",
        "]\n",
        "\n",
        "def canonicalize(text: str) -> str:\n",
        "    t = text.strip().lower()\n",
        "    for pat in VOLATILE_PATTERNS:\n",
        "        t = re.sub(pat, \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def sha256(s: str) -> str:\n",
        "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def scope_hash(prompt_norm: str, model: str, sys_hash: str, temperature: float, corpus_version: str) -> str:\n",
        "    # Unique hash for cache scope including all parameters\n",
        "    payload = f\"{prompt_norm}|{model}|{sys_hash}|{temperature}|{corpus_version}\"\n",
        "    return sha256(payload)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fycERspj0v5"
      },
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um2EQfnej0v5",
        "outputId": "30dd3d4f-0d8f-4272-c53f-bed55550a2a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is our refund policy on ?\n",
            "what is our refund policy on ?\n"
          ]
        }
      ],
      "source": [
        "q1 = \"What is our refund policy on 2025-01-15?\"\n",
        "q2 = \"what is our refund policy on 2025-01-20?\"\n",
        "print(canonicalize(q1))\n",
        "print(canonicalize(q2))\n",
        "# Both should output: \"what is our refund policy on\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilAUmXFkj0v6"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 3: Initialize Clients and Embedding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Kk9WtNFoj0v6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "EMBED_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
        "CHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")\n",
        "THRESH = float(os.getenv(\"SIMILARITY_THRESHOLD\", 0.10))\n",
        "TOP_K = int(os.getenv(\"TOP_K\", 5))\n",
        "TTL = int(os.getenv(\"CACHE_TTL_SECONDS\", 86400))\n",
        "NS = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "CORPUS_VERSION = os.getenv(\"CORPUS_VERSION\", \"v1\")\n",
        "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.2))\n",
        "\n",
        "def embed(text: str) -> np.ndarray:\n",
        "    # Generate embedding and normalize for cosine distance\n",
        "    e = client.embeddings.create(model=EMBED_MODEL, input=text)\n",
        "    vec = np.array(e.data[0].embedding, dtype=np.float32)\n",
        "    norm = np.linalg.norm(vec)\n",
        "    return vec / max(norm, 1e-12)\n",
        "\n",
        "def to_bytes(vec: np.ndarray) -> bytes:\n",
        "    return vec.astype(np.float32).tobytes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywGrUE6Lj0v6"
      },
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCa7lNNcj0v7",
        "outputId": "89070d9d-bd74-421f-ed1a-8395c2743656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: (1536,), norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "test_vec = embed(\"hello world\")\n",
        "print(f\"Embedding shape: {test_vec.shape}, norm: {np.linalg.norm(test_vec):.4f}\")\n",
        "# Should output shape (1536,) and norm ~1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QzDCsOXj0v7"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 4: Implement Vector Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "A_DoW81jj0v7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "def vector_search(query_vec, ef_runtime: int = 100, threshold: float = THRESH) -> Optional[Tuple[str, Dict[str, Any], float]]:\n",
        "    # Perform KNN search with EF_RUNTIME parameter\n",
        "    # Define the parameters for the search query\n",
        "    params = [\"vec\", to_bytes(query_vec), \"ef_runtime\", ef_runtime]\n",
        "    # Define the search query using RediSearch's query syntax\n",
        "    # * => search all documents\n",
        "    # [KNN {TOP_K} @vector $vec ...] => find the TOP_K nearest neighbors to the vector $vec\n",
        "    # AS score => return the score (distance) as \"score\"\n",
        "    # EF_RUNTIME $ef_runtime => specify the ef_runtime parameter for HNSW search\n",
        "    q = f\"*=>[KNN {TOP_K} @vector $vec EF_RUNTIME $ef_runtime AS score]\"\n",
        "    try:\n",
        "        # Execute the RediSearch query\n",
        "        res = r.execute_command(\n",
        "            \"FT.SEARCH\", INDEX, # Index name\n",
        "            q, \"PARAMS\", str(len(params)), *params, # Query and parameters\n",
        "            \"SORTBY\", \"score\", \"ASC\", # Sort results by score in ascending order (smaller distance is better)\n",
        "            \"RETURN\", \"8\", \"response\", \"model\", \"sys_hash\", \"corpus_version\", \"temperature\", \"prompt_hash\", \"score\", \"user_question\", # Return these fields, added \"user_question\"\n",
        "            \"DIALECT\", \"2\" # Use dialect 2 for parameters\n",
        "        )\n",
        "    except redis.RedisError:\n",
        "        # Handle Redis errors during search\n",
        "        return None\n",
        "\n",
        "    # Process the search results\n",
        "    total = res[0] if res else 0 # Total number of results (should be 1 if a match is found)\n",
        "    if total < 1:\n",
        "        # No results found\n",
        "        return None\n",
        "\n",
        "    # Extract document id and fields from the result\n",
        "    doc_id = res[1]\n",
        "    fields = res[2]\n",
        "    # Convert field names and values from bytes to strings\n",
        "    f = {fields[i].decode() if isinstance(fields[i], bytes) else fields[i]:\n",
        "         fields[i+1].decode() if isinstance(fields[i+1], bytes) else fields[i+1]\n",
        "         for i in range(0, len(fields), 2)}\n",
        "\n",
        "    try:\n",
        "        # Extract the score (distance)\n",
        "        distance = float(f[\"score\"])\n",
        "    except Exception:\n",
        "        # Handle error in extracting score\n",
        "        distance = 1.0\n",
        "\n",
        "    # Return the document id, fields, and distance\n",
        "    return doc_id.decode() if isinstance(doc_id, bytes) else doc_id, f, distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhzCtHhyj0v7"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 5: Build the Cache Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "DgRuimkRj0v8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "def sys_hash(system_prompt: str) -> str:\n",
        "    # Generates a SHA256 hash of the system prompt\n",
        "    return sha256(system_prompt.strip())\n",
        "\n",
        "def key(doc_id_hash: str) -> str:\n",
        "    # Creates a Redis key with a namespace prefix\n",
        "    return f\"{NS}{doc_id_hash}\"\n",
        "\n",
        "def metadata_matches(f: Dict[str, Any], model: str, sys_h: str, temp: float, corpus: str) -> bool:\n",
        "    # Checks if the metadata from a cached document matches the current query parameters\n",
        "    try:\n",
        "        if f.get(\"model\") != model: return False\n",
        "        if f.get(\"sys_hash\") != sys_h: return False\n",
        "        # Compare temperatures with a tolerance for floating point precision\n",
        "        if abs(float(f.get(\"temperature\", temp)) - temp) > 1e-6: return False\n",
        "        if f.get(\"corpus_version\") != corpus: return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        # Return False if there's an error during metadata comparison\n",
        "        return False\n",
        "\n",
        "def chat_call(system_prompt: str, user_prompt: str):\n",
        "    # Calls the OpenAI chat completion API\n",
        "    t0 = time.perf_counter()\n",
        "    resp = client.chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        temperature=TEMPERATURE,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "    )\n",
        "    latency_ms = (time.perf_counter() - t0) * 1000\n",
        "    content = resp.choices[0].message.content\n",
        "    usage = getattr(resp, \"usage\", None)\n",
        "    return content, latency_ms, usage\n",
        "\n",
        "def cache_get_or_generate(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH):\n",
        "    # Attempts to retrieve a response from the cache; if not found, calls the LLM and caches the response\n",
        "    t0 = time.perf_counter()\n",
        "    sp_hash = sys_hash(system_prompt)\n",
        "    prompt_norm = canonicalize(user_prompt)\n",
        "    p_hash = sha256(prompt_norm)\n",
        "\n",
        "    qvec = embed(prompt_norm)\n",
        "    res = vector_search(qvec, ef_runtime=ef_runtime, threshold=threshold)\n",
        "\n",
        "    # Check if a cached response was found and if its metadata matches\n",
        "    if res:\n",
        "        doc_id, fields, distance = res\n",
        "        if distance < threshold and metadata_matches(fields, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION):\n",
        "            try:\n",
        "                # Update the last hit timestamp for cache freshness\n",
        "                r.hset(doc_id, mapping={\"last_hit_at\": time.time()})\n",
        "            except redis.RedisError:\n",
        "                # Handle potential Redis errors during hset\n",
        "                pass\n",
        "            # Return the cached response details\n",
        "            return {\n",
        "                \"source\": \"cache\",\n",
        "                \"response\": fields[\"response\"],\n",
        "                \"user_question\": fields[\"user_question\"], # Include user_question for cache hits\n",
        "                \"distance\": distance,\n",
        "                \"latency_ms\": (time.perf_counter() - t0) * 1000,\n",
        "            }\n",
        "\n",
        "    # If no cache hit, call the LLM\n",
        "    content, llm_latency_ms, usage = chat_call(system_prompt, user_prompt)\n",
        "\n",
        "    # Generate a unique key for the new cache entry\n",
        "    doc_scope = scope_hash(prompt_norm, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION)\n",
        "    redis_key = key(doc_scope) # Renamed local variable from 'doc_key' to 'redis_key'\n",
        "\n",
        "    try:\n",
        "        # Prepare data to be stored in Redis Hash\n",
        "        mapping = {\n",
        "            \"prompt_hash\": p_hash,\n",
        "            \"model\": CHAT_MODEL,\n",
        "            \"sys_hash\": sp_hash,\n",
        "            \"corpus_version\": CORPUS_VERSION,\n",
        "            \"temperature\": TEMPERATURE,\n",
        "            \"created_at\": time.time(),\n",
        "            \"last_hit_at\": time.time(),\n",
        "            \"response\": content,\n",
        "            \"user_question\": user_prompt,\n",
        "            \"vector\": to_bytes(qvec), # Store the embedding as bytes\n",
        "        }\n",
        "        # Use a pipeline for atomic HSET and EXPIRE operations\n",
        "        pipe = r.pipeline(transaction=True)\n",
        "        pipe.hset(redis_key, mapping=mapping) # Use the new variable name\n",
        "        pipe.expire(redis_key, int(TTL)) # Use the new variable name\n",
        "        pipe.execute()\n",
        "    except redis.RedisError:\n",
        "        # Handle potential Redis errors during caching\n",
        "        pass\n",
        "\n",
        "    # Return the LLM response details\n",
        "    return {\n",
        "        \"source\": \"llm\",\n",
        "        \"response\": content,\n",
        "        \"user_question\": user_prompt, # Include user_question for LLM responses\n",
        "        \"distance\": None, # No distance for an LLM response\n",
        "        \"latency_ms\": llm_latency_ms,\n",
        "        \"usage\": {\n",
        "            \"prompt_tokens\": getattr(usage, \"prompt_tokens\", None) if usage else None,\n",
        "            \"completion_tokens\": getattr(usage, \"completion_tokens\", None) if usage else None,\n",
        "            \"total_tokens\": getattr(usage, \"total_tokens\", None) if usage else None,\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEMFjc1Nj0v8"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 6: Add Metrics Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "xJCdv9l0j0v8"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        # Initialize counters for cache hits and misses\n",
        "        self.hits = 0\n",
        "        self.misses = 0\n",
        "        # Lists to store latencies for cache hits and LLM calls\n",
        "        self.cache_latencies = []\n",
        "        self.llm_latencies = []\n",
        "\n",
        "    def record(self, result):\n",
        "        # Record metrics based on the source of the response (cache or LLM)\n",
        "        if result[\"source\"] == \"cache\":\n",
        "            self.hits += 1\n",
        "            self.cache_latencies.append(result[\"latency_ms\"])\n",
        "        else:\n",
        "            self.misses += 1\n",
        "            self.llm_latencies.append(result[\"latency_ms\"])\n",
        "\n",
        "    def snapshot(self):\n",
        "        # Calculate and return a snapshot of the current metrics\n",
        "        def safe_percentile(vals, p):\n",
        "            # Helper function to calculate percentiles safely\n",
        "            if not vals:\n",
        "                return None\n",
        "            sorted_vals = sorted(vals)\n",
        "            idx = int(len(sorted_vals) * p / 100) - 1\n",
        "            return sorted_vals[max(0, idx)]\n",
        "\n",
        "        return {\n",
        "            # Calculate the cache hit rate\n",
        "            \"hit_rate\": self.hits / max(self.hits + self.misses, 1),\n",
        "            # Calculate the median and 95th percentile latency for cache hits\n",
        "            \"p50_cache_ms\": statistics.median(self.cache_latencies) if self.cache_latencies else None,\n",
        "            \"p95_cache_ms\": safe_percentile(self.cache_latencies, 95),\n",
        "            # Calculate the median and 95th percentile latency for LLM calls\n",
        "            \"p50_llm_ms\": statistics.median(self.llm_latencies) if self.llm_latencies else None,\n",
        "            \"p95_llm_ms\": safe_percentile(self.llm_latencies, 95),\n",
        "        }\n",
        "\n",
        "metrics = Metrics()\n",
        "\n",
        "def answer(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH):\n",
        "    # Main function to get an answer, using the cache or calling the LLM\n",
        "    res = cache_get_or_generate(system_prompt, user_prompt, ef_runtime=ef_runtime, threshold=threshold)\n",
        "    # Record the result in the metrics tracker\n",
        "    metrics.record(res)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llYkrzInj0v9"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Run and Validate\n",
        "\n",
        "### Warm the Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbMRxWUrj0v9",
        "outputId": "ceeea3bb-6566-4b79-e1e1-e22920b74853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warming cache...\n",
            "cache 1198.7ms\n",
            "cache 414.2ms\n",
            "cache 238.3ms\n"
          ]
        }
      ],
      "source": [
        "SYSTEM_PROMPT = \"You are a concise support assistant for ACME Corp. Use internal policy v1 for refunds and returns.\"\n",
        "seed_prompts = [\n",
        "    \"What is our refund policy?\",\n",
        "    \"How long is the return window?\",\n",
        "    \"Do you offer exchanges?\",\n",
        "]\n",
        "\n",
        "print(\"Warming cache...\")\n",
        "for p in seed_prompts:\n",
        "    res = answer(SYSTEM_PROMPT, p)\n",
        "    print(f\"{res['source']} {res['latency_ms']:.1f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KRrSFDDj0v9"
      },
      "source": [
        "### Test Paraphrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rTSaG0Nj0v9",
        "outputId": "485b64dd-af27-4fff-d7c9-f23a0abf00cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing paraphrases...\n",
            "New User Question: Can I get a refund? What's the policy?\n",
            "Found in Cache: Can I get a refund? What's the policy?\n",
            "Distance: 1.00135803223e-05\n",
            "--------------------\n",
            "New User Question: What's the time limit to return an item?\n",
            "Found in Cache: What's the time limit to return an item?\n",
            "Distance: 5.96046447754e-07\n",
            "--------------------\n",
            "New User Question: Is it possible to swap a product for another?\n",
            "Found in Cache: Is it possible to swap a product for another?\n",
            "Distance: 0.0\n",
            "--------------------\n",
            "New User Question: How do refunds work here?\n",
            "Found in Cache: How do refunds work here?\n",
            "Distance: 0.0\n",
            "--------------------\n",
            "New User Question: For how many days can I return stuff?\n",
            "Found in Cache: For how many days can I return stuff?\n",
            "Distance: -1.19209289551e-07\n",
            "--------------------\n",
            "New User Question: Where is the closest store?\n",
            "Found in Cache: N/A\n",
            "Distance: N/A\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "paraphrases = [\n",
        "    \"Can I get a refund? What's the policy?\",\n",
        "    \"What's the time limit to return an item?\",\n",
        "    \"Is it possible to swap a product for another?\",\n",
        "    \"How do refunds work here?\",\n",
        "    \"For how many days can I return stuff?\",\n",
        "    \"Where is the closest store?\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting paraphrases...\")\n",
        "for p in paraphrases:\n",
        "    res = answer(SYSTEM_PROMPT, p)\n",
        "    cached_question = res.get('user_question') if res['source'] == 'cache' else 'N/A'\n",
        "    distance = res.get('distance') if res['source'] == 'cache' else 'N/A'\n",
        "\n",
        "    print(f\"New User Question: {p}\")\n",
        "    print(f\"Found in Cache: {cached_question}\")\n",
        "    print(f\"Distance: {distance}\")\n",
        "    print(\"-\" * 20) # Separator for clarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qv5LJEaj0v9"
      },
      "source": [
        "### Print Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tog6pXSXj0v-",
        "outputId": "4bddfd82-1500-4f8d-c219-47599928949f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics: {'hit_rate': 1.0, 'p50_cache_ms': 332.80478500000754, 'p95_cache_ms': 414.19990899976256, 'p50_llm_ms': None, 'p95_llm_ms': None}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nMetrics:\", metrics.snapshot())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJZeOZFjj0v-"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "<ul>\n",
        "<li>First run: all `llm` sources, ~500‚Äì1000ms latency\n",
        "</li>\n",
        "<li>Paraphrases: mostly `cache` sources, <50ms latency, distance <0.10\n",
        "</li>\n",
        "<li>Hit rate: 60‚Äì80% for paraphrases\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Tuning the Similarity Threshold\n",
        "\n",
        "The threshold controls cache precision. Lower = stricter (fewer false hits), higher = more lenient (more hits, risk of incorrect matches)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbmSSSBbj0v-"
      },
      "outputs": [],
      "source": [
        "def sweep_thresholds(thresholds):\n",
        "    for t in thresholds:\n",
        "        print(f\"\\nThreshold={t}\")\n",
        "        for p in paraphrases:\n",
        "            res = cache_get_or_generate(SYSTEM_PROMPT, p, ef_runtime=150, threshold=t)\n",
        "            print(f\"{p} => {res['source']} dist={res.get('distance')}\")\n",
        "\n",
        "sweep_thresholds([0.06, 0.08, 0.10, 0.12, 0.14])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8x6YDIxj0v-"
      },
      "source": [
        "Start with 0.10 and adjust based on false positive rate.\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Inspect the Cache\n",
        "\n",
        "**Count indexed documents:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDJ3Iy_Jj0v_",
        "outputId": "93820d1f-d87a-47c9-dcd0-3e271b49b134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cached documents: 8\n"
          ]
        }
      ],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n",
        "num_docs = info[info.index(b'num_docs') + 1]\n",
        "print(f\"Cached documents: {num_docs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWjbEHpkj0v_"
      },
      "source": [
        "**Inspect a document:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zebeV7cDj0v_",
        "outputId": "dc5970c5-02b6-4451-8bd5-a8259e2ab511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8 documents:\n",
            "\n",
            "--- Document 1 (Key: sc:v1:c8e5c9c137a16100fb8a9ef22c1fb777bb54ce0d6fb6c2d43fca7d06d18d9ba9) ---\n",
            "  created_at: 1760983444.0548046\n",
            "  user_question: What's the time limit to return an item?\n",
            "  corpus_version: v1\n",
            "  response: You have 30 days from the date of purchase to return an item for a refund.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: caa2e995938c57b4448bf558b60933f95cd6d194f5069b43b157783c67b96a2e\n",
            "  last_hit_at: 1760984119.8511696\n",
            "\n",
            "--- Document 2 (Key: sc:v1:f9aae8fe264d8648b0d743ac37a381b81a024c23346accf990e667975ce161f8) ---\n",
            "  created_at: 1760983442.772621\n",
            "  user_question: Can I get a refund? What's the policy?\n",
            "  corpus_version: v1\n",
            "  response: ACME Corp's refund policy allows for returns within 30 days of purchase. Items must be in original condition and packaging. To initiate a refund, please contact customer service with your order details. Refunds will be processed to the original payment method.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: f2d10ed9054b3578d680ed8845a7f71cb565708f65f0572ef5637efefc79fa9b\n",
            "  last_hit_at: 1760984119.505433\n",
            "\n",
            "--- Document 3 (Key: sc:v1:493173b9d91727a583115a82e58560182ee3f8ba4aa15b878f637e774b5df763) ---\n",
            "  created_at: 1760983122.1202822\n",
            "  user_question: How long is the return window?\n",
            "  corpus_version: v1\n",
            "  response: The return window is 30 days from the date of purchase.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 159d9f1ce048fad930a43a6b337a5476ac857e573dceed1cf53c36ed8140c193\n",
            "  last_hit_at: 1760984114.2397966\n",
            "\n",
            "--- Document 4 (Key: sc:v1:19405170e08b89965d0880bdc2ab3f8554cfea909245bcba02228420e9058f52) ---\n",
            "  created_at: 1760983450.8660142\n",
            "  user_question: How do refunds work here?\n",
            "  corpus_version: v1\n",
            "  response: Refunds at ACME Corp are processed according to our internal policy v1. Customers can request a refund within 30 days of purchase, provided the item is in its original condition and packaging. To initiate a refund, please contact our customer service with your order details. Once approved, refunds are typically processed within 5-7 business days.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 19ad77b240b3f1ec5dc46b8ab23aad2e9cc6534cd62ad21f10c9b814902ee467\n",
            "  last_hit_at: 1760984120.2934136\n",
            "\n",
            "--- Document 5 (Key: sc:v1:336fe24d30d935b0538bc5e7ad88350e8f54bb8b7ec5e02cb135ce2ef2b05e59) ---\n",
            "  created_at: 1760983452.0339828\n",
            "  user_question: For how many days can I return stuff?\n",
            "  corpus_version: v1\n",
            "  response: You can return items within 30 days of purchase for a full refund, provided they are in original condition and packaging.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 90eca5306038bfdf1af3a0611a6485d6f835ba1b712cb0011c19f1bcb46a1a2e\n",
            "  last_hit_at: 1760984120.636013\n",
            "\n",
            "--- Document 6 (Key: sc:v1:fd1a249118ff83ede5d8f4d14d19dc6720c498176a7f9a0a08636f9841a0f0fa) ---\n",
            "  created_at: 1760983124.2233534\n",
            "  user_question: Do you offer exchanges?\n",
            "  corpus_version: v1\n",
            "  response: ACME Corp does not offer exchanges. If you need to return an item, you can request a refund and then place a new order for the desired item. Please refer to our refund and return policy for more details.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: fe8a40968fc5fd2fb51084f84b99daac2e538f86fff748eee50a458729742905\n",
            "  last_hit_at: 1760984114.4770987\n",
            "\n",
            "--- Document 7 (Key: sc:v1:553a2bbe49d193f0cebe0db682bc3e677cb08dac67dd4c8d629c615493ddfd5a) ---\n",
            "  created_at: 1760983121.2424734\n",
            "  user_question: What is our refund policy?\n",
            "  corpus_version: v1\n",
            "  response: ACME Corp's refund policy allows customers to request a refund within 30 days of purchase. Items must be in their original condition and packaging. Certain items, such as personalized or digital products, may be non-refundable. To initiate a refund, customers should contact our support team with their order details.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 99714355d2b61be5bfb3cb69b5212932f6880d7eed984c5d61c4128db77bbfb3\n",
            "  last_hit_at: 1760984113.8255992\n",
            "\n",
            "--- Document 8 (Key: sc:v1:53be35cec9fdc4ea25bb97e0dad6453653175495796272a771f0373351893006) ---\n",
            "  created_at: 1760983448.3538146\n",
            "  user_question: Is it possible to swap a product for another?\n",
            "  corpus_version: v1\n",
            "  response: ACME Corp does not offer direct product swaps. If you would like to exchange a product, you will need to return the original item for a refund and then place a new order for the desired product. Please refer to our refund and return policy for more details on the process.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 76f867eed9447dc297ba3d54ea5cfae0ad4f7301fe3db626f04baf39a5fb609c\n",
            "  last_hit_at: 1760984120.1742842\n"
          ]
        }
      ],
      "source": [
        "def print_cached_documents(max_docs: int = None):\n",
        "    keys = r.keys(f\"{NS}*\")\n",
        "    if keys:\n",
        "        print(f\"Found {len(keys)} documents:\")\n",
        "        # Limit the keys to iterate if max_docs is specified\n",
        "        keys_to_print = keys[:max_docs] if max_docs is not None else keys\n",
        "\n",
        "        for i, key in enumerate(keys_to_print):\n",
        "            print(f\"\\n--- Document {i+1} (Key: {key.decode()}) ---\")\n",
        "            doc = r.hgetall(key)\n",
        "            # Decode bytes to string for all fields except 'vector' and print them\n",
        "            for k, v in doc.items():\n",
        "                decoded_key = k.decode()\n",
        "                if decoded_key == \"vector\":\n",
        "                    # Skip printing the vector field\n",
        "                    continue\n",
        "                else:\n",
        "                    # Decode other fields and print with a label\n",
        "                    decoded_value = v.decode() if isinstance(v, bytes) else v\n",
        "                    print(f\"  {decoded_key}: {decoded_value}\")\n",
        "        if max_docs is not None and len(keys) > max_docs:\n",
        "            print(f\"\\n... and {len(keys) - max_docs} more documents (showing first {max_docs})\")\n",
        "    else:\n",
        "        print(\"No documents found in the index.\")\n",
        "\n",
        "# Call the function to print documents (prints all by default)\n",
        "print_cached_documents()\n",
        "\n",
        "# Example of printing only the first 3 documents:\n",
        "# print_cached_documents(max_docs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1n83Loqj0wE"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "You've built a production-grade semantic cache with Redis Vector. The system normalizes queries, generates embeddings, performs fast vector search, and returns cached responses when similarity is high. This approach cuts latency by 10‚Äì20x and reduces LLM costs by 60‚Äì80% for repeated queries.\n",
        "\n",
        "**Key design decisions:**\n",
        "\n",
        "<ul>\n",
        "<li>**Canonicalization** stabilizes cache keys across paraphrases\n",
        "</li>\n",
        "<li>**HNSW indexing** enables sub-50ms vector search at scale\n",
        "</li>\n",
        "<li>**Metadata gating** ensures cache hits respect model, temperature, and system prompt changes\n",
        "</li>\n",
        "<li>**TTL and namespace versioning** provide safe invalidation paths\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "**Next steps:**\n",
        "\n",
        "<ul>\n",
        "<li>Add query-side metadata filters in `FT.SEARCH` to reduce false candidates (e.g., `@model:{gpt-4o-mini} @sys_hash:{<hash>}`)\n",
        "</li>\n",
        "<li>Integrate Prometheus and Grafana for observability (track hit rate, p95 latency, cache size)\n",
        "</li>\n",
        "<li>Implement LRU eviction or score-based pruning for long-running caches\n",
        "</li>\n",
        "<li>Explore quantization (FLOAT16) to reduce memory footprint\n",
        "</li>\n",
        "<li>Scale with Redis Cluster for multi-tenant or high-throughput workloads\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "For more on building intelligent systems, see our guides on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/build-rag-pipeline\">building a RAG pipeline</a> and <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/optimize-llm-context\">optimizing LLM context windows</a>."
      ]
    }
  ],
  "metadata": {
    "title": "Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs",
    "description": "Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}