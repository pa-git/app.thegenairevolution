{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQhv5b9Vj0vw"
      },
      "source": [
        "#  The GenAI Revolution Cookbook\n",
        "\n",
        "**Title:** Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs\n",
        "\n",
        "**Description:** Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.\n",
        "\n",
        "---\n",
        "\n",
        "*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFnUUE6Bj0vx"
      },
      "source": [
        "Most LLM applications waste money and time answering the same question phrased slightly differently. A semantic cache solves this by recognizing when a new query is semantically similar to a previous one and returning the cached response instantly, eliminating the need for an LLM call.\n",
        "\n",
        "This guide walks you through building a production-grade semantic cache using embeddings and Redis Vector. You'll create a Redis-backed semantic cache, complete with thresholds, TTLs, and metrics. By the end, you'll have working code, a tunable architecture, and a clear path to immediate latency and cost reductions.\n",
        "\n",
        "**What you'll build:**\n",
        "\n",
        "<ul>\n",
        "<li>A Redis HNSW vector index for semantic similarity search\n",
        "</li>\n",
        "<li>A cache layer that normalizes queries, generates embeddings, and retrieves cached responses\n",
        "</li>\n",
        "<li>A demo script to validate cache hit rates and latency improvements\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "<ul>\n",
        "<li>Python 3.9+\n",
        "</li>\n",
        "<li>Redis Stack (local via Docker or managed Redis Cloud)\n",
        "</li>\n",
        "<li>OpenAI API key\n",
        "</li>\n",
        "<li>Basic familiarity with embeddings and vector search\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "If you're using Google Colab or a cloud notebook, connect to a managed Redis Stack instance (e.g., Redis Cloud) instead of running Docker locally.\n",
        "\n",
        "For a deeper understanding of how LLMs manage memory and the concept of context rot, see our article on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/context-rot-why-llms-forget-as-their-memory-grows\">why LLMs \"forget\" as their memory grows</a>.\n",
        "\n",
        "<hr>\n",
        "\n",
        "## How It Works (High-Level Overview)\n",
        "\n",
        "**The paraphrase problem:** Users ask the same question in many ways. \"What's your refund policy?\" and \"Can I get my money back?\" are semantically identical, but traditional caching treats them as different keys.\n",
        "\n",
        "**The embedding advantage:** Embeddings map text into a high-dimensional vector space where semantically similar phrases cluster together. By comparing query embeddings using cosine similarity, you can detect paraphrases and return cached responses.\n",
        "\n",
        "**Why Redis Vector:** Redis Stack provides HNSW (Hierarchical Navigable Small World) indexing for fast approximate nearest neighbor search. It combines low-latency vector search with Redis's native TTL, tagging, and filtering capabilities, making it ideal for production caching.\n",
        "\n",
        "**Architecture:**\n",
        "\n",
        "<ol>\n",
        "<li>Normalize the user query (lowercase, strip volatile patterns like timestamps)\n",
        "</li>\n",
        "<li>Generate an embedding for the normalized query\n",
        "</li>\n",
        "<li>Search the Redis HNSW index for the nearest cached embedding\n",
        "</li>\n",
        "<li>If distance < threshold and metadata matches (model, temperature, system prompt hash), return the cached response\n",
        "</li>\n",
        "<li>Otherwise, call the LLM, cache the new response with its embedding, and return it\n",
        "</li>\n",
        "</ol>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Setup & Installation\n",
        "\n",
        "### Option 1: Managed Redis (Recommended for Notebooks)\n",
        "\n",
        "Sign up for a free Redis Cloud account at <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://redis.com/try-free\">redis.com/try-free</a> and create a Redis Stack database. Copy the connection URL.\n",
        "\n",
        "In your notebook or terminal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3L-n9TWyj0vz"
      },
      "outputs": [],
      "source": [
        "%pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # Authorize Google Drive access\n",
        "\n",
        "import json, os\n",
        "import getpass, subprocess\n",
        "\n",
        "password = getpass.getpass(\"Enter decryption password: \")\n",
        "\n",
        "# Decrypt the GPG file into secrets.json\n",
        "cmd = [\n",
        "    \"gpg\",\n",
        "    \"--batch\",\n",
        "    \"--yes\",\n",
        "    \"--passphrase-fd\", \"0\",\n",
        "    \"--output\", \"secrets.json\",\n",
        "    \"--decrypt\", \"/content/drive/MyDrive/shared/secrets.json.gpg\"\n",
        "]\n",
        "subprocess.run(cmd, input=password.encode(), check=True)\n",
        "\n",
        "# Load and set secrets\n",
        "with open(\"secrets.json\") as f:\n",
        "    secrets = json.load(f)\n",
        "    os.environ.update(secrets)\n",
        "\n",
        "# Print only the names (keys) of the variables set\n",
        "print(\"Variables loaded from secrets:\")\n",
        "for key in secrets.keys():\n",
        "    print(\"-\", key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf-DDZCas71k",
        "outputId": "286e248d-9ab1-42b2-8428-6ca3a7bd12d1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Enter decryption password: 路路路路路路路路路路\n",
            "Variables loaded from secrets:\n",
            "- OPENAI_API_KEY\n",
            "- REDIS_URL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjHOIKHSj0v0"
      },
      "source": [
        "Set environment variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6NP21I1bj0v0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"REDIS_URL\"] = \"redis://default:password@your-redis-host:port\"\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "os.environ[\"EMBEDDING_MODEL\"] = \"text-embedding-3-small\"\n",
        "os.environ[\"CHAT_MODEL\"] = \"gpt-4o-mini\"\n",
        "os.environ[\"SIMILARITY_THRESHOLD\"] = \"0.10\"\n",
        "os.environ[\"TOP_K\"] = \"5\"\n",
        "os.environ[\"CACHE_TTL_SECONDS\"] = \"86400\"\n",
        "os.environ[\"CACHE_NAMESPACE\"] = \"sc:v1:\"\n",
        "os.environ[\"CORPUS_VERSION\"] = \"v1\"\n",
        "os.environ[\"TEMPERATURE\"] = \"0.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QktgskEcj0v1"
      },
      "source": [
        "### Option 2: Local Redis with Docker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbHhqS92j0v1"
      },
      "outputs": [],
      "source": [
        "docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J56cAYaUj0v2"
      },
      "source": [
        "Create a `.env` file:\n",
        "\n",
        "<pre><code>REDIS_URL=redis://localhost:6379\n",
        "OPENAI_API_KEY=sk-...\n",
        "EMBEDDING_MODEL=text-embedding-3-small\n",
        "CHAT_MODEL=gpt-4o-mini\n",
        "SIMILARITY_THRESHOLD=0.10\n",
        "TOP_K=5\n",
        "CACHE_TTL_SECONDS=86400\n",
        "CACHE_NAMESPACE=sc:v1:\n",
        "CORPUS_VERSION=v1\n",
        "TEMPERATURE=0.2\n",
        "</code></pre>\n",
        "\n",
        "Install dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc7pGvZlj0v2"
      },
      "outputs": [],
      "source": [
        "pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3zXkp7yj0v3"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Step-by-Step Implementation\n",
        "\n",
        "### Step 1: Create the Redis HNSW Index\n",
        "\n",
        "The index stores embeddings and metadata for cached responses. We use HNSW for fast approximate nearest neighbor search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46FV0WrQj0v3",
        "outputId": "c5fa0ea5-8c72-416f-efb6-3d1857650dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using index name: sc_index\n",
            "Dropped existing index 'sc_index' including documents.\n",
            "Index 'sc_index' created.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import redis\n",
        "import time\n",
        "\n",
        "r = redis.Redis.from_url(os.getenv(\"REDIS_URL\"))\n",
        "\n",
        "INDEX = \"sc_index\" # Make sure to update this variable if you want a different index name\n",
        "PREFIX = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "DIM = 1536  # Dimension for text-embedding-3-small\n",
        "M = 16  # HNSW graph connectivity\n",
        "EF_CONSTRUCTION = 200  # HNSW construction quality\n",
        "\n",
        "def create_index():\n",
        "    print(f\"Using index name: {INDEX}\") # Print the index name being used\n",
        "\n",
        "    # Drop index if it exists, and delete associated documents (DD)\n",
        "    try:\n",
        "        r.execute_command(\"FT.DROPINDEX\", INDEX, \"DD\")\n",
        "        print(f\"Dropped existing index '{INDEX}' including documents.\")\n",
        "    except redis.ResponseError:\n",
        "        print(f\"Index '{INDEX}' did not exist, proceeding with creation.\")\n",
        "        pass # Index does not exist, safe to ignore\n",
        "\n",
        "    # Create index with vector field and metadata tags\n",
        "    cmd = [\n",
        "        \"FT.CREATE\", INDEX,  # Command to create a full-text search index with the given name\n",
        "        \"ON\", \"HASH\",  # Index applies to Redis Hash data structures\n",
        "        \"PREFIX\", \"1\", PREFIX,  # Only index keys starting with the defined prefix\n",
        "        \"SCHEMA\",  # Define the schema of the index\n",
        "        \"prompt_hash\", \"TAG\",  # Tag field for hashing the canonicalized prompt\n",
        "        \"model\", \"TAG\",  # Tag field for the LLM model used\n",
        "        \"sys_hash\", \"TAG\",  # Tag field for hashing the system prompt\n",
        "        \"corpus_version\", \"TAG\",  # Tag field for tracking the version of the underlying corpus\n",
        "        \"temperature\", \"NUMERIC\",  # Numeric field for the temperature parameter used by the LLM\n",
        "        \"created_at\", \"NUMERIC\",  # Numeric field for the creation timestamp\n",
        "        \"last_hit_at\", \"NUMERIC\",  # Numeric field for the timestamp of the last cache hit\n",
        "        \"response\", \"TEXT\",  # Text field for the LLM's response\n",
        "        \"user_question\", \"TEXT\", # Text field for the original user question\n",
        "        \"vector\", \"VECTOR\", \"HNSW\", \"10\",  # Define a vector field named \"vector\" using the HNSW algorithm. \"10\" specifies the number of pairs for the HNSW vector definition.\n",
        "        \"TYPE\", \"FLOAT32\",  # Specify the data type of the vector embeddings\n",
        "        \"DIM\", str(DIM),  # Specify the dimension of the vector embeddings\n",
        "        \"DISTANCE_METRIC\", \"COSINE\",  # Specify the distance metric to use for vector similarity search\n",
        "        \"M\", str(M),  # HNSW parameter: number of established connections for each element during graph construction\n",
        "        \"EF_CONSTRUCTION\", str(EF_CONSTRUCTION),  # HNSW parameter: size of the dynamic list for heuristic search during graph construction\n",
        "    ]\n",
        "    r.execute_command(*cmd)\n",
        "    print(f\"Index '{INDEX}' created.\")\n",
        "\n",
        "create_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnf6RGpNj0v4"
      },
      "source": [
        "**Validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCqqWYtRj0v4",
        "outputId": "96f3b266-4d15-4ad4-f089-6555c1c3d3e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index Info:\n",
            "  index_name: sc_index\n",
            "  num_docs: 0\n"
          ]
        }
      ],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n",
        "\n",
        "# Helper function to decode bytes to string\n",
        "def decode_bytes(item):\n",
        "    if isinstance(item, bytes):\n",
        "        return item.decode()\n",
        "    return item\n",
        "\n",
        "# Parse the info output for better readability\n",
        "parsed_info = {}\n",
        "for i in range(0, len(info), 2):\n",
        "    key = decode_bytes(info[i])\n",
        "    value = info[i+1]\n",
        "    if isinstance(value, list):\n",
        "        # Decode lists of bytes\n",
        "        parsed_info[key] = [decode_bytes(item) for item in value]\n",
        "    else:\n",
        "        parsed_info[key] = decode_bytes(value)\n",
        "\n",
        "print(\"Index Info:\")\n",
        "print(f\"  index_name: {parsed_info.get('index_name')}\")\n",
        "print(f\"  num_docs: {parsed_info.get('num_docs')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkYR0jDUj0v4"
      },
      "source": [
        "You should see `num_docs: 0` initially.\n",
        "\n",
        "<hr>\n",
        "\n",
        "### Step 2: Normalize Queries for Stable Cache Keys\n",
        "\n",
        "Canonicalization removes volatile elements (timestamps, UUIDs, IDs) and normalizes whitespace to ensure paraphrases map to the same cache key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Reod8eyHj0v5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import hashlib\n",
        "\n",
        "# Note: Normalization adequacy depends on expected query variations and embedding model robustness.\n",
        "VOLATILE_PATTERNS = [\n",
        "    # ISO timestamps and variations\n",
        "    r\"\\b\\d{4}-\\d{2}-\\d{2}(T|\\s)\\d{2}:\\d{2}(:\\d{2})?(Z|[+-]\\d{2}:\\d{2})?\\b\",\n",
        "    # Common date formats (MM/DD/YYYY, DD/MM/YYYY, YYYY/MM/DD, YYYY-MM-DD)\n",
        "    r\"\\b\\d{1,4}[-/.]?\\d{1,2}[-/.]?\\d{2,4}\\b\", # Updated to be more flexible with separators and year length\n",
        "    # UUID v4\n",
        "    r\"\\b[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\\b\",\n",
        "    # Long IDs (6+ digits)\n",
        "    r\"\\b\\d{6,}\\b\",\n",
        "    # Email addresses (often contain volatile parts or personally identifiable info)\n",
        "    r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\",\n",
        "]\n",
        "\n",
        "def canonicalize(text: str) -> str:\n",
        "    # Removes volatile patterns (like dates, IDs) and standardizes whitespace\n",
        "    # to create a consistent representation of the query for caching.\n",
        "    t = text.strip().lower()\n",
        "    for pat in VOLATILE_PATTERNS:\n",
        "        t = re.sub(pat, \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def sha256(s: str) -> str:\n",
        "    # Generates a SHA256 hash of a string. Used for creating stable identifiers\n",
        "    # for prompts and system prompts.\n",
        "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def scope_hash(prompt_norm: str, model: str, sys_hash: str, temperature: float, corpus_version: str) -> str:\n",
        "    # Creates a unique hash that defines the scope of a cache entry.\n",
        "    # This ensures that a cache hit is only valid if all relevant parameters\n",
        "    # (normalized prompt, model, system prompt hash, temperature, corpus version) match.\n",
        "    payload = f\"{prompt_norm}|{model}|{sys_hash}|{temperature}|{corpus_version}\"\n",
        "    return sha256(payload)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fycERspj0v5"
      },
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um2EQfnej0v5",
        "outputId": "33252341-ee3a-4e92-db08-3e4b0dae9348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is our refund policy on ?\n",
            "what is our refund policy on ?\n"
          ]
        }
      ],
      "source": [
        "q1 = \"What is our refund policy on 2025-01-15?\"\n",
        "q2 = \"what is our refund policy on 2025-01-20?\"\n",
        "print(canonicalize(q1))\n",
        "print(canonicalize(q2))\n",
        "# Both should output: \"what is our refund policy on\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilAUmXFkj0v6"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 3: Initialize Clients and Embedding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Kk9WtNFoj0v6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "EMBED_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
        "CHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")\n",
        "THRESH = float(os.getenv(\"SIMILARITY_THRESHOLD\", 0.10))\n",
        "TOP_K = int(os.getenv(\"TOP_K\", 5))\n",
        "TTL = int(os.getenv(\"CACHE_TTL_SECONDS\", 86400))\n",
        "NS = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "CORPUS_VERSION = os.getenv(\"CORPUS_VERSION\", \"v1\")\n",
        "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.2))\n",
        "\n",
        "def embed(text: str) -> np.ndarray:\n",
        "    # Generates a vector embedding for the input text using the specified embedding model.\n",
        "    # The vector is then L2 normalized, which is standard practice for cosine similarity search.\n",
        "    e = client.embeddings.create(model=EMBED_MODEL, input=text)\n",
        "    vec = np.array(e.data[0].embedding, dtype=np.float32)\n",
        "    norm = np.linalg.norm(vec)\n",
        "    return vec / max(norm, 1e-12) # L2 normalization\n",
        "\n",
        "def to_bytes(vec: np.ndarray) -> bytes:\n",
        "    # Converts a NumPy array (the vector embedding) into bytes.\n",
        "    # This is necessary for storing the vector data in Redis, as Redis\n",
        "    # stores data as bytes.\n",
        "    return vec.astype(np.float32).tobytes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywGrUE6Lj0v6"
      },
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCa7lNNcj0v7",
        "outputId": "efbc2fa5-d43b-4f39-905b-fbbc548b4943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: (1536,), norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "test_vec = embed(\"hello world\")\n",
        "print(f\"Embedding shape: {test_vec.shape}, norm: {np.linalg.norm(test_vec):.4f}\")\n",
        "# Should output shape (1536,) and norm ~1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QzDCsOXj0v7"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 4: Implement Vector Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A_DoW81jj0v7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "def vector_search(query_vec, ef_runtime: int = 100, threshold: float = THRESH) -> Optional[Tuple[str, Dict[str, Any], float]]:\n",
        "    # Performs a vector similarity search in the Redis HNSW index.\n",
        "    # It searches for the nearest neighbor(s) to the query vector and\n",
        "    # returns the document(s) that are within the specified distance threshold.\n",
        "    # Perform KNN search with EF_RUNTIME parameter\n",
        "    # Define the parameters for the search query\n",
        "    params = [\"vec\", to_bytes(query_vec), \"ef_runtime\", ef_runtime]\n",
        "    # Define the search query using RediSearch's query syntax\n",
        "    # * => search all documents\n",
        "    # [KNN {TOP_K} @vector $vec => search for KNN of the vector parameter named \"vec\"\n",
        "    # AS score => return the score (distance) as \"score\"\n",
        "    # EF_RUNTIME $ef_runtime => specify the ef_runtime parameter for HNSW search\n",
        "    q = f\"*=>[KNN {TOP_K} @vector $vec AS score]\"\n",
        "    try:\n",
        "        # Execute the RediSearch query\n",
        "        res = r.execute_command(\n",
        "            \"FT.SEARCH\", INDEX, # Index name\n",
        "            q, \"PARAMS\", str(len(params)), *params, # Query and parameters\n",
        "            \"SORTBY\", \"score\", \"ASC\", # Sort results by score in ascending order (smaller distance is better)\n",
        "            \"RETURN\", \"8\", \"response\", \"model\", \"sys_hash\", \"corpus_version\", \"temperature\", \"prompt_hash\", \"score\", \"user_question\", # Return these fields, added \"user_question\"\n",
        "            \"DIALECT\", \"2\" # Use dialect 2 for parameters\n",
        "        )\n",
        "    except redis.RedisError as e:\n",
        "        # Handle Redis errors during search\n",
        "        print(f\"Redis search error: {e}\") # Modified to print the exception\n",
        "        return None\n",
        "\n",
        "    # Process the search results\n",
        "    total = res[0] if res else 0 # Total number of results (should be 1 if a match is found)\n",
        "    if total < 1:\n",
        "        # No results found\n",
        "        return None\n",
        "\n",
        "    # Extract document id and fields from the result\n",
        "    doc_id = res[1]\n",
        "    fields = res[2]\n",
        "    # Convert field names and values from bytes to strings\n",
        "    f = {fields[i].decode() if isinstance(fields[i], bytes) else fields[i]:\n",
        "         fields[i+1].decode() if isinstance(fields[i+1], bytes) else fields[i+1]\n",
        "         for i in range(0, len(fields), 2)}\n",
        "\n",
        "    try:\n",
        "        # Extract the score (distance)\n",
        "        distance = float(f[\"score\"])\n",
        "    except Exception:\n",
        "        # Handle error in extracting score\n",
        "        print(\"Error extracting score\") # Added error print for debugging\n",
        "        distance = 1.0\n",
        "\n",
        "    # Return the document id, fields, and distance\n",
        "    return doc_id.decode() if isinstance(doc_id, bytes) else doc_id, f, distance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# _s = \"What are the rules for getting a refund?\"\n",
        "# _s = \"What's the rules for getting refunds?\"\n",
        "_s = 'What is your refund policy?'\n",
        "_pn = canonicalize(_s)\n",
        "_v = embed(_pn)\n",
        "# _v = embed(_s)\n",
        "vector_search(_v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKl1mIa-24D_",
        "outputId": "f539a55d-921d-4599-9df9-da1b343b781e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('sc:v1:71f8a352abd2593556cf47089f0618ed0e4f184e490218e7545127147c68eb99',\n",
              " {'score': '-1.19209289551e-07',\n",
              "  'response': \"ACME Corp's refund policy allows customers to request a refund within 30 days of purchase. Items must be in original condition and packaging. To initiate a refund, please contact our customer service with your order details. Certain items may be non-refundable, such as personalized or clearance items. For more specific inquiries, please refer to our detailed policy or contact support.\",\n",
              "  'model': 'gpt-4o-mini',\n",
              "  'sys_hash': 'b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e',\n",
              "  'corpus_version': 'v1',\n",
              "  'temperature': '0.2',\n",
              "  'prompt_hash': 'b92b567fdfe4b52a6f2f93624b704f18c64c3212e2ee688b7dd2f68f10a81a50',\n",
              "  'user_question': 'What is your refund policy?'},\n",
              " -1.19209289551e-07)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhzCtHhyj0v7"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 5: Build the Cache Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DgRuimkRj0v8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "def sys_hash(system_prompt: str) -> str:\n",
        "    # Generates a SHA256 hash of the system prompt\n",
        "    return sha256(system_prompt.strip())\n",
        "\n",
        "def key(doc_id_hash: str) -> str:\n",
        "    # Creates a Redis key with a namespace prefix\n",
        "    return f\"{NS}{doc_id_hash}\"\n",
        "\n",
        "def metadata_matches(f: Dict[str, Any], model: str, sys_h: str, temp: float, corpus: str) -> bool:\n",
        "    # Checks if the metadata from a cached document matches the current query parameters\n",
        "    try:\n",
        "        if f.get(\"model\") != model: return False\n",
        "        if f.get(\"sys_hash\") != sys_h: return False\n",
        "        # Compare temperatures with a tolerance for floating point precision\n",
        "        if abs(float(f.get(\"temperature\", temp)) - temp) > 1e-6: return False\n",
        "        if f.get(\"corpus_version\") != corpus: return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        # Return False if there's an error during metadata comparison\n",
        "        return False\n",
        "\n",
        "def chat_call(system_prompt: str, user_prompt: str):\n",
        "    # Calls the OpenAI chat completion API\n",
        "    t0 = time.perf_counter()\n",
        "    resp = client.chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        temperature=TEMPERATURE,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "    )\n",
        "    latency_ms = (time.perf_counter() - t0) * 1000\n",
        "    content = resp.choices[0].message.content\n",
        "    usage = getattr(resp, \"usage\", None)\n",
        "    return content, latency_ms, usage\n",
        "\n",
        "def cache_get_or_generate(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH, add_to_cache: bool = True):\n",
        "    # Attempts to retrieve a response from the cache; if not found, calls the LLM and caches the response (optionally)\n",
        "    t0 = time.perf_counter()\n",
        "    sp_hash = sys_hash(system_prompt)\n",
        "    prompt_norm = canonicalize(user_prompt)\n",
        "    p_hash = sha256(prompt_norm)\n",
        "\n",
        "    qvec = embed(prompt_norm)\n",
        "\n",
        "    # --- Cache Lookup ---\n",
        "    res = vector_search(qvec, ef_runtime=ef_runtime, threshold=threshold)\n",
        "\n",
        "    # Check if a cached response was found and if its metadata matches\n",
        "    if res:\n",
        "        doc_id, fields, distance = res\n",
        "        if distance < threshold and metadata_matches(fields, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION):\n",
        "            try:\n",
        "                # Update the last hit timestamp for cache freshness\n",
        "                r.hset(doc_id, mapping={\"last_hit_at\": time.time()})\n",
        "            except redis.RedisError:\n",
        "                # Handle potential Redis errors during hset\n",
        "                pass\n",
        "            # Return the cached response details\n",
        "            return {\n",
        "                \"source\": \"cache\",\n",
        "                \"response\": fields[\"response\"],\n",
        "                \"user_question\": fields[\"user_question\"], # Include user_question for cache hits\n",
        "                \"distance\": distance,\n",
        "                \"latency_ms\": (time.perf_counter() - t0) * 1000,\n",
        "                \"closest_match_before_llm\": None # No pre-LLM closest match info on a cache hit\n",
        "            }\n",
        "\n",
        "    # --- Cache Miss - Call LLM and Cache (Optionally) ---\n",
        "\n",
        "    # If no cache hit, perform a debugging search for the closest match *before* adding the new item\n",
        "    closest_res_before_llm = vector_search(qvec, ef_runtime=ef_runtime, threshold=1.0) # Use high threshold to find closest regardless of match\n",
        "\n",
        "    content, llm_latency_ms, usage = chat_call(system_prompt, user_prompt)\n",
        "\n",
        "    # Only add to cache if add_to_cache is True\n",
        "    if add_to_cache:\n",
        "        # Generate a unique key for the new cache entry\n",
        "        doc_scope = scope_hash(prompt_norm, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION)\n",
        "        redis_key = key(doc_scope)\n",
        "\n",
        "        try:\n",
        "            # Prepare data to be stored in Redis Hash\n",
        "            mapping = {\n",
        "                \"prompt_hash\": p_hash,\n",
        "                \"model\": CHAT_MODEL,\n",
        "                \"sys_hash\": sp_hash,\n",
        "                \"corpus_version\": CORPUS_VERSION,\n",
        "                \"temperature\": TEMPERATURE,\n",
        "                \"created_at\": time.time(),\n",
        "                \"last_hit_at\": time.time(),\n",
        "                \"response\": content,\n",
        "                \"user_question\": user_prompt,\n",
        "                \"vector\": to_bytes(qvec), # Store the embedding as bytes\n",
        "            }\n",
        "            # Use a pipeline for atomic HSET and EXPIRE operations\n",
        "            pipe = r.pipeline(transaction=True)\n",
        "            pipe.hset(redis_key, mapping=mapping)\n",
        "            pipe.expire(redis_key, int(TTL)) # Set the time-to-live for the cache entry\n",
        "            pipe.execute()\n",
        "        except redis.RedisError:\n",
        "            # Handle potential Redis errors during caching\n",
        "            pass\n",
        "\n",
        "    # Prepare closest match info for the return dictionary\n",
        "    closest_match_info = None\n",
        "    if closest_res_before_llm:\n",
        "         doc_id, fields, distance = closest_res_before_llm\n",
        "         closest_match_info = {\n",
        "             \"user_question\": fields.get('user_question'),\n",
        "             \"distance\": distance\n",
        "         }\n",
        "\n",
        "\n",
        "    # Return the LLM response details\n",
        "    return {\n",
        "        \"source\": \"llm\",\n",
        "        \"response\": content,\n",
        "        \"user_question\": user_prompt, # Include user_question for LLM responses\n",
        "        \"distance\": None, # No distance for an LLM response\n",
        "        \"latency_ms\": llm_latency_ms,\n",
        "        \"usage\": {\n",
        "            \"prompt_tokens\": getattr(usage, \"prompt_tokens\", None) if usage else None,\n",
        "            \"completion_tokens\": getattr(usage, \"completion_tokens\", None) if usage else None,\n",
        "            \"total_tokens\": getattr(usage, \"total_tokens\", None) if usage else None,\n",
        "        },\n",
        "        \"closest_match_before_llm\": closest_match_info # Include closest match info before LLM call\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEMFjc1Nj0v8"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 6: Add Metrics Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xJCdv9l0j0v8"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        # Initialize counters for cache hits and misses\n",
        "        self.hits = 0\n",
        "        self.misses = 0\n",
        "        # Lists to store latencies for cache hits and LLM calls\n",
        "        self.cache_latencies = []\n",
        "        self.llm_latencies = []\n",
        "\n",
        "    def record(self, result):\n",
        "        # Record metrics based on the source of the response (cache or LLM)\n",
        "        if result[\"source\"] == \"cache\":\n",
        "            self.hits += 1\n",
        "            self.cache_latencies.append(result[\"latency_ms\"])\n",
        "        else:\n",
        "            self.misses += 1\n",
        "            self.llm_latencies.append(result[\"latency_ms\"])\n",
        "\n",
        "    def snapshot(self):\n",
        "        # Calculate and return a snapshot of the current metrics\n",
        "        def safe_percentile(vals, p):\n",
        "            # Helper function to calculate percentiles safely\n",
        "            if not vals:\n",
        "                return None\n",
        "            sorted_vals = sorted(vals)\n",
        "            idx = int(len(sorted_vals) * p / 100) - 1\n",
        "            return sorted_vals[max(0, idx)]\n",
        "\n",
        "        return {\n",
        "            # Calculate the cache hit rate\n",
        "            \"hit_rate\": self.hits / max(self.hits + self.misses, 1),\n",
        "            # Calculate the median and 95th percentile latency for cache hits\n",
        "            \"p50_cache_ms\": statistics.median(self.cache_latencies) if self.cache_latencies else None,\n",
        "            \"p95_cache_ms\": safe_percentile(self.cache_latencies, 95),\n",
        "            # Calculate the median and 95th percentile latency for LLM calls\n",
        "            \"p50_llm_ms\": statistics.median(self.llm_latencies) if self.llm_latencies else None,\n",
        "            \"p95_llm_ms\": safe_percentile(self.llm_latencies, 95),\n",
        "        }\n",
        "\n",
        "metrics = Metrics()\n",
        "\n",
        "# Modify the answer function to accept add_to_cache and pass it down\n",
        "def answer(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH, add_to_cache: bool = True):\n",
        "    # Main function to get an answer, using the cache or calling the LLM\n",
        "    # Pass the add_to_cache parameter to cache_get_or_generate\n",
        "    res = cache_get_or_generate(system_prompt, user_prompt, ef_runtime=ef_runtime, threshold=threshold, add_to_cache=add_to_cache)\n",
        "    # Record the result in the metrics tracker\n",
        "    metrics.record(res)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llYkrzInj0v9"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Run and Validate\n",
        "\n",
        "### Warm the Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbMRxWUrj0v9",
        "outputId": "1327c363-a73e-4b01-fcc2-c61a93b62087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warming cache...\n",
            "cache 213.8ms\n",
            "cache 257.7ms\n",
            "cache 160.2ms\n"
          ]
        }
      ],
      "source": [
        "SYSTEM_PROMPT = \"You are a concise support assistant for ACME Corp. Use internal policy v1 for refunds and returns.\"\n",
        "seed_prompts = [\n",
        "    \"What is your refund policy?\",\n",
        "    \"How long is the return window?\",\n",
        "    \"Do you offer exchanges?\",\n",
        "]\n",
        "\n",
        "print(\"Warming cache...\")\n",
        "for p in seed_prompts:\n",
        "    res = answer(SYSTEM_PROMPT, p, add_to_cache=True)\n",
        "    print(f\"{res['source']} {res['latency_ms']:.1f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KRrSFDDj0v9"
      },
      "source": [
        "### Test Paraphrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rTSaG0Nj0v9",
        "outputId": "5bcedba5-ab74-4a01-b99c-5bbab16c1e07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing paraphrases...\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: What is your refund policy?\n",
            "Canonicalized: what is your refund policy?\n",
            "Result: CACHE HIT\n",
            "  Cached Question: What is your refund policy?\n",
            "  Distance: 0.00\n",
            "  Latency: 250.8ms\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: What are the rules for getting a refund?\n",
            "Canonicalized: what are the rules for getting a refund?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1712.3ms\n",
            "  Token Usage: Prompt=41, Completion=106, Total=147\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What is your refund policy?'\n",
            "    Distance: 0.33\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: What's the time limit to return an item?\n",
            "Canonicalized: what's the time limit to return an item?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 888.8ms\n",
            "  Token Usage: Prompt=41, Completion=17, Total=58\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.32\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Is it possible to swap a product for another?\n",
            "Canonicalized: is it possible to swap a product for another?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1625.1ms\n",
            "  Token Usage: Prompt=42, Completion=47, Total=89\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'Do you offer exchanges?'\n",
            "    Distance: 0.48\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: For how many days can I return stuff?\n",
            "Canonicalized: for how many days can i return stuff?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 739.2ms\n",
            "  Token Usage: Prompt=41, Completion=26, Total=67\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What's the time limit to return an item?'\n",
            "    Distance: 0.32\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Where is the closest store?\n",
            "Canonicalized: where is the closest store?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 988.2ms\n",
            "  Token Usage: Prompt=38, Completion=27, Total=65\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What's the time limit to return an item?'\n",
            "    Distance: 0.75\n",
            "    Current THRESHOLD: 0.1000\n"
          ]
        }
      ],
      "source": [
        "paraphrases = [\n",
        "    \"What is your refund policy?\",\n",
        "    \"What are the rules for getting a refund?\",\n",
        "    \"What's the time limit to return an item?\",\n",
        "    \"Is it possible to swap a product for another?\",\n",
        "    \"For how many days can I return stuff?\",\n",
        "    \"Where is the closest store?\" # Example of a query that should not hit the cache\n",
        "]\n",
        "\n",
        "print(\"\\nTesting paraphrases...\")\n",
        "for p in paraphrases:\n",
        "    print(f\"\\n--- Testing Paraphrase ---\")\n",
        "    print(f\"Original: {p}\")\n",
        "    canonical_p = canonicalize(p)\n",
        "    print(f\"Canonicalized: {canonical_p}\")\n",
        "\n",
        "    # We don't want to polute the cache while testing\n",
        "    res = answer(SYSTEM_PROMPT, p, add_to_cache=True)\n",
        "\n",
        "    if res['source'] == 'cache':\n",
        "        print(f\"Result: CACHE HIT\")\n",
        "        print(f\"  Cached Question: {res.get('user_question')}\")\n",
        "        print(f\"  Distance: {res.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "    else: # res['source'] == 'llm'\n",
        "        print(f\"Result: CACHE MISS (LLM Call)\")\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "        if res.get('usage'):\n",
        "             print(f\"  Token Usage: Prompt={res['usage'].get('prompt_tokens')}, Completion={res['usage'].get('completion_tokens')}, Total={res['usage'].get('total_tokens')}\")\n",
        "\n",
        "        # Display closest match information found *before* the LLM call\n",
        "        closest_info = res.get('closest_match_before_llm')\n",
        "        if closest_info:\n",
        "            print(f\"  Closest match in cache (before LLM call):\")\n",
        "            print(f\"    Original Cached Q: '{closest_info.get('user_question')}'\")\n",
        "            print(f\"    Distance: {closest_info.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "            print(f\"    Current THRESHOLD: {THRESH:.4f}\")\n",
        "        else:\n",
        "            print(f\"  No close match found in cache (even with high threshold) before LLM call.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qv5LJEaj0v9"
      },
      "source": [
        "### Print Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tog6pXSXj0v-",
        "outputId": "4b4a61cd-e33f-47b2-a2fe-32b71091a45e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics: {'hit_rate': 0.3333333333333333, 'p50_cache_ms': 232.2904489992652, 'p95_cache_ms': 250.7682739997108, 'p50_llm_ms': 1061.3453065016074, 'p95_llm_ms': 1625.1475069984735}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nMetrics:\", metrics.snapshot())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "f4cbc777",
        "outputId": "0b879bb1-3a68-4220-c5bd-23edfd948518"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get the snapshot of the metrics\n",
        "metrics_snapshot = metrics.snapshot()\n",
        "\n",
        "# Extract data for plotting\n",
        "labels = ['Cache (P50)', 'Cache (P95)', 'LLM (P50)', 'LLM (P95)']\n",
        "latency_values = [\n",
        "    metrics_snapshot.get('p50_cache_ms'),\n",
        "    metrics_snapshot.get('p95_cache_ms'),\n",
        "    metrics_snapshot.get('p50_llm_ms'),\n",
        "    metrics_snapshot.get('p95_llm_ms')\n",
        "]\n",
        "\n",
        "# Filter out None values if no cache hits or LLM calls occurred\n",
        "filtered_labels = [labels[i] for i in range(len(latency_values)) if latency_values[i] is not None]\n",
        "filtered_values = [value for value in latency_values if value is not None]\n",
        "\n",
        "if not filtered_values:\n",
        "    print(\"No latency data available to plot.\")\n",
        "else:\n",
        "    # Create the bar chart\n",
        "    x = np.arange(len(filtered_labels))\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    bars = ax.bar(x, filtered_values, color=['skyblue', 'deepskyblue', 'lightcoral', 'indianred'])\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_ylabel('Latency (ms)')\n",
        "    ax.set_title('Cache vs. LLM Latency (P50 and P95)')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(filtered_labels)\n",
        "    ax.set_ylim(0, max(filtered_values) * 1.2) # Set y-axis limit\n",
        "\n",
        "    # Add value labels on top of the bars\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval + 5, f'{yval:.1f}', ha='center', va='bottom')\n",
        "\n",
        "    # Display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Optionally, print the hit rate separately\n",
        "print(f\"\\nCache Hit Rate: {metrics_snapshot.get('hit_rate', 0.0):.2f}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cache Hit Rate: 0.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJZeOZFjj0v-"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "<ul>\n",
        "<li>First run: all `llm` sources, ~5001000ms latency\n",
        "</li>\n",
        "<li>Paraphrases: mostly `cache` sources, <50ms latency, distance <0.10\n",
        "</li>\n",
        "<li>Hit rate: 6080% for paraphrases\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Tuning the Similarity Threshold\n",
        "\n",
        "The threshold controls cache precision. Lower = stricter (fewer false hits), higher = more lenient (more hits, risk of incorrect matches)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbmSSSBbj0v-"
      },
      "outputs": [],
      "source": [
        "def sweep_thresholds(thresholds):\n",
        "    for t in thresholds:\n",
        "        print(f\"\\nThreshold={t}\")\n",
        "        for p in paraphrases:\n",
        "            res = cache_get_or_generate(SYSTEM_PROMPT, p, ef_runtime=150, threshold=t, add_to_cache=False)\n",
        "            distance_str = f\"{res.get('distance'):.2f}\" if res.get('distance') is not None else 'N/A'\n",
        "            cached_question_str = f\" (Cached: {res.get('user_question')})\" if res['source'] == 'cache' else ''\n",
        "            print(f\"{p} => {res['source']} dist={distance_str}{cached_question_str}\")\n",
        "\n",
        "# Assuming 'paraphrases' list is defined earlier in the notebook\n",
        "sweep_thresholds([0.06, 0.08, 0.10, 0.12, 0.14])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8x6YDIxj0v-"
      },
      "source": [
        "Start with 0.10 and adjust based on false positive rate.\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Inspect the Cache\n",
        "\n",
        "**Count indexed documents:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDJ3Iy_Jj0v_",
        "outputId": "9dee5923-57b3-40d2-b588-5a06fffab6c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cached documents: 8\n"
          ]
        }
      ],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n",
        "num_docs = info[info.index(b'num_docs') + 1]\n",
        "print(f\"Cached documents: {num_docs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWjbEHpkj0v_"
      },
      "source": [
        "**Inspect a document:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zebeV7cDj0v_",
        "outputId": "0143df77-841e-45e8-f091-9a6dd3037257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8 documents:\n",
            "\n",
            "--- Document 1 (Key: sc:v1:c8e5c9c137a16100fb8a9ef22c1fb777bb54ce0d6fb6c2d43fca7d06d18d9ba9) ---\n",
            "  created_at: 1761002535.8008976\n",
            "  user_question: What's the time limit to return an item?\n",
            "  corpus_version: v1\n",
            "  response: The time limit to return an item is 30 days from the date of purchase.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: caa2e995938c57b4448bf558b60933f95cd6d194f5069b43b157783c67b96a2e\n",
            "  last_hit_at: 1761002535.8008978\n",
            "\n",
            "--- Document 2 (Key: sc:v1:493173b9d91727a583115a82e58560182ee3f8ba4aa15b878f637e774b5df763) ---\n",
            "  created_at: 1761002332.5939157\n",
            "  user_question: How long is the return window?\n",
            "  corpus_version: v1\n",
            "  response: The return window is 30 days from the date of purchase.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 159d9f1ce048fad930a43a6b337a5476ac857e573dceed1cf53c36ed8140c193\n",
            "  last_hit_at: 1761002444.170771\n",
            "\n",
            "--- Document 3 (Key: sc:v1:336fe24d30d935b0538bc5e7ad88350e8f54bb8b7ec5e02cb135ce2ef2b05e59) ---\n",
            "  created_at: 1761002538.4214282\n",
            "  user_question: For how many days can I return stuff?\n",
            "  corpus_version: v1\n",
            "  response: You can return items within 30 days of the purchase date for a full refund, provided they are in original condition and packaging.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 90eca5306038bfdf1af3a0611a6485d6f835ba1b712cb0011c19f1bcb46a1a2e\n",
            "  last_hit_at: 1761002538.4214284\n",
            "\n",
            "--- Document 4 (Key: sc:v1:fd1a249118ff83ede5d8f4d14d19dc6720c498176a7f9a0a08636f9841a0f0fa) ---\n",
            "  created_at: 1761002333.9443336\n",
            "  user_question: Do you offer exchanges?\n",
            "  corpus_version: v1\n",
            "  response: ACME Corp does not offer exchanges. If you need a different item, please return the original product for a refund and place a new order.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: fe8a40968fc5fd2fb51084f84b99daac2e538f86fff748eee50a458729742905\n",
            "  last_hit_at: 1761002444.331585\n",
            "\n",
            "--- Document 5 (Key: sc:v1:1f86d0093a9813e762733e9f884c01c5467272b173b3e17c24ed84d75f05aa79) ---\n",
            "  created_at: 1761002534.6564517\n",
            "  user_question: What are the rules for getting a refund?\n",
            "  corpus_version: v1\n",
            "  response: To qualify for a refund at ACME Corp, the following rules apply:\n",
            "\n",
            "1. **Time Frame**: Requests must be made within 30 days of purchase.\n",
            "2. **Condition**: Items must be in original condition, unused, and in their original packaging.\n",
            "3. **Proof of Purchase**: A receipt or order confirmation is required.\n",
            "4. **Exclusions**: Certain items, such as personalized products or digital downloads, may not be eligible for refunds.\n",
            "\n",
            "Please ensure you meet these criteria when requesting a refund.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 61e100d35be163e2b24e8343afd1574ecd98f2e8b5c442e91282da0c05f85540\n",
            "  last_hit_at: 1761002534.656452\n",
            "\n",
            "--- Document 6 (Key: sc:v1:a49d9decb96d02fa87711c7ca47e29c5a2615511aa50db725dd160e008579f7b) ---\n",
            "  created_at: 1761002539.6424563\n",
            "  user_question: Where is the closest store?\n",
            "  corpus_version: v1\n",
            "  response: I don't have access to store locations. Please visit our website or use the store locator feature to find the closest ACME Corp store.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 87fdf43e323e65f8bae9f6944a7e1e22af540e90f2ae9346789087508abfa6ca\n",
            "  last_hit_at: 1761002539.6424563\n",
            "\n",
            "--- Document 7 (Key: sc:v1:71f8a352abd2593556cf47089f0618ed0e4f184e490218e7545127147c68eb99) ---\n",
            "  created_at: 1761002331.7866697\n",
            "  user_question: What is your refund policy?\n",
            "  corpus_version: v1\n",
            "  response: ACME Corp's refund policy allows customers to request a refund within 30 days of purchase. Items must be in original condition and packaging. To initiate a refund, please contact our customer service with your order details. Certain items may be non-refundable, such as personalized or clearance items. For more specific inquiries, please refer to our detailed policy or contact support.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: b92b567fdfe4b52a6f2f93624b704f18c64c3212e2ee688b7dd2f68f10a81a50\n",
            "  last_hit_at: 1761002532.739657\n",
            "\n",
            "--- Document 8 (Key: sc:v1:53be35cec9fdc4ea25bb97e0dad6453653175495796272a771f0373351893006) ---\n",
            "  created_at: 1761002537.56585\n",
            "  user_question: Is it possible to swap a product for another?\n",
            "  corpus_version: v1\n",
            "  response: No, ACME Corp does not offer product swaps. If you wish to return a product, you can initiate a return and then place a new order for the desired item. Please refer to our refund and return policy for more details.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 76f867eed9447dc297ba3d54ea5cfae0ad4f7301fe3db626f04baf39a5fb609c\n",
            "  last_hit_at: 1761002537.5658503\n"
          ]
        }
      ],
      "source": [
        "def print_cached_documents(max_docs: int = None):\n",
        "    keys = r.keys(f\"{NS}*\")\n",
        "    if keys:\n",
        "        print(f\"Found {len(keys)} documents:\")\n",
        "        # Limit the keys to iterate if max_docs is specified\n",
        "        keys_to_print = keys[:max_docs] if max_docs is not None else keys\n",
        "\n",
        "        for i, key in enumerate(keys_to_print):\n",
        "            print(f\"\\n--- Document {i+1} (Key: {key.decode()}) ---\")\n",
        "            doc = r.hgetall(key)\n",
        "            # Decode bytes to string for all fields except 'vector' and print them\n",
        "            for k, v in doc.items():\n",
        "                decoded_key = k.decode()\n",
        "                if decoded_key == \"vector\":\n",
        "                    # Skip printing the vector field\n",
        "                    continue\n",
        "                else:\n",
        "                    # Decode other fields and print with a label\n",
        "                    decoded_value = v.decode() if isinstance(v, bytes) else v\n",
        "                    print(f\"  {decoded_key}: {decoded_value}\")\n",
        "        if max_docs is not None and len(keys) > max_docs:\n",
        "            print(f\"\\n... and {len(keys) - max_docs} more documents (showing first {max_docs})\")\n",
        "    else:\n",
        "        print(\"No documents found in the index.\")\n",
        "\n",
        "# Call the function to print documents (prints all by default)\n",
        "print_cached_documents()\n",
        "\n",
        "# Example of printing only the first 3 documents:\n",
        "# print_cached_documents(max_docs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1n83Loqj0wE"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "You've built a production-grade semantic cache with Redis Vector. The system normalizes queries, generates embeddings, performs fast vector search, and returns cached responses when similarity is high. This approach cuts latency by 1020x and reduces LLM costs by 6080% for repeated queries.\n",
        "\n",
        "**Key design decisions:**\n",
        "\n",
        "<ul>\n",
        "<li>**Canonicalization** stabilizes cache keys across paraphrases\n",
        "</li>\n",
        "<li>**HNSW indexing** enables sub-50ms vector search at scale\n",
        "</li>\n",
        "<li>**Metadata gating** ensures cache hits respect model, temperature, and system prompt changes\n",
        "</li>\n",
        "<li>**TTL and namespace versioning** provide safe invalidation paths\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "**Next steps:**\n",
        "\n",
        "<ul>\n",
        "<li>Add query-side metadata filters in `FT.SEARCH` to reduce false candidates (e.g., `@model:{gpt-4o-mini} @sys_hash:{<hash>}`)\n",
        "</li>\n",
        "<li>Integrate Prometheus and Grafana for observability (track hit rate, p95 latency, cache size)\n",
        "</li>\n",
        "<li>Implement LRU eviction or score-based pruning for long-running caches\n",
        "</li>\n",
        "<li>Explore quantization (FLOAT16) to reduce memory footprint\n",
        "</li>\n",
        "<li>Scale with Redis Cluster for multi-tenant or high-throughput workloads\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "For more on building intelligent systems, see our guides on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/build-rag-pipeline\">building a RAG pipeline</a> and <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/optimize-llm-context\">optimizing LLM context windows</a>."
      ]
    }
  ],
  "metadata": {
    "title": "Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs",
    "description": "Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}