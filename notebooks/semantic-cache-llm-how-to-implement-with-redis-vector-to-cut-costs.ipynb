{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d90e7ac-331f-4b01-fb5e-a7d64064bf3c",
        "id": "fwJkcN3cHAqG"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing paraphrases...\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Can I get a refund? What's the policy?\n",
            "Canonicalized: can i get a refund? what's the policy?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1128.1ms\n",
            "  Token Usage: Prompt=42, Completion=50, Total=92\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What is our refund policy?'\n",
            "    Distance: 0.20\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: What's the time limit to return an item?\n",
            "Canonicalized: what's the time limit to return an item?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 741.0ms\n",
            "  Token Usage: Prompt=41, Completion=17, Total=58\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.32\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Is it possible to swap a product for another?\n",
            "Canonicalized: is it possible to swap a product for another?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1201.5ms\n",
            "  Token Usage: Prompt=42, Completion=53, Total=95\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'Do you offer exchanges?'\n",
            "    Distance: 0.48\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: For how many days can I return stuff?\n",
            "Canonicalized: for how many days can i return stuff?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 598.6ms\n",
            "  Token Usage: Prompt=41, Completion=26, Total=67\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.36\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Where is the closest store?\n",
            "Canonicalized: where is the closest store?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 2028.0ms\n",
            "  Token Usage: Prompt=38, Completion=26, Total=64\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.77\n",
            "    Current THRESHOLD: 0.1000\n"
          ]
        }
      ],
      "source": [
        "paraphrases = [\n",
        "    \"Can I get a refund? What's the policy?\",\n",
        "    \"What's the time limit to return an item?\",\n",
        "    \"Is it possible to swap a product for another?\",\n",
        "    \"For how many days can I return stuff?\",\n",
        "    \"Where is the closest store?\" # Example of a query that should not hit the cache\n",
        "]\n",
        "\n",
        "print(\"\\nTesting paraphrases...\")\n",
        "for p in paraphrases:\n",
        "    print(f\"\\n--- Testing Paraphrase ---\")\n",
        "    print(f\"Original: {p}\")\n",
        "    canonical_p = canonicalize(p)\n",
        "    print(f\"Canonicalized: {canonical_p}\")\n",
        "\n",
        "    res = answer(SYSTEM_PROMPT, p, add_to_cache=False)\n",
        "\n",
        "    if res['source'] == 'cache':\n",
        "        print(f\"Result: CACHE HIT\")\n",
        "        print(f\"  Cached Question: {res.get('user_question')}\")\n",
        "        print(f\"  Distance: {res.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "    else: # res['source'] == 'llm'\n",
        "        print(f\"Result: CACHE MISS (LLM Call)\")\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "        if res.get('usage'):\n",
        "             print(f\"  Token Usage: Prompt={res['usage'].get('prompt_tokens')}, Completion={res['usage'].get('completion_tokens')}, Total={res['usage'].get('total_tokens')}\")\n",
        "\n",
        "        # Display closest match information found *before* the LLM call\n",
        "        closest_info = res.get('closest_match_before_llm')\n",
        "        if closest_info:\n",
        "            print(f\"  Closest match in cache (before LLM call):\")\n",
        "            print(f\"    Original Cached Q: '{closest_info.get('user_question')}'\")\n",
        "            print(f\"    Distance: {closest_info.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "            print(f\"    Current THRESHOLD: {THRESH:.4f}\")\n",
        "        else:\n",
        "            print(f\"  No close match found in cache (even with high threshold) before LLM call.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d90e7ac-331f-4b01-fb5e-a7d64064bf3c",
        "id": "MAd7zKC0HCmA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing paraphrases...\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Can I get a refund? What's the policy?\n",
            "Canonicalized: can i get a refund? what's the policy?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1128.1ms\n",
            "  Token Usage: Prompt=42, Completion=50, Total=92\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What is our refund policy?'\n",
            "    Distance: 0.20\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: What's the time limit to return an item?\n",
            "Canonicalized: what's the time limit to return an item?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 741.0ms\n",
            "  Token Usage: Prompt=41, Completion=17, Total=58\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.32\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Is it possible to swap a product for another?\n",
            "Canonicalized: is it possible to swap a product for another?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1201.5ms\n",
            "  Token Usage: Prompt=42, Completion=53, Total=95\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'Do you offer exchanges?'\n",
            "    Distance: 0.48\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: For how many days can I return stuff?\n",
            "Canonicalized: for how many days can i return stuff?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 598.6ms\n",
            "  Token Usage: Prompt=41, Completion=26, Total=67\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.36\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Where is the closest store?\n",
            "Canonicalized: where is the closest store?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 2028.0ms\n",
            "  Token Usage: Prompt=38, Completion=26, Total=64\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.77\n",
            "    Current THRESHOLD: 0.1000\n"
          ]
        }
      ],
      "source": [
        "paraphrases = [\n",
        "    \"Can I get a refund? What's the policy?\",\n",
        "    \"What's the time limit to return an item?\",\n",
        "    \"Is it possible to swap a product for another?\",\n",
        "    \"For how many days can I return stuff?\",\n",
        "    \"Where is the closest store?\" # Example of a query that should not hit the cache\n",
        "]\n",
        "\n",
        "print(\"\\nTesting paraphrases...\")\n",
        "for p in paraphrases:\n",
        "    print(f\"\\n--- Testing Paraphrase ---\")\n",
        "    print(f\"Original: {p}\")\n",
        "    canonical_p = canonicalize(p)\n",
        "    print(f\"Canonicalized: {canonical_p}\")\n",
        "\n",
        "    res = answer(SYSTEM_PROMPT, p, add_to_cache=False)\n",
        "\n",
        "    if res['source'] == 'cache':\n",
        "        print(f\"Result: CACHE HIT\")\n",
        "        print(f\"  Cached Question: {res.get('user_question')}\")\n",
        "        print(f\"  Distance: {res.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "    else: # res['source'] == 'llm'\n",
        "        print(f\"Result: CACHE MISS (LLM Call)\")\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "        if res.get('usage'):\n",
        "             print(f\"  Token Usage: Prompt={res['usage'].get('prompt_tokens')}, Completion={res['usage'].get('completion_tokens')}, Total={res['usage'].get('total_tokens')}\")\n",
        "\n",
        "        # Display closest match information found *before* the LLM call\n",
        "        closest_info = res.get('closest_match_before_llm')\n",
        "        if closest_info:\n",
        "            print(f\"  Closest match in cache (before LLM call):\")\n",
        "            print(f\"    Original Cached Q: '{closest_info.get('user_question')}'\")\n",
        "            print(f\"    Distance: {closest_info.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "            print(f\"    Current THRESHOLD: {THRESH:.4f}\")\n",
        "        else:\n",
        "            print(f\"  No close match found in cache (even with high threshold) before LLM call.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d90e7ac-331f-4b01-fb5e-a7d64064bf3c",
        "id": "ZGeCS3dbHC5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing paraphrases...\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Can I get a refund? What's the policy?\n",
            "Canonicalized: can i get a refund? what's the policy?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1128.1ms\n",
            "  Token Usage: Prompt=42, Completion=50, Total=92\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What is our refund policy?'\n",
            "    Distance: 0.20\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: What's the time limit to return an item?\n",
            "Canonicalized: what's the time limit to return an item?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 741.0ms\n",
            "  Token Usage: Prompt=41, Completion=17, Total=58\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.32\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Is it possible to swap a product for another?\n",
            "Canonicalized: is it possible to swap a product for another?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1201.5ms\n",
            "  Token Usage: Prompt=42, Completion=53, Total=95\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'Do you offer exchanges?'\n",
            "    Distance: 0.48\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: For how many days can I return stuff?\n",
            "Canonicalized: for how many days can i return stuff?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 598.6ms\n",
            "  Token Usage: Prompt=41, Completion=26, Total=67\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.36\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Where is the closest store?\n",
            "Canonicalized: where is the closest store?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 2028.0ms\n",
            "  Token Usage: Prompt=38, Completion=26, Total=64\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.77\n",
            "    Current THRESHOLD: 0.1000\n"
          ]
        }
      ],
      "source": [
        "paraphrases = [\n",
        "    \"Can I get a refund? What's the policy?\",\n",
        "    \"What's the time limit to return an item?\",\n",
        "    \"Is it possible to swap a product for another?\",\n",
        "    \"For how many days can I return stuff?\",\n",
        "    \"Where is the closest store?\" # Example of a query that should not hit the cache\n",
        "]\n",
        "\n",
        "print(\"\\nTesting paraphrases...\")\n",
        "for p in paraphrases:\n",
        "    print(f\"\\n--- Testing Paraphrase ---\")\n",
        "    print(f\"Original: {p}\")\n",
        "    canonical_p = canonicalize(p)\n",
        "    print(f\"Canonicalized: {canonical_p}\")\n",
        "\n",
        "    res = answer(SYSTEM_PROMPT, p, add_to_cache=False)\n",
        "\n",
        "    if res['source'] == 'cache':\n",
        "        print(f\"Result: CACHE HIT\")\n",
        "        print(f\"  Cached Question: {res.get('user_question')}\")\n",
        "        print(f\"  Distance: {res.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "    else: # res['source'] == 'llm'\n",
        "        print(f\"Result: CACHE MISS (LLM Call)\")\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "        if res.get('usage'):\n",
        "             print(f\"  Token Usage: Prompt={res['usage'].get('prompt_tokens')}, Completion={res['usage'].get('completion_tokens')}, Total={res['usage'].get('total_tokens')}\")\n",
        "\n",
        "        # Display closest match information found *before* the LLM call\n",
        "        closest_info = res.get('closest_match_before_llm')\n",
        "        if closest_info:\n",
        "            print(f\"  Closest match in cache (before LLM call):\")\n",
        "            print(f\"    Original Cached Q: '{closest_info.get('user_question')}'\")\n",
        "            print(f\"    Distance: {closest_info.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "            print(f\"    Current THRESHOLD: {THRESH:.4f}\")\n",
        "        else:\n",
        "            print(f\"  No close match found in cache (even with high threshold) before LLM call.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQhv5b9Vj0vw"
      },
      "source": [
        "# üìì The GenAI Revolution Cookbook\n",
        "\n",
        "**Title:** Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs\n",
        "\n",
        "**Description:** Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.\n",
        "\n",
        "---\n",
        "\n",
        "*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFnUUE6Bj0vx"
      },
      "source": [
        "Most LLM applications waste money and time answering the same question phrased slightly differently. A semantic cache solves this by recognizing when a new query is semantically similar to a previous one and returning the cached response instantly, eliminating the need for an LLM call.\n",
        "\n",
        "This guide walks you through building a production-grade semantic cache using embeddings and Redis Vector. You'll create a Redis-backed semantic cache, complete with thresholds, TTLs, and metrics. By the end, you'll have working code, a tunable architecture, and a clear path to immediate latency and cost reductions.\n",
        "\n",
        "**What you'll build:**\n",
        "\n",
        "<ul>\n",
        "<li>A Redis HNSW vector index for semantic similarity search\n",
        "</li>\n",
        "<li>A cache layer that normalizes queries, generates embeddings, and retrieves cached responses\n",
        "</li>\n",
        "<li>A demo script to validate cache hit rates and latency improvements\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "<ul>\n",
        "<li>Python 3.9+\n",
        "</li>\n",
        "<li>Redis Stack (local via Docker or managed Redis Cloud)\n",
        "</li>\n",
        "<li>OpenAI API key\n",
        "</li>\n",
        "<li>Basic familiarity with embeddings and vector search\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "If you're using Google Colab or a cloud notebook, connect to a managed Redis Stack instance (e.g., Redis Cloud) instead of running Docker locally.\n",
        "\n",
        "For a deeper understanding of how LLMs manage memory and the concept of context rot, see our article on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/context-rot-why-llms-forget-as-their-memory-grows\">why LLMs \"forget\" as their memory grows</a>.\n",
        "\n",
        "<hr>\n",
        "\n",
        "## How It Works (High-Level Overview)\n",
        "\n",
        "**The paraphrase problem:** Users ask the same question in many ways. \"What's your refund policy?\" and \"Can I get my money back?\" are semantically identical, but traditional caching treats them as different keys.\n",
        "\n",
        "**The embedding advantage:** Embeddings map text into a high-dimensional vector space where semantically similar phrases cluster together. By comparing query embeddings using cosine similarity, you can detect paraphrases and return cached responses.\n",
        "\n",
        "**Why Redis Vector:** Redis Stack provides HNSW (Hierarchical Navigable Small World) indexing for fast approximate nearest neighbor search. It combines low-latency vector search with Redis's native TTL, tagging, and filtering capabilities, making it ideal for production caching.\n",
        "\n",
        "**Architecture:**\n",
        "\n",
        "<ol>\n",
        "<li>Normalize the user query (lowercase, strip volatile patterns like timestamps)\n",
        "</li>\n",
        "<li>Generate an embedding for the normalized query\n",
        "</li>\n",
        "<li>Search the Redis HNSW index for the nearest cached embedding\n",
        "</li>\n",
        "<li>If distance < threshold and metadata matches (model, temperature, system prompt hash), return the cached response\n",
        "</li>\n",
        "<li>Otherwise, call the LLM, cache the new response with its embedding, and return it\n",
        "</li>\n",
        "</ol>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Setup & Installation\n",
        "\n",
        "### Option 1: Managed Redis (Recommended for Notebooks)\n",
        "\n",
        "Sign up for a free Redis Cloud account at <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://redis.com/try-free\">redis.com/try-free</a> and create a Redis Stack database. Copy the connection URL.\n",
        "\n",
        "In your notebook or terminal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L-n9TWyj0vz",
        "outputId": "6d8b4a8f-e4e4-4fd7-b47e-cb4bbe13ff70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting redis\n",
            "  Downloading redis-6.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Downloading redis-6.4.0-py3-none-any.whl (279 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m279.8/279.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: redis\n",
            "Successfully installed redis-6.4.0\n"
          ]
        }
      ],
      "source": [
        "%pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjHOIKHSj0v0"
      },
      "source": [
        "Set environment variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6NP21I1bj0v0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"REDIS_URL\"] = \"redis://default:password@your-redis-host:port\"\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "os.environ[\"EMBEDDING_MODEL\"] = \"text-embedding-3-small\"\n",
        "os.environ[\"CHAT_MODEL\"] = \"gpt-4o-mini\"\n",
        "os.environ[\"SIMILARITY_THRESHOLD\"] = \"0.10\"\n",
        "os.environ[\"TOP_K\"] = \"5\"\n",
        "os.environ[\"CACHE_TTL_SECONDS\"] = \"86400\"\n",
        "os.environ[\"CACHE_NAMESPACE\"] = \"sc:v1:\"\n",
        "os.environ[\"CORPUS_VERSION\"] = \"v1\"\n",
        "os.environ[\"TEMPERATURE\"] = \"0.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QktgskEcj0v1"
      },
      "source": [
        "### Option 2: Local Redis with Docker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbHhqS92j0v1"
      },
      "outputs": [],
      "source": [
        "docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J56cAYaUj0v2"
      },
      "source": [
        "Create a `.env` file:\n",
        "\n",
        "<pre><code>REDIS_URL=redis://localhost:6379\n",
        "OPENAI_API_KEY=sk-...\n",
        "EMBEDDING_MODEL=text-embedding-3-small\n",
        "CHAT_MODEL=gpt-4o-mini\n",
        "SIMILARITY_THRESHOLD=0.10\n",
        "TOP_K=5\n",
        "CACHE_TTL_SECONDS=86400\n",
        "CACHE_NAMESPACE=sc:v1:\n",
        "CORPUS_VERSION=v1\n",
        "TEMPERATURE=0.2\n",
        "</code></pre>\n",
        "\n",
        "Install dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc7pGvZlj0v2"
      },
      "outputs": [],
      "source": [
        "pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3zXkp7yj0v3"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Step-by-Step Implementation\n",
        "\n",
        "### Step 1: Create the Redis HNSW Index\n",
        "\n",
        "The index stores embeddings and metadata for cached responses. We use HNSW for fast approximate nearest neighbor search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46FV0WrQj0v3",
        "outputId": "edf442e8-208c-4379-bf1a-8fa3b6f99ee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using index name: sc_index\n",
            "Dropped existing index 'sc_index' including documents.\n",
            "Index 'sc_index' created.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import redis\n",
        "import time\n",
        "\n",
        "r = redis.Redis.from_url(os.getenv(\"REDIS_URL\"))\n",
        "\n",
        "INDEX = \"sc_index\" # Make sure to update this variable if you want a different index name\n",
        "PREFIX = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "DIM = 1536  # Dimension for text-embedding-3-small\n",
        "M = 16  # HNSW graph connectivity\n",
        "EF_CONSTRUCTION = 200  # HNSW construction quality\n",
        "\n",
        "def create_index():\n",
        "    print(f\"Using index name: {INDEX}\") # Print the index name being used\n",
        "\n",
        "    # Drop index if it exists, and delete associated documents (DD)\n",
        "    try:\n",
        "        r.execute_command(\"FT.DROPINDEX\", INDEX, \"DD\")\n",
        "        print(f\"Dropped existing index '{INDEX}' including documents.\")\n",
        "    except redis.ResponseError:\n",
        "        print(f\"Index '{INDEX}' did not exist, proceeding with creation.\")\n",
        "        pass # Index does not exist, safe to ignore\n",
        "\n",
        "    # Create index with vector field and metadata tags\n",
        "    cmd = [\n",
        "        \"FT.CREATE\", INDEX,  # Command to create a full-text search index with the given name\n",
        "        \"ON\", \"HASH\",  # Index applies to Redis Hash data structures\n",
        "        \"PREFIX\", \"1\", PREFIX,  # Only index keys starting with the defined prefix\n",
        "        \"SCHEMA\",  # Define the schema of the index\n",
        "        \"prompt_hash\", \"TAG\",  # Tag field for hashing the canonicalized prompt\n",
        "        \"model\", \"TAG\",  # Tag field for the LLM model used\n",
        "        \"sys_hash\", \"TAG\",  # Tag field for hashing the system prompt\n",
        "        \"corpus_version\", \"TAG\",  # Tag field for tracking the version of the underlying corpus\n",
        "        \"temperature\", \"NUMERIC\",  # Numeric field for the temperature parameter used by the LLM\n",
        "        \"created_at\", \"NUMERIC\",  # Numeric field for the creation timestamp\n",
        "        \"last_hit_at\", \"NUMERIC\",  # Numeric field for the timestamp of the last cache hit\n",
        "        \"response\", \"TEXT\",  # Text field for the LLM's response\n",
        "        \"user_question\", \"TEXT\", # Text field for the original user question\n",
        "        \"vector\", \"VECTOR\", \"HNSW\", \"10\",  # Define a vector field named \"vector\" using the HNSW algorithm. \"10\" specifies the number of pairs for the HNSW vector definition.\n",
        "        \"TYPE\", \"FLOAT32\",  # Specify the data type of the vector embeddings\n",
        "        \"DIM\", str(DIM),  # Specify the dimension of the vector embeddings\n",
        "        \"DISTANCE_METRIC\", \"COSINE\",  # Specify the distance metric to use for vector similarity search\n",
        "        \"M\", str(M),  # HNSW parameter: number of established connections for each element during graph construction\n",
        "        \"EF_CONSTRUCTION\", str(EF_CONSTRUCTION),  # HNSW parameter: size of the dynamic list for heuristic search during graph construction\n",
        "    ]\n",
        "    r.execute_command(*cmd)\n",
        "    print(f\"Index '{INDEX}' created.\")\n",
        "\n",
        "create_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnf6RGpNj0v4"
      },
      "source": [
        "**Validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCqqWYtRj0v4",
        "outputId": "4f31c17b-2779-4f45-9209-065128c3d482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index Info:\n",
            "  index_name: sc_index\n",
            "  num_docs: 0\n"
          ]
        }
      ],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n",
        "\n",
        "# Helper function to decode bytes to string\n",
        "def decode_bytes(item):\n",
        "    if isinstance(item, bytes):\n",
        "        return item.decode()\n",
        "    return item\n",
        "\n",
        "# Parse the info output for better readability\n",
        "parsed_info = {}\n",
        "for i in range(0, len(info), 2):\n",
        "    key = decode_bytes(info[i])\n",
        "    value = info[i+1]\n",
        "    if isinstance(value, list):\n",
        "        # Decode lists of bytes\n",
        "        parsed_info[key] = [decode_bytes(item) for item in value]\n",
        "    else:\n",
        "        parsed_info[key] = decode_bytes(value)\n",
        "\n",
        "print(\"Index Info:\")\n",
        "print(f\"  index_name: {parsed_info.get('index_name')}\")\n",
        "print(f\"  num_docs: {parsed_info.get('num_docs')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkYR0jDUj0v4"
      },
      "source": [
        "You should see `num_docs: 0` initially.\n",
        "\n",
        "<hr>\n",
        "\n",
        "### Step 2: Normalize Queries for Stable Cache Keys\n",
        "\n",
        "Canonicalization removes volatile elements (timestamps, UUIDs, IDs) and normalizes whitespace to ensure paraphrases map to the same cache key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "Reod8eyHj0v5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import hashlib\n",
        "\n",
        "# Note: Normalization adequacy depends on expected query variations and embedding model robustness.\n",
        "VOLATILE_PATTERNS = [\n",
        "    # ISO timestamps and variations\n",
        "    r\"\\b\\d{4}-\\d{2}-\\d{2}(T|\\s)\\d{2}:\\d{2}(:\\d{2})?(Z|[+-]\\d{2}:\\d{2})?\\b\",\n",
        "    # Common date formats (MM/DD/YYYY, DD/MM/YYYY, YYYY/MM/DD, YYYY-MM-DD)\n",
        "    r\"\\b\\d{1,4}[-/.]?\\d{1,2}[-/.]?\\d{2,4}\\b\", # Updated to be more flexible with separators and year length\n",
        "    # UUID v4\n",
        "    r\"\\b[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\\b\",\n",
        "    # Long IDs (6+ digits)\n",
        "    r\"\\b\\d{6,}\\b\",\n",
        "    # Email addresses (often contain volatile parts or personally identifiable info)\n",
        "    r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\",\n",
        "]\n",
        "\n",
        "def canonicalize(text: str) -> str:\n",
        "    t = text.strip().lower()\n",
        "    for pat in VOLATILE_PATTERNS:\n",
        "        t = re.sub(pat, \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def sha256(s: str) -> str:\n",
        "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def scope_hash(prompt_norm: str, model: str, sys_hash: str, temperature: float, corpus_version: str) -> str:\n",
        "    # Unique hash for cache scope including all parameters\n",
        "    payload = f\"{prompt_norm}|{model}|{sys_hash}|{temperature}|{corpus_version}\"\n",
        "    return sha256(payload)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fycERspj0v5"
      },
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um2EQfnej0v5",
        "outputId": "b30f661e-7f19-4ee9-c0ed-5c88b4bac0d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is our refund policy on ?\n",
            "what is our refund policy on ?\n"
          ]
        }
      ],
      "source": [
        "q1 = \"What is our refund policy on 2025-01-15?\"\n",
        "q2 = \"what is our refund policy on 2025-01-20?\"\n",
        "print(canonicalize(q1))\n",
        "print(canonicalize(q2))\n",
        "# Both should output: \"what is our refund policy on\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilAUmXFkj0v6"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 3: Initialize Clients and Embedding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "Kk9WtNFoj0v6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "EMBED_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
        "CHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")\n",
        "THRESH = float(os.getenv(\"SIMILARITY_THRESHOLD\", 0.10))\n",
        "TOP_K = int(os.getenv(\"TOP_K\", 5))\n",
        "TTL = int(os.getenv(\"CACHE_TTL_SECONDS\", 86400))\n",
        "NS = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "CORPUS_VERSION = os.getenv(\"CORPUS_VERSION\", \"v1\")\n",
        "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.2))\n",
        "\n",
        "def embed(text: str) -> np.ndarray:\n",
        "    # Generate embedding and normalize for cosine distance\n",
        "    e = client.embeddings.create(model=EMBED_MODEL, input=text)\n",
        "    vec = np.array(e.data[0].embedding, dtype=np.float32)\n",
        "    norm = np.linalg.norm(vec)\n",
        "    return vec / max(norm, 1e-12)\n",
        "\n",
        "def to_bytes(vec: np.ndarray) -> bytes:\n",
        "    return vec.astype(np.float32).tobytes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywGrUE6Lj0v6"
      },
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCa7lNNcj0v7",
        "outputId": "3596d653-b53f-484b-baaa-8243352c1f8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: (1536,), norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "test_vec = embed(\"hello world\")\n",
        "print(f\"Embedding shape: {test_vec.shape}, norm: {np.linalg.norm(test_vec):.4f}\")\n",
        "# Should output shape (1536,) and norm ~1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QzDCsOXj0v7"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 4: Implement Vector Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "A_DoW81jj0v7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "def vector_search(query_vec, ef_runtime: int = 100, threshold: float = THRESH) -> Optional[Tuple[str, Dict[str, Any], float]]:\n",
        "    # Perform KNN search with EF_RUNTIME parameter\n",
        "    # Define the parameters for the search query\n",
        "    params = [\"vec\", to_bytes(query_vec), \"ef_runtime\", ef_runtime]\n",
        "    # Define the search query using RediSearch's query syntax\n",
        "    # * => search all documents\n",
        "    # [KNN {TOP_K} @vector $vec ...] => find the TOP_K nearest neighbors to the vector $vec\n",
        "    # AS score => return the score (distance) as \"score\"\n",
        "    # EF_RUNTIME $ef_runtime => specify the ef_runtime parameter for HNSW search\n",
        "    q = f\"*=>[KNN {TOP_K} @vector $vec EF_RUNTIME $ef_runtime AS score]\"\n",
        "    try:\n",
        "        # Execute the RediSearch query\n",
        "        res = r.execute_command(\n",
        "            \"FT.SEARCH\", INDEX, # Index name\n",
        "            q, \"PARAMS\", str(len(params)), *params, # Query and parameters\n",
        "            \"SORTBY\", \"score\", \"ASC\", # Sort results by score in ascending order (smaller distance is better)\n",
        "            \"RETURN\", \"8\", \"response\", \"model\", \"sys_hash\", \"corpus_version\", \"temperature\", \"prompt_hash\", \"score\", \"user_question\", # Return these fields, added \"user_question\"\n",
        "            \"DIALECT\", \"2\" # Use dialect 2 for parameters\n",
        "        )\n",
        "    except redis.RedisError:\n",
        "        # Handle Redis errors during search\n",
        "        return None\n",
        "\n",
        "    # Process the search results\n",
        "    total = res[0] if res else 0 # Total number of results (should be 1 if a match is found)\n",
        "    if total < 1:\n",
        "        # No results found\n",
        "        return None\n",
        "\n",
        "    # Extract document id and fields from the result\n",
        "    doc_id = res[1]\n",
        "    fields = res[2]\n",
        "    # Convert field names and values from bytes to strings\n",
        "    f = {fields[i].decode() if isinstance(fields[i], bytes) else fields[i]:\n",
        "         fields[i+1].decode() if isinstance(fields[i+1], bytes) else fields[i+1]\n",
        "         for i in range(0, len(fields), 2)}\n",
        "\n",
        "    try:\n",
        "        # Extract the score (distance)\n",
        "        distance = float(f[\"score\"])\n",
        "    except Exception:\n",
        "        # Handle error in extracting score\n",
        "        distance = 1.0\n",
        "\n",
        "    # Return the document id, fields, and distance\n",
        "    return doc_id.decode() if isinstance(doc_id, bytes) else doc_id, f, distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhzCtHhyj0v7"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 5: Build the Cache Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "DgRuimkRj0v8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "def sys_hash(system_prompt: str) -> str:\n",
        "    # Generates a SHA256 hash of the system prompt\n",
        "    return sha256(system_prompt.strip())\n",
        "\n",
        "def key(doc_id_hash: str) -> str:\n",
        "    # Creates a Redis key with a namespace prefix\n",
        "    return f\"{NS}{doc_id_hash}\"\n",
        "\n",
        "def metadata_matches(f: Dict[str, Any], model: str, sys_h: str, temp: float, corpus: str) -> bool:\n",
        "    # Checks if the metadata from a cached document matches the current query parameters\n",
        "    try:\n",
        "        if f.get(\"model\") != model: return False\n",
        "        if f.get(\"sys_hash\") != sys_h: return False\n",
        "        # Compare temperatures with a tolerance for floating point precision\n",
        "        if abs(float(f.get(\"temperature\", temp)) - temp) > 1e-6: return False\n",
        "        if f.get(\"corpus_version\") != corpus: return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        # Return False if there's an error during metadata comparison\n",
        "        return False\n",
        "\n",
        "def chat_call(system_prompt: str, user_prompt: str):\n",
        "    # Calls the OpenAI chat completion API\n",
        "    t0 = time.perf_counter()\n",
        "    resp = client.chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        temperature=TEMPERATURE,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "    )\n",
        "    latency_ms = (time.perf_counter() - t0) * 1000\n",
        "    content = resp.choices[0].message.content\n",
        "    usage = getattr(resp, \"usage\", None)\n",
        "    return content, latency_ms, usage\n",
        "\n",
        "def cache_get_or_generate(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH, add_to_cache: bool = True):\n",
        "    # Attempts to retrieve a response from the cache; if not found, calls the LLM and caches the response (optionally)\n",
        "    t0 = time.perf_counter()\n",
        "    sp_hash = sys_hash(system_prompt)\n",
        "    prompt_norm = canonicalize(user_prompt)\n",
        "    p_hash = sha256(prompt_norm)\n",
        "\n",
        "    qvec = embed(prompt_norm)\n",
        "\n",
        "    # --- Cache Lookup ---\n",
        "    res = vector_search(qvec, ef_runtime=ef_runtime, threshold=threshold)\n",
        "\n",
        "    # Check if a cached response was found and if its metadata matches\n",
        "    if res:\n",
        "        doc_id, fields, distance = res\n",
        "        if distance < threshold and metadata_matches(fields, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION):\n",
        "            try:\n",
        "                # Update the last hit timestamp for cache freshness\n",
        "                r.hset(doc_id, mapping={\"last_hit_at\": time.time()})\n",
        "            except redis.RedisError:\n",
        "                # Handle potential Redis errors during hset\n",
        "                pass\n",
        "            # Return the cached response details\n",
        "            return {\n",
        "                \"source\": \"cache\",\n",
        "                \"response\": fields[\"response\"],\n",
        "                \"user_question\": fields[\"user_question\"], # Include user_question for cache hits\n",
        "                \"distance\": distance,\n",
        "                \"latency_ms\": (time.perf_counter() - t0) * 1000,\n",
        "                \"closest_match_before_llm\": None # No pre-LLM closest match info on a cache hit\n",
        "            }\n",
        "\n",
        "    # --- Cache Miss - Call LLM and Cache (Optionally) ---\n",
        "\n",
        "    # If no cache hit, perform a debugging search for the closest match *before* adding the new item\n",
        "    closest_res_before_llm = vector_search(qvec, ef_runtime=ef_runtime, threshold=1.0) # Use high threshold to find closest regardless of match\n",
        "\n",
        "    content, llm_latency_ms, usage = chat_call(system_prompt, user_prompt)\n",
        "\n",
        "    # Only add to cache if add_to_cache is True\n",
        "    if add_to_cache:\n",
        "        # Generate a unique key for the new cache entry\n",
        "        doc_scope = scope_hash(prompt_norm, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION)\n",
        "        redis_key = key(doc_scope)\n",
        "\n",
        "        try:\n",
        "            # Prepare data to be stored in Redis Hash\n",
        "            mapping = {\n",
        "                \"prompt_hash\": p_hash,\n",
        "                \"model\": CHAT_MODEL,\n",
        "                \"sys_hash\": sp_hash,\n",
        "                \"corpus_version\": CORPUS_VERSION,\n",
        "                \"temperature\": TEMPERATURE,\n",
        "                \"created_at\": time.time(),\n",
        "                \"last_hit_at\": time.time(),\n",
        "                \"response\": content,\n",
        "                \"user_question\": user_prompt,\n",
        "                \"vector\": to_bytes(qvec), # Store the embedding as bytes\n",
        "            }\n",
        "            # Use a pipeline for atomic HSET and EXPIRE operations\n",
        "            pipe = r.pipeline(transaction=True)\n",
        "            pipe.hset(redis_key, mapping=mapping)\n",
        "            pipe.expire(redis_key, int(TTL)) # Set the time-to-live for the cache entry\n",
        "            pipe.execute()\n",
        "        except redis.RedisError:\n",
        "            # Handle potential Redis errors during caching\n",
        "            pass\n",
        "\n",
        "    # Prepare closest match info for the return dictionary\n",
        "    closest_match_info = None\n",
        "    if closest_res_before_llm:\n",
        "         doc_id, fields, distance = closest_res_before_llm\n",
        "         closest_match_info = {\n",
        "             \"user_question\": fields.get('user_question'),\n",
        "             \"distance\": distance\n",
        "         }\n",
        "\n",
        "\n",
        "    # Return the LLM response details\n",
        "    return {\n",
        "        \"source\": \"llm\",\n",
        "        \"response\": content,\n",
        "        \"user_question\": user_prompt, # Include user_question for LLM responses\n",
        "        \"distance\": None, # No distance for an LLM response\n",
        "        \"latency_ms\": llm_latency_ms,\n",
        "        \"usage\": {\n",
        "            \"prompt_tokens\": getattr(usage, \"prompt_tokens\", None) if usage else None,\n",
        "            \"completion_tokens\": getattr(usage, \"completion_tokens\", None) if usage else None,\n",
        "            \"total_tokens\": getattr(usage, \"total_tokens\", None) if usage else None,\n",
        "        },\n",
        "        \"closest_match_before_llm\": closest_match_info # Include closest match info before LLM call\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEMFjc1Nj0v8"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 6: Add Metrics Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "xJCdv9l0j0v8"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        # Initialize counters for cache hits and misses\n",
        "        self.hits = 0\n",
        "        self.misses = 0\n",
        "        # Lists to store latencies for cache hits and LLM calls\n",
        "        self.cache_latencies = []\n",
        "        self.llm_latencies = []\n",
        "\n",
        "    def record(self, result):\n",
        "        # Record metrics based on the source of the response (cache or LLM)\n",
        "        if result[\"source\"] == \"cache\":\n",
        "            self.hits += 1\n",
        "            self.cache_latencies.append(result[\"latency_ms\"])\n",
        "        else:\n",
        "            self.misses += 1\n",
        "            self.llm_latencies.append(result[\"latency_ms\"])\n",
        "\n",
        "    def snapshot(self):\n",
        "        # Calculate and return a snapshot of the current metrics\n",
        "        def safe_percentile(vals, p):\n",
        "            # Helper function to calculate percentiles safely\n",
        "            if not vals:\n",
        "                return None\n",
        "            sorted_vals = sorted(vals)\n",
        "            idx = int(len(sorted_vals) * p / 100) - 1\n",
        "            return sorted_vals[max(0, idx)]\n",
        "\n",
        "        return {\n",
        "            # Calculate the cache hit rate\n",
        "            \"hit_rate\": self.hits / max(self.hits + self.misses, 1),\n",
        "            # Calculate the median and 95th percentile latency for cache hits\n",
        "            \"p50_cache_ms\": statistics.median(self.cache_latencies) if self.cache_latencies else None,\n",
        "            \"p95_cache_ms\": safe_percentile(self.cache_latencies, 95),\n",
        "            # Calculate the median and 95th percentile latency for LLM calls\n",
        "            \"p50_llm_ms\": statistics.median(self.llm_latencies) if self.llm_latencies else None,\n",
        "            \"p95_llm_ms\": safe_percentile(self.llm_latencies, 95),\n",
        "        }\n",
        "\n",
        "metrics = Metrics()\n",
        "\n",
        "# Modify the answer function to accept add_to_cache and pass it down\n",
        "def answer(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH, add_to_cache: bool = True):\n",
        "    # Main function to get an answer, using the cache or calling the LLM\n",
        "    # Pass the add_to_cache parameter to cache_get_or_generate\n",
        "    res = cache_get_or_generate(system_prompt, user_prompt, ef_runtime=ef_runtime, threshold=threshold, add_to_cache=add_to_cache)\n",
        "    # Record the result in the metrics tracker\n",
        "    metrics.record(res)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llYkrzInj0v9"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Run and Validate\n",
        "\n",
        "### Warm the Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbMRxWUrj0v9",
        "outputId": "672df682-6cc7-4621-e0b1-1124f1fb31f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warming cache...\n",
            "llm 1061.9ms\n",
            "llm 692.4ms\n",
            "llm 1076.4ms\n"
          ]
        }
      ],
      "source": [
        "SYSTEM_PROMPT = \"You are a concise support assistant for ACME Corp. Use internal policy v1 for refunds and returns.\"\n",
        "seed_prompts = [\n",
        "    \"What is our refund policy?\",\n",
        "    \"How long is the return window?\",\n",
        "    \"Do you offer exchanges?\",\n",
        "]\n",
        "\n",
        "print(\"Warming cache...\")\n",
        "for p in seed_prompts:\n",
        "    res = answer(SYSTEM_PROMPT, p)\n",
        "    print(f\"{res['source']} {res['latency_ms']:.1f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KRrSFDDj0v9"
      },
      "source": [
        "### Test Paraphrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rTSaG0Nj0v9",
        "outputId": "5d90e7ac-331f-4b01-fb5e-a7d64064bf3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing paraphrases...\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Can I get a refund? What's the policy?\n",
            "Canonicalized: can i get a refund? what's the policy?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1128.1ms\n",
            "  Token Usage: Prompt=42, Completion=50, Total=92\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What is our refund policy?'\n",
            "    Distance: 0.20\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: What's the time limit to return an item?\n",
            "Canonicalized: what's the time limit to return an item?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 741.0ms\n",
            "  Token Usage: Prompt=41, Completion=17, Total=58\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.32\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Is it possible to swap a product for another?\n",
            "Canonicalized: is it possible to swap a product for another?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1201.5ms\n",
            "  Token Usage: Prompt=42, Completion=53, Total=95\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'Do you offer exchanges?'\n",
            "    Distance: 0.48\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: For how many days can I return stuff?\n",
            "Canonicalized: for how many days can i return stuff?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 598.6ms\n",
            "  Token Usage: Prompt=41, Completion=26, Total=67\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.36\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Where is the closest store?\n",
            "Canonicalized: where is the closest store?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 2028.0ms\n",
            "  Token Usage: Prompt=38, Completion=26, Total=64\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.77\n",
            "    Current THRESHOLD: 0.1000\n"
          ]
        }
      ],
      "source": [
        "paraphrases = [\n",
        "    \"Can I get a refund? What's the policy?\",\n",
        "    \"What's the time limit to return an item?\",\n",
        "    \"Is it possible to swap a product for another?\",\n",
        "    \"For how many days can I return stuff?\",\n",
        "    \"Where is the closest store?\" # Example of a query that should not hit the cache\n",
        "]\n",
        "\n",
        "print(\"\\nTesting paraphrases...\")\n",
        "for p in paraphrases:\n",
        "    print(f\"\\n--- Testing Paraphrase ---\")\n",
        "    print(f\"Original: {p}\")\n",
        "    canonical_p = canonicalize(p)\n",
        "    print(f\"Canonicalized: {canonical_p}\")\n",
        "\n",
        "    res = answer(SYSTEM_PROMPT, p, add_to_cache=False)\n",
        "\n",
        "    if res['source'] == 'cache':\n",
        "        print(f\"Result: CACHE HIT\")\n",
        "        print(f\"  Cached Question: {res.get('user_question')}\")\n",
        "        print(f\"  Distance: {res.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "    else: # res['source'] == 'llm'\n",
        "        print(f\"Result: CACHE MISS (LLM Call)\")\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "        if res.get('usage'):\n",
        "             print(f\"  Token Usage: Prompt={res['usage'].get('prompt_tokens')}, Completion={res['usage'].get('completion_tokens')}, Total={res['usage'].get('total_tokens')}\")\n",
        "\n",
        "        # Display closest match information found *before* the LLM call\n",
        "        closest_info = res.get('closest_match_before_llm')\n",
        "        if closest_info:\n",
        "            print(f\"  Closest match in cache (before LLM call):\")\n",
        "            print(f\"    Original Cached Q: '{closest_info.get('user_question')}'\")\n",
        "            print(f\"    Distance: {closest_info.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "            print(f\"    Current THRESHOLD: {THRESH:.4f}\")\n",
        "        else:\n",
        "            print(f\"  No close match found in cache (even with high threshold) before LLM call.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qv5LJEaj0v9"
      },
      "source": [
        "### Print Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tog6pXSXj0v-",
        "outputId": "b6130459-8c1c-4a3e-cfa2-72026d9ab1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics: {'hit_rate': 0.0, 'p50_cache_ms': None, 'p95_cache_ms': None, 'p50_llm_ms': 668.486534999829, 'p95_llm_ms': 2007.052681999994}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nMetrics:\", metrics.snapshot())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJZeOZFjj0v-"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "<ul>\n",
        "<li>First run: all `llm` sources, ~500‚Äì1000ms latency\n",
        "</li>\n",
        "<li>Paraphrases: mostly `cache` sources, <50ms latency, distance <0.10\n",
        "</li>\n",
        "<li>Hit rate: 60‚Äì80% for paraphrases\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Tuning the Similarity Threshold\n",
        "\n",
        "The threshold controls cache precision. Lower = stricter (fewer false hits), higher = more lenient (more hits, risk of incorrect matches)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbmSSSBbj0v-",
        "outputId": "f3c62082-1efa-47e2-f44b-98915e7040f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Threshold=0.06\n",
            "Can I get a refund? What's the policy? => llm dist=N/A\n",
            "What's the time limit to return an item? => llm dist=N/A\n",
            "Is it possible to swap a product for another? => llm dist=N/A\n",
            "For how many days can I return stuff? => llm dist=N/A\n",
            "Where is the closest store? => llm dist=N/A\n",
            "\n",
            "Threshold=0.08\n",
            "Can I get a refund? What's the policy? => llm dist=N/A\n",
            "What's the time limit to return an item? => llm dist=N/A\n",
            "Is it possible to swap a product for another? => llm dist=N/A\n",
            "For how many days can I return stuff? => llm dist=N/A\n",
            "Where is the closest store? => llm dist=N/A\n",
            "\n",
            "Threshold=0.1\n",
            "Can I get a refund? What's the policy? => llm dist=N/A\n",
            "What's the time limit to return an item? => llm dist=N/A\n",
            "Is it possible to swap a product for another? => llm dist=N/A\n",
            "For how many days can I return stuff? => llm dist=N/A\n",
            "Where is the closest store? => llm dist=N/A\n",
            "\n",
            "Threshold=0.12\n",
            "Can I get a refund? What's the policy? => llm dist=N/A\n",
            "What's the time limit to return an item? => llm dist=N/A\n",
            "Is it possible to swap a product for another? => llm dist=N/A\n",
            "For how many days can I return stuff? => llm dist=N/A\n",
            "Where is the closest store? => llm dist=N/A\n",
            "\n",
            "Threshold=0.14\n",
            "Can I get a refund? What's the policy? => llm dist=N/A\n",
            "What's the time limit to return an item? => llm dist=N/A\n",
            "Is it possible to swap a product for another? => llm dist=N/A\n",
            "For how many days can I return stuff? => llm dist=N/A\n",
            "Where is the closest store? => llm dist=N/A\n"
          ]
        }
      ],
      "source": [
        "def sweep_thresholds(thresholds):\n",
        "    for t in thresholds:\n",
        "        print(f\"\\nThreshold={t}\")\n",
        "        for p in paraphrases:\n",
        "            res = cache_get_or_generate(SYSTEM_PROMPT, p, ef_runtime=150, threshold=t, add_to_cache=False)\n",
        "            distance_str = f\"{res.get('distance'):.2f}\" if res.get('distance') is not None else 'N/A'\n",
        "            cached_question_str = f\" (Cached: {res.get('user_question')})\" if res['source'] == 'cache' else ''\n",
        "            print(f\"{p} => {res['source']} dist={distance_str}{cached_question_str}\")\n",
        "\n",
        "# Assuming 'paraphrases' list is defined earlier in the notebook\n",
        "sweep_thresholds([0.06, 0.08, 0.10, 0.12, 0.14])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8x6YDIxj0v-"
      },
      "source": [
        "Start with 0.10 and adjust based on false positive rate.\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Inspect the Cache\n",
        "\n",
        "**Count indexed documents:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDJ3Iy_Jj0v_",
        "outputId": "b0c954fc-b8b1-41f1-ddaf-f84fe881dc69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cached documents: 3\n"
          ]
        }
      ],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n",
        "num_docs = info[info.index(b'num_docs') + 1]\n",
        "print(f\"Cached documents: {num_docs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWjbEHpkj0v_"
      },
      "source": [
        "**Inspect a document:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zebeV7cDj0v_",
        "outputId": "e19bc1aa-0c59-4af7-b4ed-80cc6350ac24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 documents:\n",
            "\n",
            "--- Document 1 (Key: sc:v1:fd1a249118ff83ede5d8f4d14d19dc6720c498176a7f9a0a08636f9841a0f0fa) ---\n",
            "  created_at: 1760988143.0178475\n",
            "  user_question: Do you offer exchanges?\n",
            "  corpus_version: v1\n",
            "  response: ACME Corp. does not offer exchanges. If you need a different item, please return the original item for a refund and place a new order.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: fe8a40968fc5fd2fb51084f84b99daac2e538f86fff748eee50a458729742905\n",
            "  last_hit_at: 1760988143.0178475\n",
            "\n",
            "--- Document 2 (Key: sc:v1:493173b9d91727a583115a82e58560182ee3f8ba4aa15b878f637e774b5df763) ---\n",
            "  created_at: 1760988141.7435763\n",
            "  user_question: How long is the return window?\n",
            "  corpus_version: v1\n",
            "  response: The return window is 30 days from the date of purchase.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 159d9f1ce048fad930a43a6b337a5476ac857e573dceed1cf53c36ed8140c193\n",
            "  last_hit_at: 1760988141.7435765\n",
            "\n",
            "--- Document 3 (Key: sc:v1:553a2bbe49d193f0cebe0db682bc3e677cb08dac67dd4c8d629c615493ddfd5a) ---\n",
            "  created_at: 1760988141.0061889\n",
            "  user_question: What is our refund policy?\n",
            "  corpus_version: v1\n",
            "  response: ACME Corp's refund policy allows customers to request a refund within 30 days of purchase. Items must be in original condition and packaging. To initiate a refund, customers should contact our support team with their order details. Refunds will be processed to the original payment method within 5-7 business days after approval. Certain items may be non-refundable, such as personalized or digital products.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 99714355d2b61be5bfb3cb69b5212932f6880d7eed984c5d61c4128db77bbfb3\n",
            "  last_hit_at: 1760988141.006189\n"
          ]
        }
      ],
      "source": [
        "def print_cached_documents(max_docs: int = None):\n",
        "    keys = r.keys(f\"{NS}*\")\n",
        "    if keys:\n",
        "        print(f\"Found {len(keys)} documents:\")\n",
        "        # Limit the keys to iterate if max_docs is specified\n",
        "        keys_to_print = keys[:max_docs] if max_docs is not None else keys\n",
        "\n",
        "        for i, key in enumerate(keys_to_print):\n",
        "            print(f\"\\n--- Document {i+1} (Key: {key.decode()}) ---\")\n",
        "            doc = r.hgetall(key)\n",
        "            # Decode bytes to string for all fields except 'vector' and print them\n",
        "            for k, v in doc.items():\n",
        "                decoded_key = k.decode()\n",
        "                if decoded_key == \"vector\":\n",
        "                    # Skip printing the vector field\n",
        "                    continue\n",
        "                else:\n",
        "                    # Decode other fields and print with a label\n",
        "                    decoded_value = v.decode() if isinstance(v, bytes) else v\n",
        "                    print(f\"  {decoded_key}: {decoded_value}\")\n",
        "        if max_docs is not None and len(keys) > max_docs:\n",
        "            print(f\"\\n... and {len(keys) - max_docs} more documents (showing first {max_docs})\")\n",
        "    else:\n",
        "        print(\"No documents found in the index.\")\n",
        "\n",
        "# Call the function to print documents (prints all by default)\n",
        "print_cached_documents()\n",
        "\n",
        "# Example of printing only the first 3 documents:\n",
        "# print_cached_documents(max_docs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1n83Loqj0wE"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "You've built a production-grade semantic cache with Redis Vector. The system normalizes queries, generates embeddings, performs fast vector search, and returns cached responses when similarity is high. This approach cuts latency by 10‚Äì20x and reduces LLM costs by 60‚Äì80% for repeated queries.\n",
        "\n",
        "**Key design decisions:**\n",
        "\n",
        "<ul>\n",
        "<li>**Canonicalization** stabilizes cache keys across paraphrases\n",
        "</li>\n",
        "<li>**HNSW indexing** enables sub-50ms vector search at scale\n",
        "</li>\n",
        "<li>**Metadata gating** ensures cache hits respect model, temperature, and system prompt changes\n",
        "</li>\n",
        "<li>**TTL and namespace versioning** provide safe invalidation paths\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "**Next steps:**\n",
        "\n",
        "<ul>\n",
        "<li>Add query-side metadata filters in `FT.SEARCH` to reduce false candidates (e.g., `@model:{gpt-4o-mini} @sys_hash:{<hash>}`)\n",
        "</li>\n",
        "<li>Integrate Prometheus and Grafana for observability (track hit rate, p95 latency, cache size)\n",
        "</li>\n",
        "<li>Implement LRU eviction or score-based pruning for long-running caches\n",
        "</li>\n",
        "<li>Explore quantization (FLOAT16) to reduce memory footprint\n",
        "</li>\n",
        "<li>Scale with Redis Cluster for multi-tenant or high-throughput workloads\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "For more on building intelligent systems, see our guides on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/build-rag-pipeline\">building a RAG pipeline</a> and <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/optimize-llm-context\">optimizing LLM context windows</a>."
      ]
    }
  ],
  "metadata": {
    "title": "Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs",
    "description": "Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}