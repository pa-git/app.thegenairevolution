{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQhv5b9Vj0vw"
      },
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n",
        "\n",
        "**Title:** Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs\n",
        "\n",
        "**Description:** Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.\n",
        "\n",
        "---\n",
        "\n",
        "*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFnUUE6Bj0vx"
      },
      "source": [
        "Most LLM applications waste money and time answering the same question phrased slightly differently. A semantic cache solves this by recognizing when a new query is semantically similar to a previous one and returning the cached response instantly, eliminating the need for an LLM call.\n",
        "\n",
        "This guide walks you through building a production-grade semantic cache using embeddings and Redis Vector. You'll create a Redis-backed semantic cache, complete with thresholds, TTLs, and metrics. By the end, you'll have working code, a tunable architecture, and a clear path to immediate latency and cost reductions.\n",
        "\n",
        "**What you'll build:**\n",
        "\n",
        "<ul>\n",
        "<li>A Redis HNSW vector index for semantic similarity search\n",
        "</li>\n",
        "<li>A cache layer that normalizes queries, generates embeddings, and retrieves cached responses\n",
        "</li>\n",
        "<li>A demo script to validate cache hit rates and latency improvements\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "<ul>\n",
        "<li>Python 3.9+\n",
        "</li>\n",
        "<li>Redis Stack (local via Docker or managed Redis Cloud)\n",
        "</li>\n",
        "<li>OpenAI API key\n",
        "</li>\n",
        "<li>Basic familiarity with embeddings and vector search\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "If you're using Google Colab or a cloud notebook, connect to a managed Redis Stack instance (e.g., Redis Cloud) instead of running Docker locally.\n",
        "\n",
        "For a deeper understanding of how LLMs manage memory and the concept of context rot, see our article on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/context-rot-why-llms-forget-as-their-memory-grows\">why LLMs \"forget\" as their memory grows</a>.\n",
        "\n",
        "<hr>\n",
        "\n",
        "## How It Works (High-Level Overview)\n",
        "\n",
        "**The paraphrase problem:** Users ask the same question in many ways. \"What's your refund policy?\" and \"Can I get my money back?\" are semantically identical, but traditional caching treats them as different keys.\n",
        "\n",
        "**The embedding advantage:** Embeddings map text into a high-dimensional vector space where semantically similar phrases cluster together. By comparing query embeddings using cosine similarity, you can detect paraphrases and return cached responses.\n",
        "\n",
        "**Why Redis Vector:** Redis Stack provides HNSW (Hierarchical Navigable Small World) indexing for fast approximate nearest neighbor search. It combines low-latency vector search with Redis's native TTL, tagging, and filtering capabilities, making it ideal for production caching.\n",
        "\n",
        "**Architecture:**\n",
        "\n",
        "<ol>\n",
        "<li>Normalize the user query (lowercase, strip volatile patterns like timestamps)\n",
        "</li>\n",
        "<li>Generate an embedding for the normalized query\n",
        "</li>\n",
        "<li>Search the Redis HNSW index for the nearest cached embedding\n",
        "</li>\n",
        "<li>If distance < threshold and metadata matches (model, temperature, system prompt hash), return the cached response\n",
        "</li>\n",
        "<li>Otherwise, call the LLM, cache the new response with its embedding, and return it\n",
        "</li>\n",
        "</ol>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Setup & Installation\n",
        "\n",
        "### Option 1: Managed Redis (Recommended for Notebooks)\n",
        "\n",
        "Sign up for a free Redis Cloud account at <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://redis.com/try-free\">redis.com/try-free</a> and create a Redis Stack database. Copy the connection URL.\n",
        "\n",
        "In your notebook or terminal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "3L-n9TWyj0vz",
        "outputId": "c344b053-bcb4-4fb6-fdb4-2a31719a2dd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: redis in /usr/local/lib/python3.12/dist-packages (6.4.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjHOIKHSj0v0"
      },
      "source": [
        "Set environment variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "6NP21I1bj0v0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"REDIS_URL\"] = \"redis://default:password@your-redis-host:port\"\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "os.environ[\"EMBEDDING_MODEL\"] = \"text-embedding-3-small\"\n",
        "os.environ[\"CHAT_MODEL\"] = \"gpt-4o-mini\"\n",
        "os.environ[\"SIMILARITY_THRESHOLD\"] = \"0.10\"\n",
        "os.environ[\"TOP_K\"] = \"5\"\n",
        "os.environ[\"CACHE_TTL_SECONDS\"] = \"86400\"\n",
        "os.environ[\"CACHE_NAMESPACE\"] = \"sc:v1:\"\n",
        "os.environ[\"CORPUS_VERSION\"] = \"v1\"\n",
        "os.environ[\"TEMPERATURE\"] = \"0.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QktgskEcj0v1"
      },
      "source": [
        "### Option 2: Local Redis with Docker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbHhqS92j0v1"
      },
      "outputs": [],
      "source": [
        "docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J56cAYaUj0v2"
      },
      "source": [
        "Create a `.env` file:\n",
        "\n",
        "<pre><code>REDIS_URL=redis://localhost:6379\n",
        "OPENAI_API_KEY=sk-...\n",
        "EMBEDDING_MODEL=text-embedding-3-small\n",
        "CHAT_MODEL=gpt-4o-mini\n",
        "SIMILARITY_THRESHOLD=0.10\n",
        "TOP_K=5\n",
        "CACHE_TTL_SECONDS=86400\n",
        "CACHE_NAMESPACE=sc:v1:\n",
        "CORPUS_VERSION=v1\n",
        "TEMPERATURE=0.2\n",
        "</code></pre>\n",
        "\n",
        "Install dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc7pGvZlj0v2"
      },
      "outputs": [],
      "source": [
        "pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3zXkp7yj0v3"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Step-by-Step Implementation\n",
        "\n",
        "### Step 1: Create the Redis HNSW Index\n",
        "\n",
        "The index stores embeddings and metadata for cached responses. We use HNSW for fast approximate nearest neighbor search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46FV0WrQj0v3",
        "outputId": "3b3b8a1f-a176-442d-b2d7-5f693a9350f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using index name: sc_index\n",
            "Dropped existing index 'sc_index' including documents.\n",
            "Index 'sc_index' created.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import redis\n",
        "import time\n",
        "\n",
        "r = redis.Redis.from_url(os.getenv(\"REDIS_URL\"))\n",
        "\n",
        "INDEX = \"sc_index\" # Make sure to update this variable if you want a different index name\n",
        "PREFIX = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "DIM = 1536  # Dimension for text-embedding-3-small\n",
        "M = 16  # HNSW graph connectivity\n",
        "EF_CONSTRUCTION = 200  # HNSW construction quality\n",
        "\n",
        "def create_index():\n",
        "    print(f\"Using index name: {INDEX}\") # Print the index name being used\n",
        "\n",
        "    # Drop index if it exists, and delete associated documents (DD)\n",
        "    try:\n",
        "        r.execute_command(\"FT.DROPINDEX\", INDEX, \"DD\")\n",
        "        print(f\"Dropped existing index '{INDEX}' including documents.\")\n",
        "    except redis.ResponseError:\n",
        "        print(f\"Index '{INDEX}' did not exist, proceeding with creation.\")\n",
        "        pass # Index does not exist, safe to ignore\n",
        "\n",
        "    # Create index with vector field and metadata tags\n",
        "    cmd = [\n",
        "        \"FT.CREATE\", INDEX,  # Command to create a full-text search index with the given name\n",
        "        \"ON\", \"HASH\",  # Index applies to Redis Hash data structures\n",
        "        \"PREFIX\", \"1\", PREFIX,  # Only index keys starting with the defined prefix\n",
        "        \"SCHEMA\",  # Define the schema of the index\n",
        "        \"prompt_hash\", \"TAG\",  # Tag field for hashing the canonicalized prompt\n",
        "        \"model\", \"TAG\",  # Tag field for the LLM model used\n",
        "        \"sys_hash\", \"TAG\",  # Tag field for hashing the system prompt\n",
        "        \"corpus_version\", \"TAG\",  # Tag field for tracking the version of the underlying corpus\n",
        "        \"temperature\", \"NUMERIC\",  # Numeric field for the temperature parameter used by the LLM\n",
        "        \"created_at\", \"NUMERIC\",  # Numeric field for the creation timestamp\n",
        "        \"last_hit_at\", \"NUMERIC\",  # Numeric field for the timestamp of the last cache hit\n",
        "        \"response\", \"TEXT\",  # Text field for the LLM's response\n",
        "        \"user_question\", \"TEXT\", # Text field for the original user question\n",
        "        \"vector\", \"VECTOR\", \"HNSW\", \"10\",  # Define a vector field named \"vector\" using the HNSW algorithm. \"10\" specifies the number of pairs for the HNSW vector definition.\n",
        "        \"TYPE\", \"FLOAT32\",  # Specify the data type of the vector embeddings\n",
        "        \"DIM\", str(DIM),  # Specify the dimension of the vector embeddings\n",
        "        \"DISTANCE_METRIC\", \"COSINE\",  # Specify the distance metric to use for vector similarity search\n",
        "        \"M\", str(M),  # HNSW parameter: number of established connections for each element during graph construction\n",
        "        \"EF_CONSTRUCTION\", str(EF_CONSTRUCTION),  # HNSW parameter: size of the dynamic list for heuristic search during graph construction\n",
        "    ]\n",
        "    r.execute_command(*cmd)\n",
        "    print(f\"Index '{INDEX}' created.\")\n",
        "\n",
        "create_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnf6RGpNj0v4"
      },
      "source": [
        "**Validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCqqWYtRj0v4",
        "outputId": "a72f3a4e-e8bf-4542-c72c-a4594bf35e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index Info:\n",
            "  index_name: sc_index\n",
            "  num_docs: 0\n"
          ]
        }
      ],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n",
        "\n",
        "# Helper function to decode bytes to string\n",
        "def decode_bytes(item):\n",
        "    if isinstance(item, bytes):\n",
        "        return item.decode()\n",
        "    return item\n",
        "\n",
        "# Parse the info output for better readability\n",
        "parsed_info = {}\n",
        "for i in range(0, len(info), 2):\n",
        "    key = decode_bytes(info[i])\n",
        "    value = info[i+1]\n",
        "    if isinstance(value, list):\n",
        "        # Decode lists of bytes\n",
        "        parsed_info[key] = [decode_bytes(item) for item in value]\n",
        "    else:\n",
        "        parsed_info[key] = decode_bytes(value)\n",
        "\n",
        "print(\"Index Info:\")\n",
        "print(f\"  index_name: {parsed_info.get('index_name')}\")\n",
        "print(f\"  num_docs: {parsed_info.get('num_docs')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkYR0jDUj0v4"
      },
      "source": [
        "You should see `num_docs: 0` initially.\n",
        "\n",
        "<hr>\n",
        "\n",
        "### Step 2: Normalize Queries for Stable Cache Keys\n",
        "\n",
        "Canonicalization removes volatile elements (timestamps, UUIDs, IDs) and normalizes whitespace to ensure paraphrases map to the same cache key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Reod8eyHj0v5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import hashlib\n",
        "\n",
        "# Note: Normalization adequacy depends on expected query variations and embedding model robustness.\n",
        "VOLATILE_PATTERNS = [\n",
        "    # ISO timestamps and variations\n",
        "    r\"\\b\\d{4}-\\d{2}-\\d{2}(T|\\s)\\d{2}:\\d{2}(:\\d{2})?(Z|[+-]\\d{2}:\\d{2})?\\b\",\n",
        "    # Common date formats (MM/DD/YYYY, DD/MM/YYYY, YYYY/MM/DD, YYYY-MM-DD)\n",
        "    r\"\\b\\d{1,4}[-/.]?\\d{1,2}[-/.]?\\d{2,4}\\b\", # Updated to be more flexible with separators and year length\n",
        "    # UUID v4\n",
        "    r\"\\b[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\\b\",\n",
        "    # Long IDs (6+ digits)\n",
        "    r\"\\b\\d{6,}\\b\",\n",
        "    # Email addresses (often contain volatile parts or personally identifiable info)\n",
        "    r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\",\n",
        "]\n",
        "\n",
        "def canonicalize(text: str) -> str:\n",
        "    # Removes volatile patterns (like dates, IDs) and standardizes whitespace\n",
        "    # to create a consistent representation of the query for caching.\n",
        "    t = text.strip().lower()\n",
        "    for pat in VOLATILE_PATTERNS:\n",
        "        t = re.sub(pat, \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def sha256(s: str) -> str:\n",
        "    # Generates a SHA256 hash of a string. Used for creating stable identifiers\n",
        "    # for prompts and system prompts.\n",
        "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def scope_hash(prompt_norm: str, model: str, sys_hash: str, temperature: float, corpus_version: str) -> str:\n",
        "    # Creates a unique hash that defines the scope of a cache entry.\n",
        "    # This ensures that a cache hit is only valid if all relevant parameters\n",
        "    # (normalized prompt, model, system prompt hash, temperature, corpus version) match.\n",
        "    payload = f\"{prompt_norm}|{model}|{sys_hash}|{temperature}|{corpus_version}\"\n",
        "    return sha256(payload)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fycERspj0v5"
      },
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um2EQfnej0v5",
        "outputId": "bafc1f48-1513-4057-a537-7c5dfdd0ceaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is our refund policy on ?\n",
            "what is our refund policy on ?\n"
          ]
        }
      ],
      "source": [
        "q1 = \"What is our refund policy on 2025-01-15?\"\n",
        "q2 = \"what is our refund policy on 2025-01-20?\"\n",
        "print(canonicalize(q1))\n",
        "print(canonicalize(q2))\n",
        "# Both should output: \"what is our refund policy on\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilAUmXFkj0v6"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 3: Initialize Clients and Embedding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Kk9WtNFoj0v6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "EMBED_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
        "CHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")\n",
        "THRESH = float(os.getenv(\"SIMILARITY_THRESHOLD\", 0.10))\n",
        "TOP_K = int(os.getenv(\"TOP_K\", 5))\n",
        "TTL = int(os.getenv(\"CACHE_TTL_SECONDS\", 86400))\n",
        "NS = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "CORPUS_VERSION = os.getenv(\"CORPUS_VERSION\", \"v1\")\n",
        "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.2))\n",
        "\n",
        "def embed(text: str) -> np.ndarray:\n",
        "    # Generates a vector embedding for the input text using the specified embedding model.\n",
        "    # The vector is then L2 normalized, which is standard practice for cosine\n",
        "    # similarity search (But optional as it's already handled by Redis)\n",
        "    e = client.embeddings.create(model=EMBED_MODEL, input=text)\n",
        "    vec = np.array(e.data[0].embedding, dtype=np.float32)\n",
        "    norm = np.linalg.norm(vec)\n",
        "    return vec / max(norm, 1e-12) # L2 normalization\n",
        "\n",
        "def to_bytes(vec: np.ndarray) -> bytes:\n",
        "    # Converts a NumPy array (the vector embedding) into bytes.\n",
        "    # This is necessary for storing the vector data in Redis, as Redis\n",
        "    # stores data as bytes.\n",
        "    return vec.astype(np.float32).tobytes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywGrUE6Lj0v6"
      },
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCa7lNNcj0v7",
        "outputId": "a8abd0e7-b020-4c02-be83-ac9a146d1a1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: (1536,), norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "test_vec = embed(\"hello world\")\n",
        "print(f\"Embedding shape: {test_vec.shape}, norm: {np.linalg.norm(test_vec):.4f}\")\n",
        "# Should output shape (1536,) and norm ~1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QzDCsOXj0v7"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 4: Implement Vector Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "A_DoW81jj0v7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "def vector_search(query_vec, ef_runtime: int = 100, threshold: float = THRESH) -> Optional[Tuple[str, Dict[str, Any], float]]:\n",
        "    # Performs a vector similarity search in the Redis HNSW index.\n",
        "    # It searches for the nearest neighbor(s) to the query vector and\n",
        "    # returns the document(s) that are within the specified distance threshold.\n",
        "    # Perform KNN search with EF_RUNTIME parameter\n",
        "    # Define the parameters for the search query\n",
        "    params = [\"vec\", to_bytes(query_vec), \"ef_runtime\", ef_runtime]\n",
        "    # Define the search query using RediSearch's query syntax\n",
        "    # * => search all documents\n",
        "    # [KNN {TOP_K} @vector $vec => search for KNN of the vector parameter named \"vec\"\n",
        "    # AS score => return the score (distance) as \"score\"\n",
        "    # EF_RUNTIME $ef_runtime => specify the ef_runtime parameter for HNSW search\n",
        "    q = f\"*=>[KNN {TOP_K} @vector $vec AS score]\"\n",
        "    try:\n",
        "        # Execute the RediSearch query\n",
        "        res = r.execute_command(\n",
        "            \"FT.SEARCH\", INDEX, # Index name\n",
        "            q, \"PARAMS\", str(len(params)), *params, # Query and parameters\n",
        "            \"SORTBY\", \"score\", \"ASC\", # Sort results by score in ascending order (smaller distance is better)\n",
        "            \"RETURN\", \"8\", \"response\", \"model\", \"sys_hash\", \"corpus_version\", \"temperature\", \"prompt_hash\", \"score\", \"user_question\", # Return these fields, added \"user_question\"\n",
        "            \"DIALECT\", \"2\" # Use dialect 2 for parameters\n",
        "        )\n",
        "    except redis.RedisError as e:\n",
        "        # Handle Redis errors during search\n",
        "        print(f\"Redis search error: {e}\") # Modified to print the exception\n",
        "        return None\n",
        "\n",
        "    # Process the search results\n",
        "    total = res[0] if res else 0 # Total number of results (should be 1 if a match is found)\n",
        "    if total < 1:\n",
        "        # No results found\n",
        "        return None\n",
        "\n",
        "    # Extract document id and fields from the result\n",
        "    doc_id = res[1]\n",
        "    fields = res[2]\n",
        "    # Convert field names and values from bytes to strings\n",
        "    f = {fields[i].decode() if isinstance(fields[i], bytes) else fields[i]:\n",
        "         fields[i+1].decode() if isinstance(fields[i+1], bytes) else fields[i+1]\n",
        "         for i in range(0, len(fields), 2)}\n",
        "\n",
        "    try:\n",
        "        # Extract the score (distance)\n",
        "        distance = float(f[\"score\"])\n",
        "    except Exception:\n",
        "        # Handle error in extracting score\n",
        "        print(\"Error extracting score\") # Added error print for debugging\n",
        "        distance = 1.0\n",
        "\n",
        "    # Return the document id, fields, and distance\n",
        "    return doc_id.decode() if isinstance(doc_id, bytes) else doc_id, f, distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhzCtHhyj0v7"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 5: Build the Cache Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "DgRuimkRj0v8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "def sys_hash(system_prompt: str) -> str:\n",
        "    # Generates a SHA256 hash of the system prompt\n",
        "    return sha256(system_prompt.strip())\n",
        "\n",
        "def key(doc_id_hash: str) -> str:\n",
        "    # Creates a Redis key with a namespace prefix\n",
        "    return f\"{NS}{doc_id_hash}\"\n",
        "\n",
        "def metadata_matches(f: Dict[str, Any], model: str, sys_h: str, temp: float, corpus: str) -> bool:\n",
        "    # Checks if the metadata from a cached document matches the current query parameters\n",
        "    try:\n",
        "        if f.get(\"model\") != model: return False\n",
        "        if f.get(\"sys_hash\") != sys_h: return False\n",
        "        # Compare temperatures with a tolerance for floating point precision\n",
        "        if abs(float(f.get(\"temperature\", temp)) - temp) > 1e-6: return False\n",
        "        if f.get(\"corpus_version\") != corpus: return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        # Return False if there's an error during metadata comparison\n",
        "        return False\n",
        "\n",
        "def chat_call(system_prompt: str, user_prompt: str):\n",
        "    # Calls the OpenAI chat completion API\n",
        "    t0 = time.perf_counter()\n",
        "    resp = client.chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        temperature=TEMPERATURE,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "    )\n",
        "    latency_ms = (time.perf_counter() - t0) * 1000\n",
        "    content = resp.choices[0].message.content\n",
        "    usage = getattr(resp, \"usage\", None)\n",
        "    return content, latency_ms, usage\n",
        "\n",
        "def cache_get_or_generate(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH, add_to_cache: bool = True):\n",
        "    # Attempts to retrieve a response from the cache; if not found, calls the LLM and caches the response (optionally)\n",
        "    t0 = time.perf_counter()\n",
        "    sp_hash = sys_hash(system_prompt)\n",
        "    prompt_norm = canonicalize(user_prompt)\n",
        "    p_hash = sha256(prompt_norm)\n",
        "\n",
        "    qvec = embed(prompt_norm)\n",
        "\n",
        "    # --- Cache Lookup ---\n",
        "    res = vector_search(qvec, ef_runtime=ef_runtime, threshold=threshold)\n",
        "\n",
        "    # Check if a cached response was found and if its metadata matches\n",
        "    if res:\n",
        "        doc_id, fields, distance = res\n",
        "        if distance < threshold and metadata_matches(fields, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION):\n",
        "            try:\n",
        "                # Update the last hit timestamp for cache freshness\n",
        "                r.hset(doc_id, mapping={\"last_hit_at\": time.time()})\n",
        "            except redis.RedisError:\n",
        "                # Handle potential Redis errors during hset\n",
        "                pass\n",
        "            # Return the cached response details\n",
        "            return {\n",
        "                \"source\": \"cache\",\n",
        "                \"response\": fields[\"response\"],\n",
        "                \"user_question\": fields[\"user_question\"], # Include user_question for cache hits\n",
        "                \"distance\": distance,\n",
        "                \"latency_ms\": (time.perf_counter() - t0) * 1000,\n",
        "                \"closest_match_before_llm\": None # No pre-LLM closest match info on a cache hit\n",
        "            }\n",
        "\n",
        "    # --- Cache Miss - Call LLM and Cache (Optionally) ---\n",
        "\n",
        "    # If no cache hit, perform a debugging search for the closest match *before* adding the new item\n",
        "    closest_res_before_llm = vector_search(qvec, ef_runtime=ef_runtime, threshold=1.0) # Use high threshold to find closest regardless of match\n",
        "\n",
        "    content, llm_latency_ms, usage = chat_call(system_prompt, user_prompt)\n",
        "\n",
        "    # Only add to cache if add_to_cache is True\n",
        "    if add_to_cache:\n",
        "        # Generate a unique key for the new cache entry\n",
        "        doc_scope = scope_hash(prompt_norm, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION)\n",
        "        redis_key = key(doc_scope)\n",
        "\n",
        "        try:\n",
        "            # Prepare data to be stored in Redis Hash\n",
        "            mapping = {\n",
        "                \"prompt_hash\": p_hash,\n",
        "                \"model\": CHAT_MODEL,\n",
        "                \"sys_hash\": sp_hash,\n",
        "                \"corpus_version\": CORPUS_VERSION,\n",
        "                \"temperature\": TEMPERATURE,\n",
        "                \"created_at\": time.time(),\n",
        "                \"last_hit_at\": time.time(),\n",
        "                \"response\": content,\n",
        "                \"user_question\": user_prompt,\n",
        "                \"vector\": to_bytes(qvec), # Store the embedding as bytes\n",
        "            }\n",
        "            # Use a pipeline for atomic HSET and EXPIRE operations\n",
        "            pipe = r.pipeline(transaction=True)\n",
        "            pipe.hset(redis_key, mapping=mapping)\n",
        "            pipe.expire(redis_key, int(TTL)) # Set the time-to-live for the cache entry\n",
        "            pipe.execute()\n",
        "        except redis.RedisError:\n",
        "            # Handle potential Redis errors during caching\n",
        "            pass\n",
        "\n",
        "    # Prepare closest match info for the return dictionary\n",
        "    closest_match_info = None\n",
        "    if closest_res_before_llm:\n",
        "         doc_id, fields, distance = closest_res_before_llm\n",
        "         closest_match_info = {\n",
        "             \"user_question\": fields.get('user_question'),\n",
        "             \"distance\": distance\n",
        "         }\n",
        "\n",
        "\n",
        "    # Return the LLM response details\n",
        "    return {\n",
        "        \"source\": \"llm\",\n",
        "        \"response\": content,\n",
        "        \"user_question\": user_prompt, # Include user_question for LLM responses\n",
        "        \"distance\": None, # No distance for an LLM response\n",
        "        \"latency_ms\": llm_latency_ms,\n",
        "        \"usage\": {\n",
        "            \"prompt_tokens\": getattr(usage, \"prompt_tokens\", None) if usage else None,\n",
        "            \"completion_tokens\": getattr(usage, \"completion_tokens\", None) if usage else None,\n",
        "            \"total_tokens\": getattr(usage, \"total_tokens\", None) if usage else None,\n",
        "        },\n",
        "        \"closest_match_before_llm\": closest_match_info # Include closest match info before LLM call\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEMFjc1Nj0v8"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 6: Add Metrics Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "xJCdv9l0j0v8"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        # Initialize counters for cache hits and misses\n",
        "        self.hits = 0\n",
        "        self.misses = 0\n",
        "        # Lists to store latencies for cache hits and LLM calls\n",
        "        self.cache_latencies = []\n",
        "        self.llm_latencies = []\n",
        "\n",
        "    def record(self, result):\n",
        "        # Record metrics based on the source of the response (cache or LLM)\n",
        "        if result[\"source\"] == \"cache\":\n",
        "            self.hits += 1\n",
        "            self.cache_latencies.append(result[\"latency_ms\"])\n",
        "        else:\n",
        "            self.misses += 1\n",
        "            self.llm_latencies.append(result[\"latency_ms\"])\n",
        "\n",
        "    def snapshot(self):\n",
        "        # Calculate and return a snapshot of the current metrics\n",
        "        def safe_percentile(vals, p):\n",
        "            # Helper function to calculate percentiles safely\n",
        "            if not vals:\n",
        "                return None\n",
        "            sorted_vals = sorted(vals)\n",
        "            idx = int(len(sorted_vals) * p / 100) - 1\n",
        "            return sorted_vals[max(0, idx)]\n",
        "\n",
        "        return {\n",
        "            # Calculate the cache hit rate\n",
        "            \"hit_rate\": self.hits / max(self.hits + self.misses, 1),\n",
        "            # Calculate the median and 95th percentile latency for cache hits\n",
        "            \"p50_cache_ms\": statistics.median(self.cache_latencies) if self.cache_latencies else None,\n",
        "            \"p95_cache_ms\": safe_percentile(self.cache_latencies, 95),\n",
        "            # Calculate the median and 95th percentile latency for LLM calls\n",
        "            \"p50_llm_ms\": statistics.median(self.llm_latencies) if self.llm_latencies else None,\n",
        "            \"p95_llm_ms\": safe_percentile(self.llm_latencies, 95),\n",
        "        }\n",
        "\n",
        "metrics = Metrics()\n",
        "\n",
        "# Modify the answer function to accept add_to_cache and pass it down\n",
        "def answer(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH, add_to_cache: bool = True):\n",
        "    # Main function to get an answer, using the cache or calling the LLM\n",
        "    # Pass the add_to_cache parameter to cache_get_or_generate\n",
        "    res = cache_get_or_generate(system_prompt, user_prompt, ef_runtime=ef_runtime, threshold=threshold, add_to_cache=add_to_cache)\n",
        "    # Record the result in the metrics tracker\n",
        "    metrics.record(res)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llYkrzInj0v9"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Run and Validate\n",
        "\n",
        "### Warm the Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbMRxWUrj0v9",
        "outputId": "4bab956b-6e99-4765-c3a9-fbb5215933e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warming cache...\n",
            "llm 1642.2ms\n",
            "llm 642.5ms\n",
            "llm 1087.8ms\n"
          ]
        }
      ],
      "source": [
        "SYSTEM_PROMPT = \"You are a concise support assistant for ACME Corp. Use internal policy v1 for refunds and returns.\"\n",
        "seed_prompts = [\n",
        "    \"What is your refund policy?\",\n",
        "    \"How long is the return window?\",\n",
        "    \"Do you offer exchanges?\",\n",
        "]\n",
        "\n",
        "print(\"Warming cache...\")\n",
        "for p in seed_prompts:\n",
        "    res = answer(SYSTEM_PROMPT, p, add_to_cache=True)\n",
        "    print(f\"{res['source']} {res['latency_ms']:.1f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KRrSFDDj0v9"
      },
      "source": [
        "### Test Paraphrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rTSaG0Nj0v9",
        "outputId": "9005d4f8-b500-43e8-c97b-6fc7c2407cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing paraphrases...\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Could you explain your refund policy?\n",
            "Canonicalized: could you explain your refund policy?\n",
            "Result: CACHE HIT\n",
            "  Cached Question: What is your refund policy?\n",
            "  Distance: 0.09\n",
            "  Latency: 257.5ms\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Can you tell me how refunds work?\n",
            "Canonicalized: can you tell me how refunds work?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 2534.9ms\n",
            "  Token Usage: Prompt=40, Completion=126, Total=166\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What is your refund policy?'\n",
            "    Distance: 0.30\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: How do I request a refund?\n",
            "Canonicalized: how do i request a refund?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 3073.3ms\n",
            "  Token Usage: Prompt=39, Completion=97, Total=136\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What is your refund policy?'\n",
            "    Distance: 0.35\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Do you offer refunds if I'm not satisfied?\n",
            "Canonicalized: do you offer refunds if i'm not satisfied?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 1589.6ms\n",
            "  Token Usage: Prompt=41, Completion=51, Total=92\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What is your refund policy?'\n",
            "    Distance: 0.33\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: How can I get my money back after a purchase?\n",
            "Canonicalized: how can i get my money back after a purchase?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 4816.5ms\n",
            "  Token Usage: Prompt=43, Completion=104, Total=147\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What is your refund policy?'\n",
            "    Distance: 0.51\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: What is the timeframe for returns?\n",
            "Canonicalized: what is the timeframe for returns?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 345.9ms\n",
            "  Token Usage: Prompt=39, Completion=18, Total=57\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.24\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: How long is the return window?\n",
            "Canonicalized: how long is the return window?\n",
            "Result: CACHE HIT\n",
            "  Cached Question: How long is the return window?\n",
            "  Distance: 0.00\n",
            "  Latency: 275.4ms\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Whatâ€™s the time limit to send something back?\n",
            "Canonicalized: whatâ€™s the time limit to send something back?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 897.6ms\n",
            "  Token Usage: Prompt=42, Completion=18, Total=60\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.38\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: When does the return period expire?\n",
            "Canonicalized: when does the return period expire?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 872.9ms\n",
            "  Token Usage: Prompt=39, Completion=13, Total=52\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.34\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: How many days do I have to return an item?\n",
            "Canonicalized: how many days do i have to return an item?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 659.6ms\n",
            "  Token Usage: Prompt=43, Completion=15, Total=58\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.33\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Do you permit exchanges instead of refunds?\n",
            "Canonicalized: do you permit exchanges instead of refunds?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 791.7ms\n",
            "  Token Usage: Prompt=40, Completion=32, Total=72\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'Do you offer exchanges?'\n",
            "    Distance: 0.22\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Can I exchange a product I bought?\n",
            "Canonicalized: can i exchange a product i bought?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 2989.3ms\n",
            "  Token Usage: Prompt=40, Completion=39, Total=79\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'Do you offer exchanges?'\n",
            "    Distance: 0.34\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Is it possible to swap an item for another?\n",
            "Canonicalized: is it possible to swap an item for another?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 4298.9ms\n",
            "  Token Usage: Prompt=42, Completion=45, Total=87\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'Do you offer exchanges?'\n",
            "    Distance: 0.45\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Do you allow exchanges for different sizes or colors?\n",
            "Canonicalized: do you allow exchanges for different sizes or colors?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 2397.0ms\n",
            "  Token Usage: Prompt=42, Completion=44, Total=86\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'Do you offer exchanges?'\n",
            "    Distance: 0.28\n",
            "    Current THRESHOLD: 0.1000\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: How do exchanges work in your store?\n",
            "Canonicalized: how do exchanges work in your store?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 2463.4ms\n",
            "  Token Usage: Prompt=40, Completion=87, Total=127\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'Do you offer exchanges?'\n",
            "    Distance: 0.36\n",
            "    Current THRESHOLD: 0.1000\n"
          ]
        }
      ],
      "source": [
        "paraphrases = [\n",
        "    # Refunds\n",
        "    \"Could you explain your refund policy?\",\n",
        "    \"Can you tell me how refunds work?\",\n",
        "    \"How do I request a refund?\",\n",
        "    \"Do you offer refunds if I'm not satisfied?\",\n",
        "    \"How can I get my money back after a purchase?\",\n",
        "\n",
        "    # Returns\n",
        "    \"What is the timeframe for returns?\",\n",
        "    \"How long is the return window?\",\n",
        "    \"Whatâ€™s the time limit to send something back?\",\n",
        "    \"When does the return period expire?\",\n",
        "    \"How many days do I have to return an item?\",\n",
        "\n",
        "    # Exchanges\n",
        "    \"Do you permit exchanges instead of refunds?\",\n",
        "    \"Can I exchange a product I bought?\",\n",
        "    \"Is it possible to swap an item for another?\",\n",
        "    \"Do you allow exchanges for different sizes or colors?\",\n",
        "    \"How do exchanges work in your store?\",\n",
        "]\n",
        "\n",
        "print(\"\\nTesting paraphrases...\")\n",
        "for p in paraphrases:\n",
        "    print(f\"\\n--- Testing Paraphrase ---\")\n",
        "    print(f\"Original: {p}\")\n",
        "    canonical_p = canonicalize(p)\n",
        "    print(f\"Canonicalized: {canonical_p}\")\n",
        "\n",
        "    # We don't want to polute the cache while testing\n",
        "    res = answer(SYSTEM_PROMPT, p, add_to_cache=False)\n",
        "\n",
        "    if res['source'] == 'cache':\n",
        "        print(f\"Result: CACHE HIT\")\n",
        "        print(f\"  Cached Question: {res.get('user_question')}\")\n",
        "        print(f\"  Distance: {res.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "    else: # res['source'] == 'llm'\n",
        "        print(f\"Result: CACHE MISS (LLM Call)\")\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "        if res.get('usage'):\n",
        "             print(f\"  Token Usage: Prompt={res['usage'].get('prompt_tokens')}, Completion={res['usage'].get('completion_tokens')}, Total={res['usage'].get('total_tokens')}\")\n",
        "\n",
        "        # Display closest match information found *before* the LLM call\n",
        "        closest_info = res.get('closest_match_before_llm')\n",
        "        if closest_info:\n",
        "            print(f\"  Closest match in cache (before LLM call):\")\n",
        "            print(f\"    Original Cached Q: '{closest_info.get('user_question')}'\")\n",
        "            print(f\"    Distance: {closest_info.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "            print(f\"    Current THRESHOLD: {THRESH:.4f}\")\n",
        "        else:\n",
        "            print(f\"  No close match found in cache (even with high threshold) before LLM call.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qv5LJEaj0v9"
      },
      "source": [
        "### Print Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tog6pXSXj0v-",
        "outputId": "0ae00bb7-c961-4c5b-9484-4c782de6b7d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics: {'hit_rate': 0.11347517730496454, 'p50_cache_ms': 252.76083299968377, 'p95_cache_ms': 349.1016560001299, 'p50_llm_ms': 1294.754489999832, 'p95_llm_ms': 4013.979269999254}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nMetrics:\", metrics.snapshot())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "f4cbc777",
        "outputId": "69449ab0-3476-4f1a-ee3e-741b4b9f1815"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get the snapshot of the metrics\n",
        "metrics_snapshot = metrics.snapshot()\n",
        "\n",
        "# Extract data for plotting\n",
        "labels = ['Cache (P50)', 'Cache (P95)', 'LLM (P50)', 'LLM (P95)']\n",
        "latency_values = [\n",
        "    metrics_snapshot.get('p50_cache_ms'),\n",
        "    metrics_snapshot.get('p95_cache_ms'),\n",
        "    metrics_snapshot.get('p50_llm_ms'),\n",
        "    metrics_snapshot.get('p95_llm_ms')\n",
        "]\n",
        "\n",
        "# Filter out None values if no cache hits or LLM calls occurred\n",
        "filtered_labels = [labels[i] for i in range(len(latency_values)) if latency_values[i] is not None]\n",
        "filtered_values = [value for value in latency_values if value is not None]\n",
        "\n",
        "if not filtered_values:\n",
        "    print(\"No latency data available to plot.\")\n",
        "else:\n",
        "    # Create the bar chart\n",
        "    x = np.arange(len(filtered_labels))\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    bars = ax.bar(x, filtered_values, color=['skyblue', 'deepskyblue', 'lightcoral', 'indianred'])\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_ylabel('Latency (ms)')\n",
        "    ax.set_title('Cache vs. LLM Latency (P50 and P95)')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(filtered_labels)\n",
        "    ax.set_ylim(0, max(filtered_values) * 1.2) # Set y-axis limit\n",
        "\n",
        "    # Add value labels on top of the bars\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval + 5, f'{yval:.1f}', ha='center', va='bottom')\n",
        "\n",
        "    # Display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Optionally, print the hit rate separately\n",
        "print(f\"\\nCache Hit Rate: {metrics_snapshot.get('hit_rate', 0.0):.2f}\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cache Hit Rate: 0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJZeOZFjj0v-"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "<ul>\n",
        "<li>First run: all `llm` sources, ~500â€“1000ms latency\n",
        "</li>\n",
        "<li>Paraphrases: mostly `cache` sources, <50ms latency, distance <0.10\n",
        "</li>\n",
        "<li>Hit rate: 60â€“80% for paraphrases\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Tuning the Similarity Threshold\n",
        "\n",
        "The threshold controls cache precision. Lower = stricter (fewer false hits), higher = more lenient (more hits, risk of incorrect matches)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbmSSSBbj0v-"
      },
      "outputs": [],
      "source": [
        "def sweep_thresholds(thresholds):\n",
        "    for t in thresholds:\n",
        "        print(f\"\\nThreshold={t}\")\n",
        "        for p in paraphrases:\n",
        "            res = cache_get_or_generate(SYSTEM_PROMPT, p, ef_runtime=150, threshold=t, add_to_cache=False)\n",
        "            distance_str = f\"{res.get('distance'):.2f}\" if res.get('distance') is not None else 'N/A'\n",
        "            cached_question_str = f\" (Cached: {res.get('user_question')})\" if res['source'] == 'cache' else ''\n",
        "            print(f\"{p} => {res['source']} dist={distance_str}{cached_question_str}\")\n",
        "\n",
        "# Assuming 'paraphrases' list is defined earlier in the notebook\n",
        "sweep_thresholds([0.06, 0.08, 0.10, 0.12, 0.14])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8x6YDIxj0v-"
      },
      "source": [
        "Start with 0.10 and adjust based on false positive rate.\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Inspect the Cache\n",
        "\n",
        "**Count indexed documents:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDJ3Iy_Jj0v_",
        "outputId": "9dee5923-57b3-40d2-b588-5a06fffab6c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cached documents: 8\n"
          ]
        }
      ],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n",
        "num_docs = info[info.index(b'num_docs') + 1]\n",
        "print(f\"Cached documents: {num_docs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWjbEHpkj0v_"
      },
      "source": [
        "**Inspect a document:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zebeV7cDj0v_",
        "outputId": "0143df77-841e-45e8-f091-9a6dd3037257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8 documents:\n",
            "\n",
            "--- Document 1 (Key: sc:v1:c8e5c9c137a16100fb8a9ef22c1fb777bb54ce0d6fb6c2d43fca7d06d18d9ba9) ---\n",
            "  created_at: 1761002535.8008976\n",
            "  user_question: What's the time limit to return an item?\n",
            "  corpus_version: v1\n",
            "  response: The time limit to return an item is 30 days from the date of purchase.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: caa2e995938c57b4448bf558b60933f95cd6d194f5069b43b157783c67b96a2e\n",
            "  last_hit_at: 1761002535.8008978\n",
            "\n",
            "--- Document 2 (Key: sc:v1:493173b9d91727a583115a82e58560182ee3f8ba4aa15b878f637e774b5df763) ---\n",
            "  created_at: 1761002332.5939157\n",
            "  user_question: How long is the return window?\n",
            "  corpus_version: v1\n",
            "  response: The return window is 30 days from the date of purchase.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 159d9f1ce048fad930a43a6b337a5476ac857e573dceed1cf53c36ed8140c193\n",
            "  last_hit_at: 1761002444.170771\n",
            "\n",
            "--- Document 3 (Key: sc:v1:336fe24d30d935b0538bc5e7ad88350e8f54bb8b7ec5e02cb135ce2ef2b05e59) ---\n",
            "  created_at: 1761002538.4214282\n",
            "  user_question: For how many days can I return stuff?\n",
            "  corpus_version: v1\n",
            "  response: You can return items within 30 days of the purchase date for a full refund, provided they are in original condition and packaging.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 90eca5306038bfdf1af3a0611a6485d6f835ba1b712cb0011c19f1bcb46a1a2e\n",
            "  last_hit_at: 1761002538.4214284\n",
            "\n",
            "--- Document 4 (Key: sc:v1:fd1a249118ff83ede5d8f4d14d19dc6720c498176a7f9a0a08636f9841a0f0fa) ---\n",
            "  created_at: 1761002333.9443336\n",
            "  user_question: Do you offer exchanges?\n",
            "  corpus_version: v1\n",
            "  response: ACME Corp does not offer exchanges. If you need a different item, please return the original product for a refund and place a new order.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: fe8a40968fc5fd2fb51084f84b99daac2e538f86fff748eee50a458729742905\n",
            "  last_hit_at: 1761002444.331585\n",
            "\n",
            "--- Document 5 (Key: sc:v1:1f86d0093a9813e762733e9f884c01c5467272b173b3e17c24ed84d75f05aa79) ---\n",
            "  created_at: 1761002534.6564517\n",
            "  user_question: What are the rules for getting a refund?\n",
            "  corpus_version: v1\n",
            "  response: To qualify for a refund at ACME Corp, the following rules apply:\n",
            "\n",
            "1. **Time Frame**: Requests must be made within 30 days of purchase.\n",
            "2. **Condition**: Items must be in original condition, unused, and in their original packaging.\n",
            "3. **Proof of Purchase**: A receipt or order confirmation is required.\n",
            "4. **Exclusions**: Certain items, such as personalized products or digital downloads, may not be eligible for refunds.\n",
            "\n",
            "Please ensure you meet these criteria when requesting a refund.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 61e100d35be163e2b24e8343afd1574ecd98f2e8b5c442e91282da0c05f85540\n",
            "  last_hit_at: 1761002534.656452\n",
            "\n",
            "--- Document 6 (Key: sc:v1:a49d9decb96d02fa87711c7ca47e29c5a2615511aa50db725dd160e008579f7b) ---\n",
            "  created_at: 1761002539.6424563\n",
            "  user_question: Where is the closest store?\n",
            "  corpus_version: v1\n",
            "  response: I don't have access to store locations. Please visit our website or use the store locator feature to find the closest ACME Corp store.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 87fdf43e323e65f8bae9f6944a7e1e22af540e90f2ae9346789087508abfa6ca\n",
            "  last_hit_at: 1761002539.6424563\n",
            "\n",
            "--- Document 7 (Key: sc:v1:71f8a352abd2593556cf47089f0618ed0e4f184e490218e7545127147c68eb99) ---\n",
            "  created_at: 1761002331.7866697\n",
            "  user_question: What is your refund policy?\n",
            "  corpus_version: v1\n",
            "  response: ACME Corp's refund policy allows customers to request a refund within 30 days of purchase. Items must be in original condition and packaging. To initiate a refund, please contact our customer service with your order details. Certain items may be non-refundable, such as personalized or clearance items. For more specific inquiries, please refer to our detailed policy or contact support.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: b92b567fdfe4b52a6f2f93624b704f18c64c3212e2ee688b7dd2f68f10a81a50\n",
            "  last_hit_at: 1761002532.739657\n",
            "\n",
            "--- Document 8 (Key: sc:v1:53be35cec9fdc4ea25bb97e0dad6453653175495796272a771f0373351893006) ---\n",
            "  created_at: 1761002537.56585\n",
            "  user_question: Is it possible to swap a product for another?\n",
            "  corpus_version: v1\n",
            "  response: No, ACME Corp does not offer product swaps. If you wish to return a product, you can initiate a return and then place a new order for the desired item. Please refer to our refund and return policy for more details.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 76f867eed9447dc297ba3d54ea5cfae0ad4f7301fe3db626f04baf39a5fb609c\n",
            "  last_hit_at: 1761002537.5658503\n"
          ]
        }
      ],
      "source": [
        "def print_cached_documents(max_docs: int = None):\n",
        "    keys = r.keys(f\"{NS}*\")\n",
        "    if keys:\n",
        "        print(f\"Found {len(keys)} documents:\")\n",
        "        # Limit the keys to iterate if max_docs is specified\n",
        "        keys_to_print = keys[:max_docs] if max_docs is not None else keys\n",
        "\n",
        "        for i, key in enumerate(keys_to_print):\n",
        "            print(f\"\\n--- Document {i+1} (Key: {key.decode()}) ---\")\n",
        "            doc = r.hgetall(key)\n",
        "            # Decode bytes to string for all fields except 'vector' and print them\n",
        "            for k, v in doc.items():\n",
        "                decoded_key = k.decode()\n",
        "                if decoded_key == \"vector\":\n",
        "                    # Skip printing the vector field\n",
        "                    continue\n",
        "                else:\n",
        "                    # Decode other fields and print with a label\n",
        "                    decoded_value = v.decode() if isinstance(v, bytes) else v\n",
        "                    print(f\"  {decoded_key}: {decoded_value}\")\n",
        "        if max_docs is not None and len(keys) > max_docs:\n",
        "            print(f\"\\n... and {len(keys) - max_docs} more documents (showing first {max_docs})\")\n",
        "    else:\n",
        "        print(\"No documents found in the index.\")\n",
        "\n",
        "# Call the function to print documents (prints all by default)\n",
        "print_cached_documents()\n",
        "\n",
        "# Example of printing only the first 3 documents:\n",
        "# print_cached_documents(max_docs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1n83Loqj0wE"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "You've built a production-grade semantic cache with Redis Vector. The system normalizes queries, generates embeddings, performs fast vector search, and returns cached responses when similarity is high. This approach cuts latency by 10â€“20x and reduces LLM costs by 60â€“80% for repeated queries.\n",
        "\n",
        "**Key design decisions:**\n",
        "\n",
        "<ul>\n",
        "<li>**Canonicalization** stabilizes cache keys across paraphrases\n",
        "</li>\n",
        "<li>**HNSW indexing** enables sub-50ms vector search at scale\n",
        "</li>\n",
        "<li>**Metadata gating** ensures cache hits respect model, temperature, and system prompt changes\n",
        "</li>\n",
        "<li>**TTL and namespace versioning** provide safe invalidation paths\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "**Next steps:**\n",
        "\n",
        "<ul>\n",
        "<li>Add query-side metadata filters in `FT.SEARCH` to reduce false candidates (e.g., `@model:{gpt-4o-mini} @sys_hash:{<hash>}`)\n",
        "</li>\n",
        "<li>Integrate Prometheus and Grafana for observability (track hit rate, p95 latency, cache size)\n",
        "</li>\n",
        "<li>Implement LRU eviction or score-based pruning for long-running caches\n",
        "</li>\n",
        "<li>Explore quantization (FLOAT16) to reduce memory footprint\n",
        "</li>\n",
        "<li>Scale with Redis Cluster for multi-tenant or high-throughput workloads\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "For more on building intelligent systems, see our guides on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/build-rag-pipeline\">building a RAG pipeline</a> and <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/optimize-llm-context\">optimizing LLM context windows</a>."
      ]
    }
  ],
  "metadata": {
    "title": "Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs",
    "description": "Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}