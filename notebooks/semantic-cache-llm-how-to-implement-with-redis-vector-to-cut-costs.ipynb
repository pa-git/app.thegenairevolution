{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQhv5b9Vj0vw"
      },
      "source": [
        "# üìì The GenAI Revolution Cookbook\n",
        "\n",
        "**Title:** Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs\n",
        "\n",
        "**Description:** Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.\n",
        "\n",
        "---\n",
        "\n",
        "*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFnUUE6Bj0vx"
      },
      "source": [
        "Most LLM applications waste money and time answering the same question phrased slightly differently. A semantic cache solves this by recognizing when a new query is semantically similar to a previous one and returning the cached response instantly, eliminating the need for an LLM call.\n",
        "\n",
        "This guide walks you through building a production-grade semantic cache using embeddings and Redis Vector. You'll create a Redis-backed semantic cache, complete with thresholds, TTLs, and metrics. By the end, you'll have working code, a tunable architecture, and a clear path to immediate latency and cost reductions.\n",
        "\n",
        "**What you'll build:**\n",
        "\n",
        "<ul>\n",
        "<li>A Redis HNSW vector index for semantic similarity search\n",
        "</li>\n",
        "<li>A cache layer that normalizes queries, generates embeddings, and retrieves cached responses\n",
        "</li>\n",
        "<li>A demo script to validate cache hit rates and latency improvements\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "<ul>\n",
        "<li>Python 3.9+\n",
        "</li>\n",
        "<li>Redis Stack (local via Docker or managed Redis Cloud)\n",
        "</li>\n",
        "<li>OpenAI API key\n",
        "</li>\n",
        "<li>Basic familiarity with embeddings and vector search\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "If you're using Google Colab or a cloud notebook, connect to a managed Redis Stack instance (e.g., Redis Cloud) instead of running Docker locally.\n",
        "\n",
        "For a deeper understanding of how LLMs manage memory and the concept of context rot, see our article on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/context-rot-why-llms-forget-as-their-memory-grows\">why LLMs \"forget\" as their memory grows</a>.\n",
        "\n",
        "<hr>\n",
        "\n",
        "## How It Works (High-Level Overview)\n",
        "\n",
        "**The paraphrase problem:** Users ask the same question in many ways. \"What's your refund policy?\" and \"Can I get my money back?\" are semantically identical, but traditional caching treats them as different keys.\n",
        "\n",
        "**The embedding advantage:** Embeddings map text into a high-dimensional vector space where semantically similar phrases cluster together. By comparing query embeddings using cosine similarity, you can detect paraphrases and return cached responses.\n",
        "\n",
        "**Why Redis Vector:** Redis Stack provides HNSW (Hierarchical Navigable Small World) indexing for fast approximate nearest neighbor search. It combines low-latency vector search with Redis's native TTL, tagging, and filtering capabilities, making it ideal for production caching.\n",
        "\n",
        "**Architecture:**\n",
        "\n",
        "<ol>\n",
        "<li>Normalize the user query (lowercase, strip volatile patterns like timestamps)\n",
        "</li>\n",
        "<li>Generate an embedding for the normalized query\n",
        "</li>\n",
        "<li>Search the Redis HNSW index for the nearest cached embedding\n",
        "</li>\n",
        "<li>If distance < threshold and metadata matches (model, temperature, system prompt hash), return the cached response\n",
        "</li>\n",
        "<li>Otherwise, call the LLM, cache the new response with its embedding, and return it\n",
        "</li>\n",
        "</ol>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Setup & Installation\n",
        "\n",
        "### Option 1: Managed Redis (Recommended for Notebooks)\n",
        "\n",
        "Sign up for a free Redis Cloud account at <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://redis.com/try-free\">redis.com/try-free</a> and create a Redis Stack database. Copy the connection URL.\n",
        "\n",
        "In your notebook or terminal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L-n9TWyj0vz",
        "outputId": "6d8b4a8f-e4e4-4fd7-b47e-cb4bbe13ff70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting redis\n",
            "  Downloading redis-6.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Downloading redis-6.4.0-py3-none-any.whl (279 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m279.8/279.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: redis\n",
            "Successfully installed redis-6.4.0\n"
          ]
        }
      ],
      "source": [
        "%pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjHOIKHSj0v0"
      },
      "source": [
        "Set environment variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NP21I1bj0v0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"REDIS_URL\"] = \"redis://default:password@your-redis-host:port\"\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "os.environ[\"EMBEDDING_MODEL\"] = \"text-embedding-3-small\"\n",
        "os.environ[\"CHAT_MODEL\"] = \"gpt-4o-mini\"\n",
        "os.environ[\"SIMILARITY_THRESHOLD\"] = \"0.10\"\n",
        "os.environ[\"TOP_K\"] = \"5\"\n",
        "os.environ[\"CACHE_TTL_SECONDS\"] = \"86400\"\n",
        "os.environ[\"CACHE_NAMESPACE\"] = \"sc:v1:\"\n",
        "os.environ[\"CORPUS_VERSION\"] = \"v1\"\n",
        "os.environ[\"TEMPERATURE\"] = \"0.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QktgskEcj0v1"
      },
      "source": [
        "### Option 2: Local Redis with Docker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbHhqS92j0v1"
      },
      "outputs": [],
      "source": [
        "docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J56cAYaUj0v2"
      },
      "source": [
        "Create a `.env` file:\n",
        "\n",
        "<pre><code>REDIS_URL=redis://localhost:6379\n",
        "OPENAI_API_KEY=sk-...\n",
        "EMBEDDING_MODEL=text-embedding-3-small\n",
        "CHAT_MODEL=gpt-4o-mini\n",
        "SIMILARITY_THRESHOLD=0.10\n",
        "TOP_K=5\n",
        "CACHE_TTL_SECONDS=86400\n",
        "CACHE_NAMESPACE=sc:v1:\n",
        "CORPUS_VERSION=v1\n",
        "TEMPERATURE=0.2\n",
        "</code></pre>\n",
        "\n",
        "Install dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc7pGvZlj0v2"
      },
      "outputs": [],
      "source": [
        "pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3zXkp7yj0v3"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Step-by-Step Implementation\n",
        "\n",
        "### Step 1: Create the Redis HNSW Index\n",
        "\n",
        "The index stores embeddings and metadata for cached responses. We use HNSW for fast approximate nearest neighbor search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46FV0WrQj0v3",
        "outputId": "b2720a3b-0d98-4ad1-d97d-f307feba94c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using index name: sc_index\n",
            "Dropped existing index 'sc_index' including documents.\n",
            "Index 'sc_index' created with COSINE and HNSW(M=16, EF_CONSTRUCTION=200).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import redis\n",
        "\n",
        "r = redis.Redis.from_url(os.getenv(\"REDIS_URL\"))\n",
        "\n",
        "INDEX = \"sc_index\"\n",
        "PREFIX = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "DIM = 1536\n",
        "M = 16\n",
        "EF_CONSTRUCTION = 200\n",
        "\n",
        "def create_index():\n",
        "    print(f\"Using index name: {INDEX}\")\n",
        "\n",
        "    # Drop existing index if present\n",
        "    try:\n",
        "        r.execute_command(\"FT.DROPINDEX\", INDEX, \"DD\")\n",
        "        print(f\"Dropped existing index '{INDEX}' including documents.\")\n",
        "    except redis.ResponseError:\n",
        "        print(f\"Index '{INDEX}' did not exist, proceeding with creation.\")\n",
        "\n",
        "    # Build HNSW args and let Python set the correct count\n",
        "    vector_args = [\n",
        "        \"TYPE\", \"FLOAT32\",\n",
        "        \"DIM\", str(DIM),\n",
        "        \"DISTANCE_METRIC\", \"COSINE\",\n",
        "        \"M\", str(M),\n",
        "        \"EF_CONSTRUCTION\", str(EF_CONSTRUCTION),\n",
        "    ]\n",
        "    hnsw_count = str(len(vector_args))  # <-- must equal number of tokens after \"HNSW\"\n",
        "\n",
        "    cmd = [\n",
        "        \"FT.CREATE\", INDEX,\n",
        "        \"ON\", \"HASH\",\n",
        "        \"PREFIX\", \"1\", PREFIX,\n",
        "        \"SCHEMA\",\n",
        "        \"prompt_hash\", \"TAG\",\n",
        "        \"model\", \"TAG\",\n",
        "        \"sys_hash\", \"TAG\",\n",
        "        \"corpus_version\", \"TAG\",\n",
        "        \"temperature\", \"NUMERIC\",\n",
        "        \"created_at\", \"NUMERIC\",\n",
        "        \"last_hit_at\", \"NUMERIC\",\n",
        "        \"response\", \"TEXT\",\n",
        "        \"user_question\", \"TEXT\",\n",
        "        \"vector\", \"VECTOR\", \"HNSW\", hnsw_count, *vector_args\n",
        "    ]\n",
        "\n",
        "    r.execute_command(*cmd)\n",
        "    print(f\"Index '{INDEX}' created with COSINE and HNSW(M={M}, EF_CONSTRUCTION={EF_CONSTRUCTION}).\")\n",
        "\n",
        "create_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnf6RGpNj0v4"
      },
      "source": [
        "**Validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCqqWYtRj0v4",
        "outputId": "c78f13c5-dcab-41cf-8405-d43a29809b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index Info:\n",
            "  index_name: sc_index\n",
            "  num_docs: 0\n"
          ]
        }
      ],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n",
        "\n",
        "# Helper function to decode bytes to string\n",
        "def decode_bytes(item):\n",
        "    if isinstance(item, bytes):\n",
        "        return item.decode()\n",
        "    return item\n",
        "\n",
        "# Parse the info output for better readability\n",
        "parsed_info = {}\n",
        "for i in range(0, len(info), 2):\n",
        "    key = decode_bytes(info[i])\n",
        "    value = info[i+1]\n",
        "    if isinstance(value, list):\n",
        "        # Decode lists of bytes\n",
        "        parsed_info[key] = [decode_bytes(item) for item in value]\n",
        "    else:\n",
        "        parsed_info[key] = decode_bytes(value)\n",
        "\n",
        "print(\"Index Info:\")\n",
        "print(f\"  index_name: {parsed_info.get('index_name')}\")\n",
        "print(f\"  num_docs: {parsed_info.get('num_docs')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkYR0jDUj0v4"
      },
      "source": [
        "You should see `num_docs: 0` initially.\n",
        "\n",
        "<hr>\n",
        "\n",
        "### Step 2: Normalize Queries for Stable Cache Keys\n",
        "\n",
        "Canonicalization removes volatile elements (timestamps, UUIDs, IDs) and normalizes whitespace to ensure paraphrases map to the same cache key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "Reod8eyHj0v5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import hashlib\n",
        "\n",
        "# Note: Normalization adequacy depends on expected query variations and embedding model robustness.\n",
        "VOLATILE_PATTERNS = [\n",
        "    # ISO timestamps and variations\n",
        "    r\"\\b\\d{4}-\\d{2}-\\d{2}(T|\\s)\\d{2}:\\d{2}(:\\d{2})?(Z|[+-]\\d{2}:\\d{2})?\\b\",\n",
        "    # Common date formats (MM/DD/YYYY, DD/MM/YYYY, YYYY/MM/DD, YYYY-MM-DD)\n",
        "    r\"\\b\\d{1,4}[-/.]?\\d{1,2}[-/.]?\\d{2,4}\\b\", # Updated to be more flexible with separators and year length\n",
        "    # UUID v4\n",
        "    r\"\\b[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\\b\",\n",
        "    # Long IDs (6+ digits)\n",
        "    r\"\\b\\d{6,}\\b\",\n",
        "    # Email addresses (often contain volatile parts or personally identifiable info)\n",
        "    r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\",\n",
        "]\n",
        "\n",
        "def canonicalize(text: str) -> str:\n",
        "    t = text.strip().lower()\n",
        "    for pat in VOLATILE_PATTERNS:\n",
        "        t = re.sub(pat, \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def sha256(s: str) -> str:\n",
        "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def scope_hash(prompt_norm: str, model: str, sys_hash: str, temperature: float, corpus_version: str) -> str:\n",
        "    # Unique hash for cache scope including all parameters\n",
        "    payload = f\"{prompt_norm}|{model}|{sys_hash}|{temperature}|{corpus_version}\"\n",
        "    return sha256(payload)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fycERspj0v5"
      },
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um2EQfnej0v5",
        "outputId": "23f30c1b-fc70-44d2-e57b-7519374e4aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is our refund policy on ?\n",
            "what is our refund policy on ?\n"
          ]
        }
      ],
      "source": [
        "q1 = \"What is our refund policy on 2025-01-15?\"\n",
        "q2 = \"what is our refund policy on 2025-01-20?\"\n",
        "print(canonicalize(q1))\n",
        "print(canonicalize(q2))\n",
        "# Both should output: \"what is our refund policy on\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilAUmXFkj0v6"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 3: Initialize Clients and Embedding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "Kk9WtNFoj0v6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "EMBED_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
        "CHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")\n",
        "THRESH = float(os.getenv(\"SIMILARITY_THRESHOLD\", 0.10))\n",
        "TOP_K = int(os.getenv(\"TOP_K\", 5))\n",
        "TTL = int(os.getenv(\"CACHE_TTL_SECONDS\", 86400))\n",
        "NS = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "CORPUS_VERSION = os.getenv(\"CORPUS_VERSION\", \"v1\")\n",
        "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.2))\n",
        "\n",
        "def embed(text: str) -> np.ndarray:\n",
        "    # Generate embedding and normalize for cosine distance\n",
        "    e = client.embeddings.create(model=EMBED_MODEL, input=text)\n",
        "    vec = np.array(e.data[0].embedding, dtype=np.float32)\n",
        "    norm = np.linalg.norm(vec)\n",
        "    return vec / max(norm, 1e-12)\n",
        "\n",
        "def to_bytes(vec: np.ndarray) -> bytes:\n",
        "    return vec.astype(np.float32).tobytes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywGrUE6Lj0v6"
      },
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCa7lNNcj0v7",
        "outputId": "418b50ca-ed82-44c1-f816-933eb3456efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: (1536,), norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "test_vec = embed(\"hello world\")\n",
        "print(f\"Embedding shape: {test_vec.shape}, norm: {np.linalg.norm(test_vec):.4f}\")\n",
        "# Should output shape (1536,) and norm ~1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QzDCsOXj0v7"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 4: Implement Vector Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "A_DoW81jj0v7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "DIM = 1536            # text-embedding-3-small\n",
        "TOP_K = 5             # or whatever you use\n",
        "THRESH = 0.25         # example: smaller = more similar (cosine distance)\n",
        "\n",
        "def _assert_vec_1d_f32(vec) -> np.ndarray:\n",
        "    v = np.asarray(vec, dtype=np.float32)\n",
        "    if v.ndim != 1:\n",
        "        raise ValueError(f\"query_vec must be 1D, got {v.shape}\")\n",
        "    if v.shape[0] != DIM:\n",
        "        raise ValueError(f\"vector dim {v.shape[0]} != {DIM}\")\n",
        "    return v\n",
        "\n",
        "def _to_bytes_f32(vec: np.ndarray):\n",
        "    # raw float32 bytes as required by RediSearch\n",
        "    return memoryview(vec.astype(np.float32, copy=False).tobytes())\n",
        "\n",
        "def vector_search(\n",
        "    query_vec,\n",
        "    ef_runtime: int = 200,\n",
        "    threshold: float = THRESH\n",
        ") -> Optional[Tuple[str, Dict[str, Any], float]]:\n",
        "    \"\"\"\n",
        "    Returns (doc_id, fields_dict, distance) for the nearest neighbor.\n",
        "    Distance is COSINE distance in [0, 2], usually ~0..0.4 for close matches.\n",
        "    \"\"\"\n",
        "    v = _assert_vec_1d_f32(query_vec)\n",
        "\n",
        "    # Build PARAMS: names then values. Cast ints to str for safety.\n",
        "    params = [\n",
        "        \"vec\", _to_bytes_f32(v),\n",
        "        \"ef_runtime\", str(int(ef_runtime)),\n",
        "    ]\n",
        "\n",
        "    # KNN with COSINE distance (index must be created with DISTANCE_METRIC COSINE)\n",
        "    # Smaller score = closer. We alias it as \"score\".\n",
        "    q = f\"*=>[KNN {TOP_K} @vector $vec EF_RUNTIME $ef_runtime AS score]\"\n",
        "\n",
        "    try:\n",
        "        res = r.execute_command(\n",
        "            \"FT.SEARCH\", INDEX,\n",
        "            q,\n",
        "            \"PARAMS\", str(len(params)), *params,\n",
        "            \"SORTBY\", \"score\", \"ASC\",\n",
        "            \"RETURN\", \"8\",\n",
        "                \"response\", \"model\", \"sys_hash\", \"corpus_version\",\n",
        "                \"temperature\", \"prompt_hash\", \"score\", \"user_question\",\n",
        "            \"DIALECT\", \"2\",\n",
        "        )\n",
        "    except redis.RedisError:\n",
        "        return None\n",
        "\n",
        "    if not res or res[0] < 1:\n",
        "        return None\n",
        "\n",
        "    # RediSearch returns: [total, id1, fields1, id2, fields2, ...]\n",
        "    doc_id = res[1]\n",
        "    fields = res[2]\n",
        "\n",
        "    # decode hash fields\n",
        "    f: Dict[str, Any] = {\n",
        "        (fields[i].decode() if isinstance(fields[i], bytes) else fields[i]):\n",
        "        (fields[i+1].decode() if isinstance(fields[i+1], bytes) else fields[i+1])\n",
        "        for i in range(0, len(fields), 2)\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        distance = float(f[\"score\"])  # COSINE distance: smaller is better\n",
        "    except Exception:\n",
        "        distance = float(\"inf\")\n",
        "\n",
        "    if threshold is not None and distance > threshold:\n",
        "        return None\n",
        "\n",
        "    return (doc_id.decode() if isinstance(doc_id, bytes) else doc_id), f, distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhzCtHhyj0v7"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 5: Build the Cache Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "DgRuimkRj0v8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "def sys_hash(system_prompt: str) -> str:\n",
        "    # Generates a SHA256 hash of the system prompt\n",
        "    return sha256(system_prompt.strip())\n",
        "\n",
        "def key(doc_id_hash: str) -> str:\n",
        "    # Creates a Redis key with a namespace prefix\n",
        "    return f\"{NS}{doc_id_hash}\"\n",
        "\n",
        "def metadata_matches(f: Dict[str, Any], model: str, sys_h: str, temp: float, corpus: str) -> bool:\n",
        "    # Checks if the metadata from a cached document matches the current query parameters\n",
        "    try:\n",
        "        if f.get(\"model\") != model: return False\n",
        "        if f.get(\"sys_hash\") != sys_h: return False\n",
        "        # Compare temperatures with a tolerance for floating point precision\n",
        "        if abs(float(f.get(\"temperature\", temp)) - temp) > 1e-6: return False\n",
        "        if f.get(\"corpus_version\") != corpus: return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        # Return False if there's an error during metadata comparison\n",
        "        return False\n",
        "\n",
        "def chat_call(system_prompt: str, user_prompt: str):\n",
        "    # Calls the OpenAI chat completion API\n",
        "    t0 = time.perf_counter()\n",
        "    resp = client.chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        temperature=TEMPERATURE,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "    )\n",
        "    latency_ms = (time.perf_counter() - t0) * 1000\n",
        "    content = resp.choices[0].message.content\n",
        "    usage = getattr(resp, \"usage\", None)\n",
        "    return content, latency_ms, usage\n",
        "\n",
        "def cache_get_or_generate(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH, add_to_cache: bool = True):\n",
        "    # Attempts to retrieve a response from the cache; if not found, calls the LLM and caches the response (optionally)\n",
        "    t0 = time.perf_counter()\n",
        "    sp_hash = sys_hash(system_prompt)\n",
        "    prompt_norm = canonicalize(user_prompt)\n",
        "    p_hash = sha256(prompt_norm)\n",
        "\n",
        "    qvec = embed(prompt_norm)\n",
        "\n",
        "    # --- Cache Lookup ---\n",
        "    res = vector_search(qvec, ef_runtime=ef_runtime, threshold=threshold)\n",
        "\n",
        "    # Check if a cached response was found and if its metadata matches\n",
        "    if res:\n",
        "        doc_id, fields, distance = res\n",
        "        if distance < threshold and metadata_matches(fields, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION):\n",
        "            try:\n",
        "                # Update the last hit timestamp for cache freshness\n",
        "                r.hset(doc_id, mapping={\"last_hit_at\": time.time()})\n",
        "            except redis.RedisError:\n",
        "                # Handle potential Redis errors during hset\n",
        "                pass\n",
        "            # Return the cached response details\n",
        "            return {\n",
        "                \"source\": \"cache\",\n",
        "                \"response\": fields[\"response\"],\n",
        "                \"user_question\": fields[\"user_question\"], # Include user_question for cache hits\n",
        "                \"distance\": distance,\n",
        "                \"latency_ms\": (time.perf_counter() - t0) * 1000,\n",
        "                \"closest_match_before_llm\": None # No pre-LLM closest match info on a cache hit\n",
        "            }\n",
        "\n",
        "    # --- Cache Miss - Call LLM and Cache (Optionally) ---\n",
        "\n",
        "    # If no cache hit, perform a debugging search for the closest match *before* adding the new item\n",
        "    closest_res_before_llm = vector_search(qvec, ef_runtime=ef_runtime, threshold=1.0) # Use high threshold to find closest regardless of match\n",
        "\n",
        "    content, llm_latency_ms, usage = chat_call(system_prompt, user_prompt)\n",
        "\n",
        "    # Only add to cache if add_to_cache is True\n",
        "    if add_to_cache:\n",
        "        # Generate a unique key for the new cache entry\n",
        "        doc_scope = scope_hash(prompt_norm, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION)\n",
        "        redis_key = key(doc_scope)\n",
        "\n",
        "        try:\n",
        "            # Prepare data to be stored in Redis Hash\n",
        "            mapping = {\n",
        "                \"prompt_hash\": p_hash,\n",
        "                \"model\": CHAT_MODEL,\n",
        "                \"sys_hash\": sp_hash,\n",
        "                \"corpus_version\": CORPUS_VERSION,\n",
        "                \"temperature\": TEMPERATURE,\n",
        "                \"created_at\": time.time(),\n",
        "                \"last_hit_at\": time.time(),\n",
        "                \"response\": content,\n",
        "                \"user_question\": user_prompt,\n",
        "                \"vector\": to_bytes(qvec), # Store the embedding as bytes\n",
        "            }\n",
        "            # Use a pipeline for atomic HSET and EXPIRE operations\n",
        "            pipe = r.pipeline(transaction=True)\n",
        "            pipe.hset(redis_key, mapping=mapping)\n",
        "            pipe.expire(redis_key, int(TTL)) # Set the time-to-live for the cache entry\n",
        "            pipe.execute()\n",
        "        except redis.RedisError:\n",
        "            # Handle potential Redis errors during caching\n",
        "            pass\n",
        "\n",
        "    # Prepare closest match info for the return dictionary\n",
        "    closest_match_info = None\n",
        "    if closest_res_before_llm:\n",
        "         doc_id, fields, distance = closest_res_before_llm\n",
        "         closest_match_info = {\n",
        "             \"user_question\": fields.get('user_question'),\n",
        "             \"distance\": distance\n",
        "         }\n",
        "\n",
        "\n",
        "    # Return the LLM response details\n",
        "    return {\n",
        "        \"source\": \"llm\",\n",
        "        \"response\": content,\n",
        "        \"user_question\": user_prompt, # Include user_question for LLM responses\n",
        "        \"distance\": None, # No distance for an LLM response\n",
        "        \"latency_ms\": llm_latency_ms,\n",
        "        \"usage\": {\n",
        "            \"prompt_tokens\": getattr(usage, \"prompt_tokens\", None) if usage else None,\n",
        "            \"completion_tokens\": getattr(usage, \"completion_tokens\", None) if usage else None,\n",
        "            \"total_tokens\": getattr(usage, \"total_tokens\", None) if usage else None,\n",
        "        },\n",
        "        \"closest_match_before_llm\": closest_match_info # Include closest match info before LLM call\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEMFjc1Nj0v8"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Step 6: Add Metrics Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "xJCdv9l0j0v8"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        # Initialize counters for cache hits and misses\n",
        "        self.hits = 0\n",
        "        self.misses = 0\n",
        "        # Lists to store latencies for cache hits and LLM calls\n",
        "        self.cache_latencies = []\n",
        "        self.llm_latencies = []\n",
        "\n",
        "    def record(self, result):\n",
        "        # Record metrics based on the source of the response (cache or LLM)\n",
        "        if result[\"source\"] == \"cache\":\n",
        "            self.hits += 1\n",
        "            self.cache_latencies.append(result[\"latency_ms\"])\n",
        "        else:\n",
        "            self.misses += 1\n",
        "            self.llm_latencies.append(result[\"latency_ms\"])\n",
        "\n",
        "    def snapshot(self):\n",
        "        # Calculate and return a snapshot of the current metrics\n",
        "        def safe_percentile(vals, p):\n",
        "            # Helper function to calculate percentiles safely\n",
        "            if not vals:\n",
        "                return None\n",
        "            sorted_vals = sorted(vals)\n",
        "            idx = int(len(sorted_vals) * p / 100) - 1\n",
        "            return sorted_vals[max(0, idx)]\n",
        "\n",
        "        return {\n",
        "            # Calculate the cache hit rate\n",
        "            \"hit_rate\": self.hits / max(self.hits + self.misses, 1),\n",
        "            # Calculate the median and 95th percentile latency for cache hits\n",
        "            \"p50_cache_ms\": statistics.median(self.cache_latencies) if self.cache_latencies else None,\n",
        "            \"p95_cache_ms\": safe_percentile(self.cache_latencies, 95),\n",
        "            # Calculate the median and 95th percentile latency for LLM calls\n",
        "            \"p50_llm_ms\": statistics.median(self.llm_latencies) if self.llm_latencies else None,\n",
        "            \"p95_llm_ms\": safe_percentile(self.llm_latencies, 95),\n",
        "        }\n",
        "\n",
        "metrics = Metrics()\n",
        "\n",
        "# Modify the answer function to accept add_to_cache and pass it down\n",
        "def answer(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH, add_to_cache: bool = True):\n",
        "    # Main function to get an answer, using the cache or calling the LLM\n",
        "    # Pass the add_to_cache parameter to cache_get_or_generate\n",
        "    res = cache_get_or_generate(system_prompt, user_prompt, ef_runtime=ef_runtime, threshold=threshold, add_to_cache=add_to_cache)\n",
        "    # Record the result in the metrics tracker\n",
        "    metrics.record(res)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llYkrzInj0v9"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Run and Validate\n",
        "\n",
        "### Warm the Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbMRxWUrj0v9",
        "outputId": "6837cd29-dfdb-491c-f575-e8dd5d6c1e37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warming cache...\n",
            "llm 852.1ms\n",
            "llm 503.0ms\n",
            "llm 1197.6ms\n"
          ]
        }
      ],
      "source": [
        "SYSTEM_PROMPT = \"You are a concise support assistant for ACME Corp. Use internal policy v1 for refunds and returns.\"\n",
        "seed_prompts = [\n",
        "    \"What is your refund policy?\",\n",
        "    \"How long is the return window?\",\n",
        "    \"Do you offer exchanges?\",\n",
        "]\n",
        "\n",
        "print(\"Warming cache...\")\n",
        "for p in seed_prompts:\n",
        "    res = answer(SYSTEM_PROMPT, p)\n",
        "    print(f\"{res['source']} {res['latency_ms']:.1f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KRrSFDDj0v9"
      },
      "source": [
        "### Test Paraphrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rTSaG0Nj0v9",
        "outputId": "4f98c547-5100-42d9-ef93-09fe1d5c3694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing paraphrases...\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: What are the rules for getting a refund?\n",
            "Canonicalized: what are the rules for getting a refund?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 2246.5ms\n",
            "  Token Usage: Prompt=41, Completion=107, Total=148\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'What is your refund policy?'\n",
            "    Distance: 0.33\n",
            "    Current THRESHOLD: 0.2500\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: What's the time limit to return an item?\n",
            "Canonicalized: what's the time limit to return an item?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 578.5ms\n",
            "  Token Usage: Prompt=41, Completion=17, Total=58\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.32\n",
            "    Current THRESHOLD: 0.2500\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Is it possible to swap a product for another?\n",
            "Canonicalized: is it possible to swap a product for another?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 995.4ms\n",
            "  Token Usage: Prompt=42, Completion=49, Total=91\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'Do you offer exchanges?'\n",
            "    Distance: 0.48\n",
            "    Current THRESHOLD: 0.2500\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: For how many days can I return stuff?\n",
            "Canonicalized: for how many days can i return stuff?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 674.1ms\n",
            "  Token Usage: Prompt=41, Completion=24, Total=65\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.36\n",
            "    Current THRESHOLD: 0.2500\n",
            "\n",
            "--- Testing Paraphrase ---\n",
            "Original: Where is the closest store?\n",
            "Canonicalized: where is the closest store?\n",
            "Result: CACHE MISS (LLM Call)\n",
            "  Latency: 858.3ms\n",
            "  Token Usage: Prompt=38, Completion=26, Total=64\n",
            "  Closest match in cache (before LLM call):\n",
            "    Original Cached Q: 'How long is the return window?'\n",
            "    Distance: 0.77\n",
            "    Current THRESHOLD: 0.2500\n"
          ]
        }
      ],
      "source": [
        "paraphrases = [\n",
        "    \"What are the rules for getting a refund?\",\n",
        "    \"What's the time limit to return an item?\",\n",
        "    \"Is it possible to swap a product for another?\",\n",
        "    \"For how many days can I return stuff?\",\n",
        "    \"Where is the closest store?\" # Example of a query that should not hit the cache\n",
        "]\n",
        "\n",
        "print(\"\\nTesting paraphrases...\")\n",
        "for p in paraphrases:\n",
        "    print(f\"\\n--- Testing Paraphrase ---\")\n",
        "    print(f\"Original: {p}\")\n",
        "    canonical_p = canonicalize(p)\n",
        "    print(f\"Canonicalized: {canonical_p}\")\n",
        "\n",
        "    res = answer(SYSTEM_PROMPT, p, add_to_cache=False)\n",
        "\n",
        "    if res['source'] == 'cache':\n",
        "        print(f\"Result: CACHE HIT\")\n",
        "        print(f\"  Cached Question: {res.get('user_question')}\")\n",
        "        print(f\"  Distance: {res.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "    else: # res['source'] == 'llm'\n",
        "        print(f\"Result: CACHE MISS (LLM Call)\")\n",
        "        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n",
        "        if res.get('usage'):\n",
        "             print(f\"  Token Usage: Prompt={res['usage'].get('prompt_tokens')}, Completion={res['usage'].get('completion_tokens')}, Total={res['usage'].get('total_tokens')}\")\n",
        "\n",
        "        # Display closest match information found *before* the LLM call\n",
        "        closest_info = res.get('closest_match_before_llm')\n",
        "        if closest_info:\n",
        "            print(f\"  Closest match in cache (before LLM call):\")\n",
        "            print(f\"    Original Cached Q: '{closest_info.get('user_question')}'\")\n",
        "            print(f\"    Distance: {closest_info.get('distance'):.2f}\") # Formatted to 2 decimal places\n",
        "            print(f\"    Current THRESHOLD: {THRESH:.4f}\")\n",
        "        else:\n",
        "            print(f\"  No close match found in cache (even with high threshold) before LLM call.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qv5LJEaj0v9"
      },
      "source": [
        "### Print Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tog6pXSXj0v-",
        "outputId": "b6130459-8c1c-4a3e-cfa2-72026d9ab1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics: {'hit_rate': 0.0, 'p50_cache_ms': None, 'p95_cache_ms': None, 'p50_llm_ms': 668.486534999829, 'p95_llm_ms': 2007.052681999994}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nMetrics:\", metrics.snapshot())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJZeOZFjj0v-"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "<ul>\n",
        "<li>First run: all `llm` sources, ~500‚Äì1000ms latency\n",
        "</li>\n",
        "<li>Paraphrases: mostly `cache` sources, <50ms latency, distance <0.10\n",
        "</li>\n",
        "<li>Hit rate: 60‚Äì80% for paraphrases\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Tuning the Similarity Threshold\n",
        "\n",
        "The threshold controls cache precision. Lower = stricter (fewer false hits), higher = more lenient (more hits, risk of incorrect matches)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbmSSSBbj0v-",
        "outputId": "54507070-9f2f-46ac-b3df-6d9877ae10d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Threshold=0.06\n",
            "What are the rules for getting a refund? => llm dist=N/A\n",
            "What's the time limit to return an item? => llm dist=N/A\n",
            "Is it possible to swap a product for another? => llm dist=N/A\n",
            "For how many days can I return stuff? => llm dist=N/A\n",
            "Where is the closest store? => llm dist=N/A\n",
            "\n",
            "Threshold=0.08\n",
            "What are the rules for getting a refund? => llm dist=N/A\n",
            "What's the time limit to return an item? => llm dist=N/A\n",
            "Is it possible to swap a product for another? => llm dist=N/A\n",
            "For how many days can I return stuff? => llm dist=N/A\n",
            "Where is the closest store? => llm dist=N/A\n",
            "\n",
            "Threshold=0.1\n",
            "What are the rules for getting a refund? => llm dist=N/A\n",
            "What's the time limit to return an item? => llm dist=N/A\n",
            "Is it possible to swap a product for another? => llm dist=N/A\n",
            "For how many days can I return stuff? => llm dist=N/A\n",
            "Where is the closest store? => llm dist=N/A\n",
            "\n",
            "Threshold=0.12\n",
            "What are the rules for getting a refund? => llm dist=N/A\n",
            "What's the time limit to return an item? => llm dist=N/A\n",
            "Is it possible to swap a product for another? => llm dist=N/A\n",
            "For how many days can I return stuff? => llm dist=N/A\n",
            "Where is the closest store? => llm dist=N/A\n",
            "\n",
            "Threshold=0.14\n",
            "What are the rules for getting a refund? => llm dist=N/A\n",
            "What's the time limit to return an item? => llm dist=N/A\n",
            "Is it possible to swap a product for another? => llm dist=N/A\n",
            "For how many days can I return stuff? => llm dist=N/A\n",
            "Where is the closest store? => llm dist=N/A\n"
          ]
        }
      ],
      "source": [
        "def sweep_thresholds(thresholds):\n",
        "    for t in thresholds:\n",
        "        print(f\"\\nThreshold={t}\")\n",
        "        for p in paraphrases:\n",
        "            res = cache_get_or_generate(SYSTEM_PROMPT, p, ef_runtime=150, threshold=t, add_to_cache=False)\n",
        "            distance_str = f\"{res.get('distance'):.2f}\" if res.get('distance') is not None else 'N/A'\n",
        "            cached_question_str = f\" (Cached: {res.get('user_question')})\" if res['source'] == 'cache' else ''\n",
        "            print(f\"{p} => {res['source']} dist={distance_str}{cached_question_str}\")\n",
        "\n",
        "# Assuming 'paraphrases' list is defined earlier in the notebook\n",
        "sweep_thresholds([0.06, 0.08, 0.10, 0.12, 0.14])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8x6YDIxj0v-"
      },
      "source": [
        "Start with 0.10 and adjust based on false positive rate.\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Inspect the Cache\n",
        "\n",
        "**Count indexed documents:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDJ3Iy_Jj0v_",
        "outputId": "0e66e7a9-b780-4b6a-d691-ee146fc4bd83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cached documents: 3\n"
          ]
        }
      ],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n",
        "num_docs = info[info.index(b'num_docs') + 1]\n",
        "print(f\"Cached documents: {num_docs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWjbEHpkj0v_"
      },
      "source": [
        "**Inspect a document:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zebeV7cDj0v_",
        "outputId": "ef78388f-5fa5-4049-d36f-ac4ab131dfd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 documents:\n",
            "\n",
            "--- Document 1 (Key: sc:v1:71f8a352abd2593556cf47089f0618ed0e4f184e490218e7545127147c68eb99) ---\n",
            "  created_at: 1760990562.3841846\n",
            "  user_question: What is your refund policy?\n",
            "  corpus_version: v1\n",
            "  response: ACME Corp's refund policy allows customers to request a refund within 30 days of purchase. Items must be in original condition and packaging. To initiate a return, please contact customer support for a return authorization. Refunds will be processed within 7-10 business days after the returned item is received. Certain items may be non-refundable, such as personalized or clearance items.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: b92b567fdfe4b52a6f2f93624b704f18c64c3212e2ee688b7dd2f68f10a81a50\n",
            "  last_hit_at: 1760990562.3841848\n",
            "\n",
            "--- Document 2 (Key: sc:v1:fd1a249118ff83ede5d8f4d14d19dc6720c498176a7f9a0a08636f9841a0f0fa) ---\n",
            "  created_at: 1760990564.471353\n",
            "  user_question: Do you offer exchanges?\n",
            "  corpus_version: v1\n",
            "  response: ACME Corp. does not offer exchanges. If you need a different item, please return the original item for a refund and place a new order.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: fe8a40968fc5fd2fb51084f84b99daac2e538f86fff748eee50a458729742905\n",
            "  last_hit_at: 1760990564.4713533\n",
            "\n",
            "--- Document 3 (Key: sc:v1:493173b9d91727a583115a82e58560182ee3f8ba4aa15b878f637e774b5df763) ---\n",
            "  created_at: 1760990563.3306954\n",
            "  user_question: How long is the return window?\n",
            "  corpus_version: v1\n",
            "  response: The return window is 30 days from the date of purchase.\n",
            "  temperature: 0.2\n",
            "  model: gpt-4o-mini\n",
            "  sys_hash: b5a08e9baff303bbe41fbb13c2233eba5b506fb7366023ca3b922ae02b2ff51e\n",
            "  prompt_hash: 159d9f1ce048fad930a43a6b337a5476ac857e573dceed1cf53c36ed8140c193\n",
            "  last_hit_at: 1760990563.3306956\n"
          ]
        }
      ],
      "source": [
        "def print_cached_documents(max_docs: int = None):\n",
        "    keys = r.keys(f\"{NS}*\")\n",
        "    if keys:\n",
        "        print(f\"Found {len(keys)} documents:\")\n",
        "        # Limit the keys to iterate if max_docs is specified\n",
        "        keys_to_print = keys[:max_docs] if max_docs is not None else keys\n",
        "\n",
        "        for i, key in enumerate(keys_to_print):\n",
        "            print(f\"\\n--- Document {i+1} (Key: {key.decode()}) ---\")\n",
        "            doc = r.hgetall(key)\n",
        "            # Decode bytes to string for all fields except 'vector' and print them\n",
        "            for k, v in doc.items():\n",
        "                decoded_key = k.decode()\n",
        "                if decoded_key == \"vector\":\n",
        "                    # Skip printing the vector field\n",
        "                    continue\n",
        "                else:\n",
        "                    # Decode other fields and print with a label\n",
        "                    decoded_value = v.decode() if isinstance(v, bytes) else v\n",
        "                    print(f\"  {decoded_key}: {decoded_value}\")\n",
        "        if max_docs is not None and len(keys) > max_docs:\n",
        "            print(f\"\\n... and {len(keys) - max_docs} more documents (showing first {max_docs})\")\n",
        "    else:\n",
        "        print(\"No documents found in the index.\")\n",
        "\n",
        "# Call the function to print documents (prints all by default)\n",
        "print_cached_documents()\n",
        "\n",
        "# Example of printing only the first 3 documents:\n",
        "# print_cached_documents(max_docs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1n83Loqj0wE"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "You've built a production-grade semantic cache with Redis Vector. The system normalizes queries, generates embeddings, performs fast vector search, and returns cached responses when similarity is high. This approach cuts latency by 10‚Äì20x and reduces LLM costs by 60‚Äì80% for repeated queries.\n",
        "\n",
        "**Key design decisions:**\n",
        "\n",
        "<ul>\n",
        "<li>**Canonicalization** stabilizes cache keys across paraphrases\n",
        "</li>\n",
        "<li>**HNSW indexing** enables sub-50ms vector search at scale\n",
        "</li>\n",
        "<li>**Metadata gating** ensures cache hits respect model, temperature, and system prompt changes\n",
        "</li>\n",
        "<li>**TTL and namespace versioning** provide safe invalidation paths\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "**Next steps:**\n",
        "\n",
        "<ul>\n",
        "<li>Add query-side metadata filters in `FT.SEARCH` to reduce false candidates (e.g., `@model:{gpt-4o-mini} @sys_hash:{<hash>}`)\n",
        "</li>\n",
        "<li>Integrate Prometheus and Grafana for observability (track hit rate, p95 latency, cache size)\n",
        "</li>\n",
        "<li>Implement LRU eviction or score-based pruning for long-running caches\n",
        "</li>\n",
        "<li>Explore quantization (FLOAT16) to reduce memory footprint\n",
        "</li>\n",
        "<li>Scale with Redis Cluster for multi-tenant or high-throughput workloads\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "For more on building intelligent systems, see our guides on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/build-rag-pipeline\">building a RAG pipeline</a> and <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/optimize-llm-context\">optimizing LLM context windows</a>."
      ]
    }
  ],
  "metadata": {
    "title": "Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs",
    "description": "Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}