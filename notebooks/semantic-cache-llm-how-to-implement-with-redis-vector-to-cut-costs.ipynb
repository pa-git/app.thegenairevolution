{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8p5ebsWWMpY"
      },
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n",
        "\n",
        "**Title:** Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs\n",
        "\n",
        "**Description:** Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.\n",
        "\n",
        "---\n",
        "\n",
        "*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjf_rjBVWMpZ"
      },
      "source": [
        "## Why This Matters\n",
        "\n",
        "Most LLM applications waste money and time answering the same question phrased slightly differently. A semantic cache solves this by recognizing when a new query is semantically similar to a previous one and returning the cached response instantlyâ€”no LLM call required.\n",
        "\n",
        "This guide walks you through building a production-grade semantic cache using embeddings and Redis Vector. You'll create a FastAPI microservice with a Redis-backed semantic cache, complete with thresholds, TTLs, and metrics. By the end, you'll have working code, a tunable architecture, and a clear path to immediate latency and cost reductions.\n",
        "\n",
        "**What you'll build:**\n",
        "- A Redis HNSW vector index for semantic similarity search\n",
        "- A cache layer that normalizes queries, generates embeddings, and retrieves cached responses\n",
        "- A FastAPI endpoint to serve cached or fresh LLM answers\n",
        "- A demo script to validate cache hit rates and latency improvements\n",
        "\n",
        "**Prerequisites:**\n",
        "- Python 3.9+\n",
        "- Redis Stack (local via Docker or managed Redis Cloud)\n",
        "- OpenAI API key\n",
        "- Basic familiarity with embeddings and vector search\n",
        "\n",
        "If you're using Google Colab or a cloud notebook, connect to a managed Redis Stack instance (e.g., Redis Cloud) instead of running Docker locally.\n",
        "\n",
        "For a deeper understanding of how LLMs manage memory and the concept of context rot, see our article on [why LLMs \"forget\" as their memory grows](/article/context-rot-why-llms-forget-as-their-memory-grows).\n",
        "\n",
        "---\n",
        "\n",
        "## How It Works (High-Level Overview)\n",
        "\n",
        "**The paraphrase problem:**\n",
        "Users ask the same question in many ways. \"What's your refund policy?\" and \"Can I get my money back?\" are semantically identical, but traditional caching treats them as different keys.\n",
        "\n",
        "**The embedding advantage:**\n",
        "Embeddings map text into a high-dimensional vector space where semantically similar phrases cluster together. By comparing query embeddings using cosine similarity, you can detect paraphrases and return cached responses.\n",
        "\n",
        "**Why Redis Vector:**\n",
        "Redis Stack provides HNSW (Hierarchical Navigable Small World) indexing for fast approximate nearest neighbor search. It combines low-latency vector search with Redis's native TTL, tagging, and filtering capabilitiesâ€”ideal for production caching.\n",
        "\n",
        "**Architecture:**\n",
        "1. Normalize the user query (lowercase, strip volatile patterns like timestamps)\n",
        "2. Generate an embedding for the normalized query\n",
        "3. Search the Redis HNSW index for the nearest cached embedding\n",
        "4. If distance < threshold and metadata matches (model, temperature, system prompt hash), return the cached response\n",
        "5. Otherwise, call the LLM, cache the new response with its embedding, and return it\n",
        "\n",
        "---\n",
        "\n",
        "## Setup & Installation\n",
        "\n",
        "### Option 1: Managed Redis (Recommended for Notebooks)\n",
        "\n",
        "Sign up for a free Redis Cloud account at [redis.com/try-free](https://redis.com/try-free) and create a Redis Stack database. Copy the connection URL.\n",
        "\n",
        "In your notebook or terminal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToPcBSXvWMpZ"
      },
      "outputs": [],
      "source": [
        "%pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N-tQOA7GXzRg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAN8EyJxWMpa"
      },
      "source": [
        "Set environment variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Jef31bTWMpa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"REDIS_URL\"] = \"redis://default:password@your-redis-host:port\" # Replace with your actual Redis URL\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" # OpenAI API key\n",
        "os.environ[\"EMBEDDING_MODEL\"] = \"text-embedding-3-small\" # OpenAI embedding model to use\n",
        "os.environ[\"CHAT_MODEL\"] = \"gpt-4o-mini\" # OpenAI chat model to use\n",
        "os.environ[\"SIMILARITY_THRESHOLD\"] = \"0.10\" # Cosine similarity threshold for cache hit\n",
        "os.environ[\"TOP_K\"] = \"5\" # Number of nearest neighbors to retrieve in vector search\n",
        "os.environ[\"CACHE_TTL_SECONDS\"] = \"86400\" # Time-to-live for cache entries in seconds (1 day)\n",
        "os.environ[\"CACHE_NAMESPACE\"] = \"sc:v1:\" # Namespace for cache keys in Redis\n",
        "os.environ[\"CORPUS_VERSION\"] = \"v1\" # Version of the underlying data corpus (for cache invalidation)\n",
        "os.environ[\"TEMPERATURE\"] = \"0.2\" # Temperature parameter for the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heF4bTxqWMpa"
      },
      "source": [
        "### Option 2: Local Redis with Docker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca447bd2WMpa"
      },
      "outputs": [],
      "source": [
        "docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZXIoadlWMpa"
      },
      "source": [
        "Create a `.env` file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zuc1Gn54WMpa"
      },
      "outputs": [],
      "source": [
        "REDIS_URL=redis://localhost:6379\n",
        "OPENAI_API_KEY=sk-...\n",
        "EMBEDDING_MODEL=text-embedding-3-small\n",
        "CHAT_MODEL=gpt-4o-mini\n",
        "SIMILARITY_THRESHOLD=0.10\n",
        "TOP_K=5\n",
        "CACHE_TTL_SECONDS=86400\n",
        "CACHE_NAMESPACE=sc:v1:\n",
        "CORPUS_VERSION=v1\n",
        "TEMPERATURE=0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwjrCXHIWMpa"
      },
      "source": [
        "Install dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xTd69QvWMpa"
      },
      "outputs": [],
      "source": [
        "pip install redis openai python-dotenv numpy fastapi uvicorn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWK3IuSqWMpb"
      },
      "source": [
        "---\n",
        "\n",
        "## Step-by-Step Implementation\n",
        "\n",
        "### Step 1: Create the Redis HNSW Index\n",
        "\n",
        "The index stores embeddings and metadata for cached responses. We use HNSW for fast approximate nearest neighbor search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl7TTXpuWMpb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import redis\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "r = redis.Redis.from_url(os.getenv(\"REDIS_URL\"))\n",
        "\n",
        "INDEX = \"sc_idx\"\n",
        "PREFIX = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "DIM = 1536  # Dimension for text-embedding-3-small\n",
        "M = 16  # HNSW graph connectivity\n",
        "EF_CONSTRUCTION = 200  # HNSW construction quality\n",
        "\n",
        "def create_index():\n",
        "    try:\n",
        "        r.execute_command(\"FT.INFO\", INDEX)\n",
        "        print(\"Index already exists.\")\n",
        "        return\n",
        "    except redis.ResponseError:\n",
        "        pass\n",
        "\n",
        "    # Create index with vector field and metadata tags\n",
        "    cmd = [\n",
        "        \"FT.CREATE\", INDEX,  # Command to create a full-text search index with the given name\n",
        "        \"ON\", \"HASH\",  # Index applies to Redis Hash data structures\n",
        "        \"PREFIX\", \"1\", PREFIX,  # Only index keys starting with the defined prefix\n",
        "        \"SCHEMA\",  # Define the schema of the index\n",
        "        \"prompt_hash\", \"TAG\",  # Tag field for hashing the canonicalized prompt\n",
        "        \"model\", \"TAG\",  # Tag field for the LLM model used\n",
        "        \"sys_hash\", \"TAG\",  # Tag field for hashing the system prompt\n",
        "        \"corpus_version\", \"TAG\",  # Tag field for tracking the version of the underlying corpus\n",
        "        \"temperature\", \"NUMERIC\",  # Numeric field for the temperature parameter used by the LLM\n",
        "        \"created_at\", \"NUMERIC\",  # Numeric field for the creation timestamp\n",
        "        \"last_hit_at\", \"NUMERIC\",  # Numeric field for the timestamp of the last cache hit\n",
        "        \"response\", \"TEXT\",  # Text field for the LLM's response\n",
        "        \"user_question\", \"TEXT\", # Text field for the original user question\n",
        "        \"vector\", \"VECTOR\", \"HNSW\", \"10\",  # Define a vector field named \"vector\" using the HNSW algorithm. \"10\" specifies the number of pairs for the HNSW vector definition.\n",
        "        \"TYPE\", \"FLOAT32\",  # Specify the data type of the vector embeddings\n",
        "        \"DIM\", str(DIM),  # Specify the dimension of the vector embeddings\n",
        "        \"DISTANCE_METRIC\", \"COSINE\",  # Specify the distance metric to use for vector similarity search\n",
        "        \"M\", str(M),  # HNSW parameter: number of established connections for each element during graph construction\n",
        "        \"EF_CONSTRUCTION\", str(EF_CONSTRUCTION),  # HNSW parameter: size of the dynamic list for heuristic search during graph construction\n",
        "    ]\n",
        "    r.execute_command(*cmd)\n",
        "    print(\"Index created.\")\n",
        "\n",
        "create_index()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b27ac8ee"
      },
      "source": [
        "# Define constants from environment variables\n",
        "EMBED_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
        "CHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")\n",
        "THRESH = float(os.getenv(\"SIMILARITY_THRESHOLD\", 0.10))\n",
        "TOP_K = int(os.getenv(\"TOP_K\", 5))\n",
        "TTL = int(os.getenv(\"CACHE_TTL_SECONDS\", 86400))\n",
        "NS = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "CORPUS_VERSION = os.getenv(\"CORPUS_VERSION\", \"v1\")\n",
        "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.2))\n",
        "\n",
        "print(\"Constants defined from environment variables.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUD4mmX9WMpb"
      },
      "source": [
        "**Validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJnkpJ_HWMpb"
      },
      "outputs": [],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n",
        "print(\"Index info:\", info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv1BZsVHWMpb"
      },
      "source": [
        "You should see `num_docs: 0` initially.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Normalize Queries for Stable Cache Keys\n",
        "\n",
        "Canonicalization removes volatile elements (timestamps, UUIDs, IDs) and normalizes whitespace to ensure paraphrases map to the same cache key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXBcpXpZWMpb"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import hashlib\n",
        "\n",
        "# Note: Normalization adequacy depends on expected query variations and embedding model robustness.\n",
        "VOLATILE_PATTERNS = [\n",
        "    # ISO timestamps and variations\n",
        "    r\"\\b\\d{4}-\\d{2}-\\d{2}(T|\\s)\\d{2}:\\d{2}(:\\d{2})?(Z|[+-]\\d{2}:\\d{2})?\\b\",\n",
        "    # Common date formats (MM/DD/YYYY, DD/MM/YYYY, YYYY/MM/DD, YYYY-MM-DD)\n",
        "    r\"\\b\\d{1,4}[-/.]?\\d{1,2}[-/.]?\\d{2,4}\\b\", # Updated to be more flexible with separators and year length\n",
        "    # UUID v4\n",
        "    r\"\\b[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\\b\",\n",
        "    # Long IDs (6+ digits)\n",
        "    r\"\\b\\d{6,}\\b\",\n",
        "    # Email addresses (often contain volatile parts or personally identifiable info)\n",
        "    r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\",\n",
        "]\n",
        "\n",
        "def canonicalize(text: str) -> str:\n",
        "    t = text.strip().lower()\n",
        "    for pat in VOLATILE_PATTERNS:\n",
        "        t = re.sub(pat, \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def sha256(s: str) -> str:\n",
        "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def scope_hash(prompt_norm: str, model: str, sys_hash: str, temperature: float, corpus_version: str) -> str:\n",
        "    # Unique hash for cache scope including all parameters\n",
        "    payload = f\"{prompt_norm}|{model}|{sys_hash}|{temperature}|{corpus_version}\"\n",
        "    return sha256(payload)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfmKlWRsWMpb"
      },
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEy4rQNHWMpb"
      },
      "outputs": [],
      "source": [
        "q1 = \"What is our refund policy on 2025-01-15?\"\n",
        "q2 = \"what is our refund policy on 2025-01-20?\"\n",
        "print(canonicalize(q1))\n",
        "print(canonicalize(q2))\n",
        "# Both should output: \"what is our refund policy on\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LYik64jWMpb"
      },
      "source": [
        "---\n",
        "\n",
        "### Step 3: Initialize Clients and Embedding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snaPItHnWMpb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "EMBED_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
        "CHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")\n",
        "THRESH = float(os.getenv(\"SIMILARITY_THRESHOLD\", 0.10))\n",
        "TOP_K = int(os.getenv(\"TOP_K\", 5))\n",
        "TTL = int(os.getenv(\"CACHE_TTL_SECONDS\", 86400))\n",
        "NS = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "CORPUS_VERSION = os.getenv(\"CORPUS_VERSION\", \"v1\")\n",
        "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.2))\n",
        "\n",
        "def embed(text: str) -> np.ndarray:\n",
        "    # Generate embedding and normalize for cosine distance\n",
        "    e = client.embeddings.create(model=EMBED_MODEL, input=text)\n",
        "    vec = np.array(e.data[0].embedding, dtype=np.float32)\n",
        "    norm = np.linalg.norm(vec)\n",
        "    return vec / max(norm, 1e-12)\n",
        "\n",
        "def to_bytes(vec: np.ndarray) -> bytes:\n",
        "    return vec.astype(np.float32).tobytes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFctx38_WMpb"
      },
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an_zfif0WMpb"
      },
      "outputs": [],
      "source": [
        "test_vec = embed(\"hello world\")\n",
        "print(f\"Embedding shape: {test_vec.shape}, norm: {np.linalg.norm(test_vec):.4f}\")\n",
        "# Should output shape (1536,) and norm ~1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umtBRUJxWMpb"
      },
      "source": [
        "---\n",
        "\n",
        "### Step 4: Implement Vector Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpgLV12oWMpb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "def vector_search(query_vec, ef_runtime: int = 100, threshold: float = THRESH) -> Optional[Tuple[str, Dict[str, Any], float]]:\n",
        "    # Perform KNN search with EF_RUNTIME parameter\n",
        "    params = [\"vec\", to_bytes(query_vec), \"ef_runtime\", ef_runtime]\n",
        "    q = f\"*=>[KNN {TOP_K} @vector $vec EF_RUNTIME $ef_runtime AS score]\"\n",
        "    try:\n",
        "        res = r.execute_command(\n",
        "            \"FT.SEARCH\", INDEX,\n",
        "            q, \"PARAMS\", str(len(params)), *params,\n",
        "            \"SORTBY\", \"score\", \"ASC\",\n",
        "            \"RETURN\", \"8\", \"response\", \"model\", \"sys_hash\", \"corpus_version\", \"temperature\", \"prompt_hash\", \"user_question\", \"score\",\n",
        "            \"DIALECT\", \"2\"\n",
        "        )\n",
        "    except redis.RedisError:\n",
        "        return None\n",
        "\n",
        "    total = res[0] if res else 0\n",
        "    if total < 1:\n",
        "        return None\n",
        "\n",
        "    doc_id = res[1]\n",
        "    fields = res[2]\n",
        "    f = {fields[i].decode() if isinstance(fields[i], bytes) else fields[i]:\n",
        "         fields[i+1].decode() if isinstance(fields[i+1], bytes) else fields[i+1]\n",
        "         for i in range(0, len(fields), 2)}\n",
        "\n",
        "    try:\n",
        "        distance = float(f[\"score\"])\n",
        "    except Exception:\n",
        "        distance = 1.0\n",
        "\n",
        "    return doc_id.decode() if isinstance(doc_id, bytes) else doc_id, f, distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OxlJGOIWMpb"
      },
      "source": [
        "---\n",
        "\n",
        "### Step 5: Build the Cache Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwbdloO9WMpb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "def sys_hash(system_prompt: str) -> str:\n",
        "    return sha256(system_prompt.strip())\n",
        "\n",
        "def key(doc_id_hash: str) -> str:\n",
        "    return f\"{NS}{doc_id_hash}\"\n",
        "\n",
        "def metadata_matches(f: Dict[str, Any], model: str, sys_h: str, temp: float, corpus: str) -> bool:\n",
        "    try:\n",
        "        if f.get(\"model\") != model: return False\n",
        "        if f.get(\"sys_hash\") != sys_h: return False\n",
        "        if abs(float(f.get(\"temperature\", temp)) - temp) > 1e-6: return False\n",
        "        if f.get(\"corpus_version\") != corpus: return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def chat_call(system_prompt: str, user_prompt: str):\n",
        "    t0 = time.perf_counter()\n",
        "    resp = client.chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        temperature=TEMPERATURE,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "    )\n",
        "    latency_ms = (time.perf_counter() - t0) * 1000\n",
        "    content = resp.choices[0].message.content\n",
        "    usage = getattr(resp, \"usage\", None)\n",
        "    return content, latency_ms, usage\n",
        "\n",
        "def cache_get_or_generate(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH):\n",
        "    # Start timing the cache lookup process\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    # Generate hashes and canonicalize prompt for consistent caching\n",
        "    sp_hash = sys_hash(system_prompt)\n",
        "    prompt_norm = canonicalize(user_prompt)\n",
        "    p_hash = sha256(prompt_norm)\n",
        "\n",
        "    # Generate embedding for the normalized prompt\n",
        "    qvec = embed(prompt_norm)\n",
        "\n",
        "    # Perform vector search in Redis to find similar cached entries\n",
        "    res = vector_search(qvec, ef_runtime=ef_runtime, threshold=threshold)\n",
        "\n",
        "    # Check if a cached result was found and meets criteria\n",
        "    if res:\n",
        "        doc_id, fields, distance = res\n",
        "        # Check if the semantic distance is within the threshold and metadata matches\n",
        "        if distance < threshold and metadata_matches(fields, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION):\n",
        "            try:\n",
        "                # If cache hit, update the last hit timestamp in Redis\n",
        "                r.hset(doc_id, mapping={\"last_hit_at\": time.time()})\n",
        "            except redis.RedisError:\n",
        "                # Handle potential Redis errors during update\n",
        "                pass\n",
        "            # Return the cached response with details\n",
        "            return {\n",
        "                \"source\": \"cache\",\n",
        "                \"response\": fields[\"response\"],\n",
        "                \"distance\": distance,\n",
        "                \"latency_ms\": (time.perf_counter() - t0) * 1000,\n",
        "                \"user_question\": fields.get(\"user_question\"), # Include user_question in cache hit response\n",
        "            }\n",
        "\n",
        "    # If no cache hit, call the LLM to generate a new response\n",
        "    content, llm_latency_ms, usage = chat_call(system_prompt, user_prompt)\n",
        "\n",
        "    # Generate a unique key for the new cache entry based on prompt scope\n",
        "    doc_scope = scope_hash(prompt_norm, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION)\n",
        "    doc_key = key(doc_scope)\n",
        "\n",
        "    # Prepare data to be cached\n",
        "    try:\n",
        "        mapping = {\n",
        "            \"prompt_hash\": p_hash,\n",
        "            \"model\": CHAT_MODEL,\n",
        "            \"sys_hash\": sp_hash,\n",
        "            \"corpus_version\": CORPUS_VERSION,\n",
        "            \"temperature\": TEMPERATURE,\n",
        "            \"created_at\": time.time(),\n",
        "            \"last_hit_at\": time.time(),\n",
        "            \"response\": content,\n",
        "            \"user_question\": user_prompt,\n",
        "            \"vector\": to_bytes(qvec),\n",
        "        }\n",
        "        # Use a Redis pipeline for atomic HSET and EXPIRE commands\n",
        "        pipe = r.pipeline(transaction=True)\n",
        "        pipe.hset(doc_key, mapping=mapping)\n",
        "        pipe.expire(doc_key, int(TTL))\n",
        "        pipe.execute()\n",
        "    except redis.RedisError:\n",
        "        # Handle potential Redis errors during caching\n",
        "        pass\n",
        "\n",
        "    # Return the LLM-generated response with details\n",
        "    return {\n",
        "        \"source\": \"llm\",\n",
        "        \"response\": content,\n",
        "        \"distance\": None,\n",
        "        \"latency_ms\": llm_latency_ms,\n",
        "        \"usage\": {\n",
        "            \"prompt_tokens\": getattr(usage, \"prompt_tokens\", None) if usage else None,\n",
        "            \"completion_tokens\": getattr(usage, \"completion_tokens\", None) if usage else None,\n",
        "            \"total_tokens\": getattr(usage, \"total_tokens\", None) if usage else None,\n",
        "        },\n",
        "        \"user_question\": user_prompt, # Include user_question in LLM response\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6puABUcWMpb"
      },
      "source": [
        "---\n",
        "\n",
        "### Step 6: Add Metrics Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rCFIwKPWMpb"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        self.hits = 0\n",
        "        self.misses = 0\n",
        "        self.cache_latencies = []\n",
        "        self.llm_latencies = []\n",
        "\n",
        "    def record(self, result):\n",
        "        if result[\"source\"] == \"cache\":\n",
        "            self.hits += 1\n",
        "            self.cache_latencies.append(result[\"latency_ms\"])\n",
        "        else:\n",
        "            self.misses += 1\n",
        "            self.llm_latencies.append(result[\"latency_ms\"])\n",
        "\n",
        "    def snapshot(self):\n",
        "        def safe_percentile(vals, p):\n",
        "            if not vals:\n",
        "                return None\n",
        "            sorted_vals = sorted(vals)\n",
        "            idx = int(len(sorted_vals) * p / 100) - 1\n",
        "            return sorted_vals[max(0, idx)]\n",
        "\n",
        "        return {\n",
        "            \"hit_rate\": self.hits / max(self.hits + self.misses, 1),\n",
        "            \"p50_cache_ms\": statistics.median(self.cache_latencies) if self.cache_latencies else None,\n",
        "            \"p95_cache_ms\": safe_percentile(self.cache_latencies, 95),\n",
        "            \"p50_llm_ms\": statistics.median(self.llm_latencies) if self.llm_latencies else None,\n",
        "            \"p95_llm_ms\": safe_percentile(self.llm_latencies, 95),\n",
        "        }\n",
        "\n",
        "metrics = Metrics()\n",
        "\n",
        "def answer(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH):\n",
        "    res = cache_get_or_generate(system_prompt, user_prompt, ef_runtime=ef_runtime, threshold=threshold)\n",
        "    metrics.record(res)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxiWAM5cWMpb"
      },
      "source": [
        "---\n",
        "\n",
        "### Step 7: Build the FastAPI Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnwsOiO6WMpb"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class Query(BaseModel):\n",
        "    system_prompt: str\n",
        "    user_prompt: str\n",
        "    ef_runtime: int | None = 100\n",
        "\n",
        "@app.post(\"/semantic-cache/answer\")\n",
        "def semantic_answer(q: Query):\n",
        "    res = answer(q.system_prompt, q.user_prompt, ef_runtime=q.ef_runtime or 100)\n",
        "    return res\n",
        "\n",
        "@app.get(\"/semantic-cache/metrics\")\n",
        "def get_metrics():\n",
        "    return metrics.snapshot()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa14b1e5"
      },
      "source": [
        "%%writefile app.py\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import os\n",
        "import redis\n",
        "import time\n",
        "import re\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "import statistics\n",
        "from dotenv import load_dotenv\n",
        "from typing import Optional, Dict, Any, Tuple # Import Optional, Dict, Any, Tuple\n",
        "\n",
        "# Load environment variables (assuming .env or environment variables are set)\n",
        "load_dotenv()\n",
        "\n",
        "# Define constants from environment variables - Ensure these are defined before app is created\n",
        "EMBED_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
        "CHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")\n",
        "THRESH = float(os.getenv(\"SIMILARITY_THRESHOLD\", 0.10))\n",
        "TOP_K = int(os.getenv(\"TOP_K\", 5))\n",
        "TTL = int(os.getenv(\"CACHE_TTL_SECONDS\", 86400))\n",
        "NS = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\n",
        "CORPUS_VERSION = os.getenv(\"CORPUS_VERSION\", \"v1\")\n",
        "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.2))\n",
        "INDEX = \"sc_idx\" # Define INDEX here as it's used in create_index\n",
        "\n",
        "# Initialize Redis client - Ensure REDIS_URL is set in environment variables\n",
        "try:\n",
        "    r = redis.Redis.from_url(os.getenv(\"REDIS_URL\"))\n",
        "    # Check connection\n",
        "    r.ping()\n",
        "    print(\"Connected to Redis successfully!\")\n",
        "except redis.exceptions.ConnectionError as e:\n",
        "    print(f\"Could not connect to Redis: {e}\")\n",
        "    r = None # Set r to None if connection fails\n",
        "\n",
        "# Define HNSW parameters (should match index creation)\n",
        "DIM = 1536\n",
        "M = 16\n",
        "EF_CONSTRUCTION = 200\n",
        "\n",
        "\n",
        "# --- Index Creation Function (Moved from earlier cell) ---\n",
        "# This function should ideally be run once during setup, not every time the app starts\n",
        "# For this notebook demo, we include it, but in a production app, index creation\n",
        "# is usually handled separately.\n",
        "def create_index():\n",
        "    if not r: return # Skip if Redis connection failed\n",
        "    try:\n",
        "        r.execute_command(\"FT.INFO\", INDEX)\n",
        "        print(\"Index already exists.\")\n",
        "        return\n",
        "    except redis.ResponseError:\n",
        "        pass\n",
        "\n",
        "    cmd = [\n",
        "        \"FT.CREATE\", INDEX,\n",
        "        \"ON\", \"HASH\",\n",
        "        \"PREFIX\", \"1\", NS, # Use NS here as defined from env var\n",
        "        \"SCHEMA\",\n",
        "        \"prompt_hash\", \"TAG\",\n",
        "        \"model\", \"TAG\",\n",
        "        \"sys_hash\", \"TAG\",\n",
        "        \"corpus_version\", \"TAG\",\n",
        "        \"temperature\", \"NUMERIC\",\n",
        "        \"created_at\", \"NUMERIC\",\n",
        "        \"last_hit_at\", \"NUMERIC\",\n",
        "        \"response\", \"TEXT\",\n",
        "        \"user_question\", \"TEXT\",\n",
        "        \"vector\", \"VECTOR\", \"HNSW\", \"10\",\n",
        "        \"TYPE\", \"FLOAT32\",\n",
        "        \"DIM\", str(DIM),\n",
        "        \"DISTANCE_METRIC\", \"COSINE\",\n",
        "        \"M\", str(M),\n",
        "        \"EF_CONSTRUCTION\", str(EF_CONSTRUCTION),\n",
        "    ]\n",
        "    try:\n",
        "        r.execute_command(*cmd)\n",
        "        print(\"Index created.\")\n",
        "    except redis.RedisError as e:\n",
        "        print(f\"Error creating index: {e}\")\n",
        "\n",
        "# Call create index when the app file is written/imported\n",
        "create_index()\n",
        "\n",
        "\n",
        "# --- Normalization Functions (Moved from earlier cell) ---\n",
        "VOLATILE_PATTERNS = [\n",
        "    r\"\\b\\d{4}-\\d{2}-\\d{2}(T|\\s)\\d{2}:\\d{2}(:\\d{2})?(Z|[+-]\\d{2}:\\d{2})?\\b\",\n",
        "    r\"\\b\\d{1,4}[-/.]?\\d{1,2}[-/.]?\\d{2,4}\\b\",\n",
        "    r\"\\b[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\\b\",\n",
        "    r\"\\b\\d{6,}\\b\",\n",
        "    r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\",\n",
        "]\n",
        "\n",
        "def canonicalize(text: str) -> str:\n",
        "    t = text.strip().lower()\n",
        "    for pat in VOLATILE_PATTERNS:\n",
        "        t = re.sub(pat, \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def sha256(s: str) -> str:\n",
        "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def scope_hash(prompt_norm: str, model: str, sys_hash: str, temperature: float, corpus_version: str) -> str:\n",
        "    payload = f\"{prompt_norm}|{model}|{sys_hash}|{temperature}|{corpus_version}\"\n",
        "    return sha256(payload)\n",
        "\n",
        "def sys_hash(system_prompt: str) -> str:\n",
        "    return sha256(system_prompt.strip())\n",
        "\n",
        "def key(doc_id_hash: str) -> str:\n",
        "    return f\"{NS}{doc_id_hash}\"\n",
        "\n",
        "\n",
        "# --- OpenAI Client & Embedding Function (Moved from earlier cell) ---\n",
        "# Initialize OpenAI client - Ensure OPENAI_API_KEY is set in environment variables\n",
        "try:\n",
        "    client = OpenAI()\n",
        "    # Optional: check if client can connect/authenticate\n",
        "    # client.models.list()\n",
        "    print(\"OpenAI client initialized.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing OpenAI client: {e}\")\n",
        "    client = None # Set client to None if initialization fails\n",
        "\n",
        "\n",
        "def embed(text: str) -> np.ndarray:\n",
        "    if not client: raise ConnectionError(\"OpenAI client not initialized.\")\n",
        "    # Generate embedding and normalize for cosine distance\n",
        "    e = client.embeddings.create(model=EMBED_MODEL, input=text)\n",
        "    vec = np.array(e.data[0].embedding, dtype=np.float32)\n",
        "    norm = np.linalg.norm(vec)\n",
        "    return vec / max(norm, 1e-12)\n",
        "\n",
        "def to_bytes(vec: np.ndarray) -> bytes:\n",
        "    return vec.astype(np.float32).tobytes()\n",
        "\n",
        "\n",
        "# --- Vector Search Function (Moved from earlier cell) ---\n",
        "def vector_search(query_vec, ef_runtime: int = 100, threshold: float = THRESH) -> Optional[Tuple[str, Dict[str, Any], float]]:\n",
        "    if not r: return None # Skip if Redis connection failed\n",
        "    # Perform KNN search with EF_RUNTIME parameter\n",
        "    params = [\"vec\", to_bytes(query_vec), \"ef_runtime\", ef_runtime]\n",
        "    # Correct query syntax: Use $vec for vector parameter in KNN search\n",
        "    q = f\"*=>[KNN {TOP_K} @vector $vec EF_RUNTIME $ef_runtime AS score]\"\n",
        "    try:\n",
        "        res = r.execute_command(\n",
        "            \"FT.SEARCH\", INDEX,\n",
        "            q, \"PARAMS\", str(len(params)), *params,\n",
        "            \"SORTBY\", \"score\", \"ASC\",\n",
        "            \"RETURN\", \"8\", \"response\", \"model\", \"sys_hash\", \"corpus_version\", \"temperature\", \"prompt_hash\", \"user_question\", \"score\",\n",
        "            \"DIALECT\", \"2\"\n",
        "        )\n",
        "    except redis.RedisError as e:\n",
        "        print(f\"Redis search error: {e}\")\n",
        "        return None\n",
        "\n",
        "    total = res[0] if res else 0\n",
        "    if total < 1:\n",
        "        return None\n",
        "\n",
        "    # Process search results\n",
        "    # The result format is [total_results, doc1_id, [field1, value1, field2, value2, ...], doc2_id, [...], ...]\n",
        "    # We expect the first result to be the best match\n",
        "    if len(res) > 1:\n",
        "        doc_id = res[1]\n",
        "        fields = res[2]\n",
        "        # Decode bytes to string for keys and values where appropriate\n",
        "        f = {fields[i].decode() if isinstance(fields[i], bytes) else fields[i]:\n",
        "             fields[i+1].decode() if isinstance(fields[i+1], bytes) else fields[i+1]\n",
        "             for i in range(0, len(fields), 2)}\n",
        "\n",
        "        try:\n",
        "            distance = float(f.get(\"score\", 1.0)) # Use .get() with a default to avoid KeyError\n",
        "        except Exception:\n",
        "            distance = 1.0\n",
        "\n",
        "        return doc_id.decode() if isinstance(doc_id, bytes) else doc_id, f, distance\n",
        "    else:\n",
        "        return None # No results found\n",
        "\n",
        "\n",
        "# --- Cache Layer Function (Moved from earlier cell) ---\n",
        "def metadata_matches(f: Dict[str, Any], model: str, sys_h: str, temp: float, corpus: str) -> bool:\n",
        "    # Ensure keys exist and types match before comparison\n",
        "    try:\n",
        "        if f.get(\"model\") != model: return False\n",
        "        if f.get(\"sys_hash\") != sys_h: return False\n",
        "        # Compare temperatures with a tolerance for floating point\n",
        "        cached_temp = float(f.get(\"temperature\", -1.0)) # Use a value outside expected range as default\n",
        "        if abs(cached_temp - temp) > 1e-6: return False\n",
        "        if f.get(\"corpus_version\") != corpus: return False\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Metadata match error: {e}\")\n",
        "        return False\n",
        "\n",
        "def chat_call(system_prompt: str, user_prompt: str):\n",
        "    if not client: raise ConnectionError(\"OpenAI client not initialized.\")\n",
        "    t0 = time.perf_counter()\n",
        "    resp = client.chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        temperature=TEMPERATURE,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "    )\n",
        "    latency_ms = (time.perf_counter() - t0) * 1000\n",
        "    content = resp.choices[0].message.content\n",
        "    usage = getattr(resp, \"usage\", None)\n",
        "    return content, latency_ms, usage\n",
        "\n",
        "def cache_get_or_generate(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH):\n",
        "    # Start timing the cache lookup process\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    # Generate hashes and canonicalize prompt for consistent caching\n",
        "    sp_hash = sys_hash(system_prompt)\n",
        "    prompt_norm = canonicalize(user_prompt)\n",
        "    p_hash = sha256(prompt_norm)\n",
        "\n",
        "    # Generate embedding for the normalized prompt\n",
        "    try:\n",
        "        qvec = embed(prompt_norm)\n",
        "    except ConnectionError as e:\n",
        "        print(f\"Embedding failed: {e}\")\n",
        "        # Fallback to LLM call if embedding fails\n",
        "        content, llm_latency_ms, usage = chat_call(system_prompt, user_prompt)\n",
        "        return {\n",
        "            \"source\": \"llm_fallback_embed_error\",\n",
        "            \"response\": content,\n",
        "            \"distance\": None,\n",
        "            \"latency_ms\": llm_latency_ms,\n",
        "            \"usage\": {\n",
        "                \"prompt_tokens\": getattr(usage, \"prompt_tokens\", None) if usage else None,\n",
        "                \"completion_tokens\": getattr(usage, \"completion_tokens\", None) if usage else None,\n",
        "                \"total_tokens\": getattr(usage, \"total_tokens\", None) if usage else None,\n",
        "            },\n",
        "            \"user_question\": user_prompt,\n",
        "        }\n",
        "\n",
        "\n",
        "    # Perform vector search in Redis to find similar cached entries\n",
        "    res = vector_search(qvec, ef_runtime=ef_runtime, threshold=threshold)\n",
        "\n",
        "    # Check if a cached result was found and meets criteria\n",
        "    if res:\n",
        "        doc_id, fields, distance = res\n",
        "        # Check if the semantic distance is within the threshold and metadata matches\n",
        "        if distance < threshold and metadata_matches(fields, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION):\n",
        "            if r: # Only HSET if Redis is connected\n",
        "                try:\n",
        "                    # If cache hit, update the last hit timestamp in Redis\n",
        "                    r.hset(doc_id, mapping={\"last_hit_at\": time.time()})\n",
        "                except redis.RedisError:\n",
        "                    # Handle potential Redis errors during update\n",
        "                    pass\n",
        "            # Return the cached response with details\n",
        "            return {\n",
        "                \"source\": \"cache\",\n",
        "                \"response\": fields[\"response\"],\n",
        "                \"distance\": distance,\n",
        "                \"latency_ms\": (time.perf_counter() - t0) * 1000,\n",
        "                \"user_question\": fields.get(\"user_question\"),\n",
        "            }\n",
        "\n",
        "    # If no cache hit, call the LLM to generate a new response\n",
        "    try:\n",
        "        content, llm_latency_ms, usage = chat_call(system_prompt, user_prompt)\n",
        "    except ConnectionError as e:\n",
        "        print(f\"LLM call failed: {e}\")\n",
        "        # Handle LLM call failure (e.g., return an error message)\n",
        "        return {\n",
        "            \"source\": \"error_llm_call\",\n",
        "            \"response\": f\"Error calling LLM: {e}\",\n",
        "            \"distance\": None,\n",
        "            \"latency_ms\": (time.perf_counter() - t0) * 1000,\n",
        "            \"usage\": None,\n",
        "            \"user_question\": user_prompt,\n",
        "        }\n",
        "\n",
        "\n",
        "    # Cache the new response if Redis is connected\n",
        "    if r:\n",
        "        # Generate a unique key for the new cache entry based on prompt scope\n",
        "        doc_scope = scope_hash(prompt_norm, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION)\n",
        "        doc_key = key(doc_scope)\n",
        "\n",
        "        # Prepare data to be cached\n",
        "        try:\n",
        "            mapping = {\n",
        "                \"prompt_hash\": p_hash,\n",
        "                \"model\": CHAT_MODEL,\n",
        "                \"sys_hash\": sp_hash,\n",
        "                \"corpus_version\": CORPUS_VERSION,\n",
        "                \"temperature\": TEMPERATURE,\n",
        "                \"created_at\": time.time(),\n",
        "                \"last_hit_at\": time.time(),\n",
        "                \"response\": content,\n",
        "                \"user_question\": user_prompt,\n",
        "                \"vector\": to_bytes(qvec),\n",
        "            }\n",
        "            # Use a Redis pipeline for atomic HSET and EXPIRE commands\n",
        "            pipe = r.pipeline(transaction=True)\n",
        "            pipe.hset(doc_key, mapping=mapping)\n",
        "            pipe.expire(doc_key, int(TTL))\n",
        "            pipe.execute()\n",
        "        except redis.RedisError as e:\n",
        "            print(f\"Error caching response: {e}\")\n",
        "            pass # Handle potential Redis errors during caching\n",
        "\n",
        "    # Return the LLM-generated response with details\n",
        "    return {\n",
        "        \"source\": \"llm\",\n",
        "        \"response\": content,\n",
        "        \"distance\": None,\n",
        "        \"latency_ms\": llm_latency_ms,\n",
        "        \"usage\": {\n",
        "            \"prompt_tokens\": getattr(usage, \"prompt_tokens\", None) if usage else None,\n",
        "            \"completion_tokens\": getattr(usage, \"completion_tokens\", None) if usage else None,\n",
        "            \"total_tokens\": getattr(usage, \"total_tokens\", None) if usage else None,\n",
        "        },\n",
        "        \"user_question\": user_prompt,\n",
        "    }\n",
        "\n",
        "# --- Metrics Tracking (Moved from earlier cell) ---\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        self.hits = 0\n",
        "        self.misses = 0\n",
        "        self.cache_latencies = []\n",
        "        self.llm_latencies = []\n",
        "\n",
        "    def record(self, result):\n",
        "        if result[\"source\"] == \"cache\":\n",
        "            self.hits += 1\n",
        "            self.cache_latencies.append(result.get(\"latency_ms\", 0)) # Use get with default\n",
        "        elif \"llm\" in result[\"source\"]: # Catch 'llm' and 'llm_fallback_embed_error'\n",
        "            self.misses += 1\n",
        "            self.llm_latencies.append(result.get(\"latency_ms\", 0)) # Use get with default\n",
        "\n",
        "    def snapshot(self):\n",
        "        def safe_percentile(vals, p):\n",
        "            if not vals:\n",
        "                return None\n",
        "            sorted_vals = sorted(vals)\n",
        "            # Adjust index calculation for 0-based indexing and handle edge cases\n",
        "            idx = max(0, min(int(len(sorted_vals) * p / 100.0) -1, len(sorted_vals)-1))\n",
        "            return sorted_vals[idx]\n",
        "\n",
        "        total_requests = self.hits + self.misses\n",
        "\n",
        "        return {\n",
        "            \"total_requests\": total_requests,\n",
        "            \"hits\": self.hits,\n",
        "            \"misses\": self.misses,\n",
        "            \"hit_rate\": self.hits / max(total_requests, 1),\n",
        "            \"p50_cache_ms\": statistics.median(self.cache_latencies) if self.cache_latencies else None,\n",
        "            \"p95_cache_ms\": safe_percentile(self.cache_latencies, 95),\n",
        "            \"p50_llm_ms\": statistics.median(self.llm_latencies) if self.llm_latencies else None,\n",
        "            \"p95_llm_ms\": safe_percentile(self.llm_latencies, 95),\n",
        "        }\n",
        "\n",
        "metrics = Metrics()\n",
        "\n",
        "def answer(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH):\n",
        "    # Ensure THRESH is defined before calling cache_get_or_generate\n",
        "    return cache_get_or_generate(system_prompt, user_prompt, ef_runtime=ef_runtime, threshold=threshold)\n",
        "\n",
        "\n",
        "# --- FastAPI App Definition ---\n",
        "app = FastAPI()\n",
        "\n",
        "class Query(BaseModel):\n",
        "    system_prompt: str\n",
        "    user_prompt: str\n",
        "    ef_runtime: int | None = 100 # Use Optional[int] or int | None for optional fields\n",
        "\n",
        "@app.post(\"/semantic-cache/answer\")\n",
        "def semantic_answer(q: Query):\n",
        "    # Pass THRESH to the answer function\n",
        "    res = answer(q.system_prompt, q.user_prompt, ef_runtime=q.ef_runtime or 100, threshold=THRESH)\n",
        "    return res\n",
        "\n",
        "@app.get(\"/semantic-cache/metrics\")\n",
        "def get_metrics():\n",
        "    return metrics.snapshot()\n",
        "\n",
        "# Note: To run this app, save this code as 'app.py' and run 'uvicorn app:app --reload' from your terminal.\n",
        "# In Colab, you would typically run uvicorn in a separate process or use a tool like ngrok to expose it.\n",
        "# The cell below (Ugms0YW6WMpc) attempts to run uvicorn directly which will block the notebook.\n",
        "# For demonstration within the notebook, you might need a different approach or just use the functions directly."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngVsezdtWMpc"
      },
      "source": [
        "**Run the service:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugms0YW6WMpc"
      },
      "outputs": [],
      "source": [
        "# Using nohup and & allows the uvicorn server to run in the background\n",
        "# in a Colab environment, so the notebook cell doesn't block and you can\n",
        "# execute subsequent cells. nohup prevents the process from terminating\n",
        "# if the notebook session disconnects, and & sends the process to the background.\n",
        "!nohup uvicorn app:app --reload &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBD5QL1CWMpc"
      },
      "source": [
        "**Test with curl:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qhMYLFYWMpc"
      },
      "outputs": [],
      "source": [
        "!curl -X POST http://localhost:8000/semantic-cache/answer \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\"system_prompt\": \"You are a helpful assistant.\", \"user_prompt\": \"What is the capital of France?\"}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YG2Vk6wWMpc"
      },
      "source": [
        "---\n",
        "\n",
        "## Run and Validate\n",
        "\n",
        "### Warm the Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm65jCM_WMpc"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"You are a concise support assistant for ACME Corp. Use internal policy v1 for refunds and returns.\"\n",
        "seed_prompts = [\n",
        "    \"What is our refund policy?\",\n",
        "    \"How long is the return window?\",\n",
        "    \"Do you offer exchanges?\",\n",
        "]\n",
        "\n",
        "print(\"Warming cache...\")\n",
        "for p in seed_prompts:\n",
        "    res = answer(SYSTEM_PROMPT, p)\n",
        "    print(f\"{res['source']} {res['latency_ms']:.1f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAO7JBq9WMpc"
      },
      "source": [
        "### Test Paraphrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utW0y3rYWMpc"
      },
      "outputs": [],
      "source": [
        "paraphrases = [\n",
        "    \"Can I get a refund? What's the policy?\",\n",
        "    \"What's the time limit to return an item?\",\n",
        "    \"Is it possible to swap a product for another?\",\n",
        "    \"How do refunds work here?\",\n",
        "    \"For how many days can I return stuff?\",\n",
        "]\n",
        "\n",
        "print(\"\\nTesting paraphrases...\")\n",
        "for p in paraphrases:\n",
        "    res = answer(SYSTEM_PROMPT, p)\n",
        "    print(f\"{p} => {res['source']} dist={res.get('distance')} {res['latency_ms']:.1f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAwHZKswWMpe"
      },
      "source": [
        "### Print Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kiksZa2WMpe"
      },
      "outputs": [],
      "source": [
        "print(\"\\nMetrics:\", metrics.snapshot())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7THL2fAWMpe"
      },
      "source": [
        "**Expected output:**\n",
        "- First run: all `llm` sources, ~500â€“1000ms latency\n",
        "- Paraphrases: mostly `cache` sources, <50ms latency, distance <0.10\n",
        "- Hit rate: 60â€“80% for paraphrases\n",
        "\n",
        "---\n",
        "\n",
        "## Tuning the Similarity Threshold\n",
        "\n",
        "The threshold controls cache precision. Lower = stricter (fewer false hits), higher = more lenient (more hits, risk of incorrect matches)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vIKfgcwWMpe"
      },
      "outputs": [],
      "source": [
        "def sweep_thresholds(thresholds):\n",
        "    for t in thresholds:\n",
        "        print(f\"\\nThreshold={t}\")\n",
        "        for p in paraphrases:\n",
        "            res = cache_get_or_generate(SYSTEM_PROMPT, p, ef_runtime=150, threshold=t)\n",
        "            print(f\"{p} => {res['source']} - {res['user_question']} - dist={res.get('distance')}\")\n",
        "\n",
        "sweep_thresholds([0.06, 0.08, 0.10, 0.12, 0.14])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VirnWb0GWMpe"
      },
      "source": [
        "Start with 0.10 and adjust based on false positive rate.\n",
        "\n",
        "---\n",
        "\n",
        "## Inspect the Cache\n",
        "\n",
        "**Count indexed documents:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F6CDIcIWMpe"
      },
      "outputs": [],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n",
        "num_docs = info[info.index(b'num_docs') + 1]\n",
        "print(f\"Cached documents: {num_docs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sac-OlOmWMpe"
      },
      "source": [
        "**Inspect a document:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq2Rlr_dWMpe"
      },
      "outputs": [],
      "source": [
        "keys = r.keys(f\"{NS}*\")\n",
        "if keys:\n",
        "    doc = r.hgetall(keys[0])\n",
        "    # Decode keys and string values, but leave the 'vector' value as bytes\n",
        "    decoded_doc = {k.decode(): v.decode() if isinstance(v, bytes) and k.decode() != \"vector\" else v for k, v in doc.items()}\n",
        "    # Print all items except the 'vector'\n",
        "    print({k: v for k, v in decoded_doc.items() if k != \"vector\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyIuTMcoWMpe"
      },
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "You've built a production-grade semantic cache with Redis Vector and FastAPI. The system normalizes queries, generates embeddings, performs fast vector search, and returns cached responses when similarity is highâ€”cutting latency by 10â€“20x and reducing LLM costs by 60â€“80% for repeated queries.\n",
        "\n",
        "**Key design decisions:**\n",
        "- **Canonicalization** stabilizes cache keys across paraphrases\n",
        "- **HNSW indexing** enables sub-50ms vector search at scale\n",
        "- **Metadata gating** ensures cache hits respect model, temperature, and system prompt changes\n",
        "- **TTL and namespace versioning** provide safe invalidation paths\n",
        "\n",
        "**Next steps:**\n",
        "- Add query-side metadata filters in `FT.SEARCH` to reduce false candidates (e.g., `@model:{gpt-4o-mini} @sys_hash:{<hash>}`)\n",
        "- Integrate Prometheus and Grafana for observability (track hit rate, p95 latency, cache size)\n",
        "- Implement LRU eviction or score-based pruning for long-running caches\n",
        "- Explore quantization (FLOAT16) to reduce memory footprint\n",
        "- Scale with Redis Cluster for multi-tenant or high-throughput workloads\n",
        "\n",
        "For more on building intelligent systems, see our guides on [building a RAG pipeline](/article/build-rag-pipeline) and [optimizing LLM context windows](/article/optimize-llm-context)."
      ]
    }
  ],
  "metadata": {
    "title": "Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs",
    "description": "Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}