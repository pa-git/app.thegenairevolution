{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** 7 Prompt Engineering Techniques to Write Effective AI Prompts\n\n**Description:** Master prompt engineering with actionable frameworks to craft effective, bias-aware prompts, reduce hallucinations, and iterate faster across multimodal tools confidently.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Structured output generation is one of the most common failure modes in production GenAI systems. You send a prompt expecting JSON with specific keys, and the model returns prose, malformed syntax, or extra commentary that breaks your parser. This guide shows you how to design prompts that reliably produce schema-compliant outputs across providers.\n\n## What Problem Are We Solving?\n\nModels trained on diverse text corpora default to fluent prose, not structured data. When you ask for JSON without explicit constraints, the model may wrap the output in markdown fences, add explanatory text, omit required keys, or invent extra fields. This breaks downstream validation, increases token costs, and requires brittle post-processing.\n\n## What's Actually Happening\n\nLanguage models predict the next token based on learned patterns. Without clear structural cues, they optimize for fluency over format. Three factors drive format drift:\n\n- **Instruction dilution.** Long prompts bury the format requirement in context, reducing its influence on generation.\n- **Schema ambiguity.** Vague instructions like \"return JSON\" don't specify keys, types, or nesting, leaving the model to guess.\n- **Output priors.** Training data contains more conversational responses than raw JSON, biasing the model toward prose wrappers.\n\n## How to Fix It\n\nUse explicit delimiters, inline schemas, and role framing to anchor the model's output structure. Here's a minimal pattern that works across providers.\n\n### Before: Vague Format Request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Extract company names from the text below.\n\nAcme Co. acquired Northwind. ACME has EU ops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "The companies mentioned are Acme Co. and Northwind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This fails validation because it's prose, not JSON.\n\n### After: Explicit Schema and Delimiters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Instructions. Extract companies as JSON key \"companies\" (array of strings). No extra text.\n\nDATA:\n\"\"\"\nAcme Co. acquired Northwind. ACME has EU ops.\n\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```json\n{\"companies\": [\"Acme Co.\", \"Northwind\", \"ACME\"]}\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This passes schema validation and is parser-ready.\n\n### Why This Works\n\n- **Delimiters** (triple quotes) separate instructions from user data, preventing the model from treating input text as part of the task description.\n- **Inline schema** (\"JSON key 'companies' (array of strings)\") specifies the exact structure, reducing ambiguity.\n- **Explicit constraint** (\"No extra text\") suppresses conversational wrappers and markdown fences.\n\n### Validation Harness\n\nTest your prompts programmatically to catch drift early. The following harness sends a prompt to OpenAI, validates the output against a JSON schema, and logs results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for schema validation and HTTP requests\n!pip install requests jsonschema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport json\nimport time\nimport logging\nimport requests\nfrom jsonschema import validate, ValidationError\nfrom google.colab import userdata\nfrom google.colab.userdata import SecretNotFoundError\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n\nkeys = [\"OPENAI_API_KEY\"]\nmissing = []\nfor k in keys:\n    value = None\n    try:\n        value = userdata.get(k)\n    except SecretNotFoundError:\n        pass\n    os.environ[k] = value if value is not None else \"\"\n    if not os.environ[k]:\n        missing.append(k)\nif missing:\n    raise EnvironmentError(f\"Missing keys: {', '.join(missing)}. Add them in Colab â†’ Settings â†’ Secrets.\")\nlogging.info(\"All keys loaded.\")\n\nSCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"companies\": {\n            \"type\": \"array\",\n            \"items\": {\"type\": \"string\"}\n        }\n    },\n    \"required\": [\"companies\"]\n}\n\ndef call_openai(prompt, model=\"gpt-4o-mini\", temperature=0.0, max_tokens=256):\n    url = \"https://api.openai.com/v1/chat/completions\"\n    headers = {\n        \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\",\n        \"Content-Type\": \"application/json\"\n    }\n    body = {\n        \"model\": model,\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n        \"temperature\": temperature,\n        \"max_tokens\": max_tokens\n    }\n    response = requests.post(url, json=body, headers=headers, timeout=30)\n    response.raise_for_status()\n    data = response.json()\n    try:\n        return data[\"choices\"][0][\"message\"][\"content\"]\n    except (KeyError, IndexError) as e:\n        logging.error(f\"Unexpected API response structure: {data}\")\n        raise\n\ndef validate_json_output(output, schema):\n    obj = json.loads(output)\n    validate(instance=obj, schema=schema)\n    return obj\n\ndef run_tests(test_prompts, schema):\n    for idx, prompt in enumerate(test_prompts):\n        logging.info(f\"Test {idx+1}: Sending prompt to model.\")\n        try:\n            output = call_openai(prompt)\n            logging.info(f\"Raw model output: {output}\")\n            obj = validate_json_output(output, schema)\n            print(\"PASS\", obj)\n        except (json.JSONDecodeError, ValidationError) as e:\n            print(\"FAIL\", output[:120] if 'output' in locals() else \"No output\", e)\n        except Exception as e:\n            print(\"ERROR\", str(e))\n        time.sleep(0.5)\n\ntest_prompts = [\n    '''Instructions. Extract companies as JSON key companies.\nDATA:\n\"\"\"\nAcme Co. acquired Northwind. ACME has EU ops.\n\"\"\"'''\n]\n\nrun_tests(test_prompts, SCHEMA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run this harness with 10â€“20 test cases covering edge cases (empty input, special characters, ambiguous entities). Track pass rate and adjust your prompt until you hit 95%+ compliance.\n\n## Key Takeaways\n\n- Use triple-quoted delimiters to separate instructions from user data and prevent instruction leakage.\n- Specify the exact JSON structure inline (key names, types, nesting) rather than relying on implicit understanding.\n- Add explicit constraints (\"No extra text\", \"Raw JSON only\") to suppress conversational wrappers.\n- Validate outputs programmatically with a schema and a test suite to catch drift before production.\n- Set temperature to 0.0 for deterministic structured outputs and increase max_tokens only if truncation occurs.\n\nThis pattern works for extraction, classification, and transformation tasks where schema compliance is non-negotiable. For tasks requiring creativity or multi-step reasoning, combine this approach with chain-of-thought or self-critique patterns covered in separate guides."
      ]
    }
  ],
  "metadata": {
    "title": "7 Prompt Engineering Techniques to Write Effective AI Prompts",
    "description": "Master prompt engineering with actionable frameworks to craft effective, bias-aware prompts, reduce hallucinations, and iterate faster across multimodal tools confidently.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}