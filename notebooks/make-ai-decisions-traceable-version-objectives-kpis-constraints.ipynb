{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Make AI Decisions Traceable: Version Objectives, KPIs, Constraints\n\n**Description:** Get a practical framework to version objectives, align constraints, and log KPIs, so every AI decision has a verifiable audit trail, faster audits, and lower compliance risk.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AI programs fail audits for one reason more than any model bug: nobody can prove, quickly and conclusively, why an AI system made a specific decision at a specific time. When a regulator asks for evidence, when a customer disputes an outcome, or when an internal review flags a fairness issue, the clock starts. If your team needs days or weeks to reconstruct what happened, you face legal exposure, operational delays, and erosion of stakeholder trust. This article gives you a practical framework to make AI decisions traceable by versioning objectives, KPIs, and constraints, and by capturing decision lineage. For a broader perspective on ethical and responsible AI practices, see our overview of [what responsible AI means for businesses today](/article/what-is-responsible-ai-and-why-it-matters-for-businesses-today-4). You will leave with concrete architecture choices, trade-offs, and a checklist you can take into your next steering meeting.\n\n## Why AI Traceability Is a Business Imperative, Not Just a Compliance Exercise\n\nAI traceability means you can answer, with evidence, why a specific decision was made at a specific time. It is not about logging every intermediate calculation. It is about capturing the artifacts and lineage that let you reconstruct intent, validate compliance, and respond to disputes or incidents.\n\n### What Traceability Delivers\n\nTraceability reduces audit cycle time from weeks to minutes, lowers legal and regulatory exposure, and accelerates incident response. When a decision is contested, you produce an evidence package that shows which model version ran, which objective and constraint policies were in effect, what the input was, and whether a human intervened. This capability translates into measurable business outcomes: fewer audit labor hours, faster dispute resolution, avoided regulatory penalties, and reduced downtime during incidents. Leaders should track metrics such as mean time to produce audit evidence, percentage of decisions with complete lineage, and dispute resolution cycle time.\n\n### Regulatory and Audit Expectations\n\nIn practice, audits are evidence exercises. If you operate in regulated contexts, you will face requirements tied to transparency, accountability, and contestability. If GDPR applies, you will also face expectations around meaningful information about logic and the ability to support data subject rights. Start with the GDPR portal, then align your internal controls. For practical steps on evaluating and validating AI systems for safety and compliance, refer to our guide on [how to test, validate, and monitor AI systems](/article/how-to-test-validate-and-monitor-ai-systems). Different jurisdictions and system types impose different obligations, so work with legal and compliance counsel to map applicable frameworks to your traceability architecture.\n\n### A Real Scenario: Threshold Update Fallout\n\nA credit decisioning system updates its fairness constraint threshold from 0.75 to 0.80. Approval rates shift across demographic groups. Three weeks later, a regulator asks why outcomes changed. Without traceability, your team scrambles through Git logs, Slack threads, and spreadsheets. With traceability, you produce a timestamped evidence package in minutes: constraint version v2.0.1, approved by the Chief Compliance Officer, deployed on a specific date, with before-and-after KPI slices by group. The regulator closes the inquiry. This is the difference traceability makes.\n\n## The Three Pillars of Traceable AI Decisions\n\nTraceability rests on three pillars: versioned objectives and constraints, KPI logging with slicing, and decision lineage capture. Each pillar addresses a specific question auditors and stakeholders will ask.\n\n### Pillar 1: Versioned Objectives and Constraints\n\nEvery AI system optimizes for something and operates under constraints. Objectives define what you are optimizing, such as minimizing false negatives in fraud detection. Constraints define boundaries, such as fairness thresholds or latency limits. If these artifacts are not versioned, you cannot prove which rules were in effect when a decision was made.\n\nTreat objectives and constraints as code. Store them in version control with metadata: version identifier, owner, approvers, changelog, timestamp, and risk rating. When you update a constraint, you create a new version. When you deploy a model, you tag it with the objective and constraint versions it was trained and validated against.\n\nFor classic ML systems, this means versioning model training objectives, fairness policies, and operational thresholds. For GenAI systems, this means versioning prompt templates, retrieval policies for RAG, guardrail rules, and human review criteria. A customer support summarization agent, for example, should log the prompt template version, the retrieval index snapshot, the policy version governing what sources are allowed, and whether a human reviewer approved the output.\n\n### Pillar 2: KPI Logging with Slicing\n\nAggregate metrics hide problems. A model with 95% overall accuracy may have 70% accuracy for a protected group. KPI logging with slicing means you log performance metrics, broken down by relevant dimensions such as geography, demographic attributes, or product line.\n\nLog KPIs at decision time or in batch, depending on your architecture. Each KPI record should include the metric name, value, slice dimensions, model version, objective version, constraint version, and timestamp. This structure lets you answer questions like \"What was the approval rate for EU applicants under constraint v2.0.1?\" without re-running experiments.\n\nFor GenAI systems, log KPIs such as retrieval precision, guardrail trigger rates, human override frequency, and user satisfaction scores, sliced by use case, user segment, or content type.\n\n### Pillar 3: Decision Lineage Capture\n\nDecision lineage is the full record of how a single decision was made. It includes a unique decision ID, a reference to the input data, the feature set version, the model version, the objective and constraint versions in effect, the output, and any human override details.\n\nInput data references should be hashed or tokenized if the data is sensitive. You do not need to store raw PII in the lineage record. You need enough information to retrieve the decision context if authorized and necessary.\n\nHuman overrides are critical. If a reviewer changes a model recommendation, log who made the change, the reason code, and the policy version that authorized the override. This record protects both the human and the organization during audits.\n\nFor GenAI systems, decision lineage includes the prompt sent to the model, the retrieved context (or a reference to the retrieval snapshot), the model response, any guardrail interventions, and human review outcomes.\n\n## Key Architecture Decisions and Their Trade-Offs\n\nImplementing traceability requires architectural choices. The following decisions shape cost, complexity, and audit readiness.\n\n### Decision 1: Granularity of Versioning\n\nYou must decide what to version and at what granularity. Versioning every hyperparameter is expensive and noisy. Versioning only the final model binary is too coarse.\n\nA practical middle ground is to version artifacts that change business behavior: objectives, constraints, model binaries, feature sets, and prompt templates for GenAI. Use semantic versioning and tag releases with approval metadata. Store version manifests in a central registry accessible to audit and operations teams.\n\n### Decision 2: Depth of Logging\n\nYou must decide how much to log per decision. Logging every intermediate feature value and model layer activation is expensive and rarely necessary. Logging only the final output is insufficient for root cause analysis.\n\nA practical approach is to log the decision ID, input reference, model version, objective and constraint versions, output, and human override details. Log additional diagnostic data only for decisions flagged for review or sampled for quality assurance. Use sampling strategies to balance cost and coverage.\n\n### Decision 3: Scope of Lineage Capture\n\nYou must decide whether to capture lineage for every decision or only for high-stakes decisions. Capturing lineage for every decision provides complete audit coverage but increases storage and processing costs. Capturing lineage only for high-stakes decisions reduces costs but creates gaps.\n\nA practical approach is to capture full lineage for all decisions in regulated or high-risk domains, and to sample lineage for lower-risk decisions. Define \"high-stakes\" based on impact, regulatory scope, and dispute likelihood. Document the sampling strategy and ensure it is auditable.\n\n### Secondary Considerations\n\n**Privacy and sensitive data**: Minimize what you log. Hash or tokenize PII. Store lineage records with role-based access controls. Define retention policies aligned with legal and regulatory requirements. Work with legal and privacy teams to answer: What are we allowed to store? For how long? Who can access raw lineage? How do we handle subject access requests and deletion while maintaining audit integrity?\n\n**Performance and cost**: Logging adds latency and storage costs. Use asynchronous logging to minimize decision latency. Compress and archive old lineage records. Use tiered storage to balance access speed and cost.\n\n**Audit store vs. analytics store**: Audit stores prioritize immutability, retention, and access controls. Analytics stores prioritize query performance and aggregation. In many cases, you will need both. Write lineage to an immutable audit store and replicate aggregated KPIs to an analytics store for dashboards and reporting.\n\n## Operationalizing Traceability: Tools and Platforms\n\nOperationalizing traceability requires tooling for versioning, logging, and lineage capture. The following capabilities are essential:\n\n**Artifact versioning and governance**: Platforms like Collibra (https://www.collibra.com/) and Alation (https://www.alation.com/) provide data governance and artifact management. If you need lightweight versioning, use Git with structured metadata files and a central registry.\n\n**KPI logging and observability**: Operationalizing KPI logging usually requires ML observability and analytics tooling. Options include Arize AI for model monitoring (https://arize.com/) and WhyLabs for monitoring and data drift (https://whylabs.ai/). If you are heavily on AWS, review Amazon SageMaker Model Monitor (https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html). For Azure, review Azure AI Foundry and monitoring capabilities (https://learn.microsoft.com/azure/ai-foundry/). For a comprehensive approach to deploying, monitoring, and scaling models in production environments, see our article on [MLOps best practices](/article/mlops-how-to-deploy-monitor-and-scale-models-in-production-2).\n\n**Decision lineage and audit logging**: Platforms like ServiceNow (https://www.servicenow.com/) and Splunk (https://www.splunk.com/) provide audit logging and incident management. For immutable audit trails, consider blockchain-based or append-only storage solutions.\n\n**Selection rubric**: Choose tools based on your risk profile and regulatory scope. Regulated or high-stakes systems require immutability, role-based access, long retention, and exportable evidence packages. Lower-stakes systems can use existing logging and BI infrastructure. Evaluate buy vs. build based on team capacity, compliance requirements, and integration complexity. Assign clear ownership: ML Platform owns the logging infrastructure, Product owns objective and constraint definitions, Risk and Compliance own audit readiness, and Legal owns privacy and retention policies.\n\n## A Practical Checklist for AI Leaders\n\nUse this checklist to assess and improve traceability in your AI systems. Group the steps into three categories: decision framework, questions to ask your team, and post-launch metrics.\n\n### Decision Framework\n\n**1. Identify high-stakes decisions**: List all AI decisions in production. Flag those subject to regulation, dispute, or high business impact. Prioritize traceability for these decisions first.\n\n**2. Define your evidence package**: For each high-stakes decision type, define what an evidence package must include. At minimum: decision ID, input reference, model version, objective version, constraint version, output, and human override details. Document the package format and storage location.\n\n**3. Establish versioning discipline**: Require that every objective, constraint, model, feature set, and prompt template be versioned before deployment. Use semantic versioning. Tag each version with owner, approvers, changelog, and risk rating.\n\n**4. Implement KPI slicing**: Identify the dimensions by which you must slice KPIs, such as geography, demographic group, product line, or use case. Ensure your logging infrastructure captures these dimensions at decision time.\n\n**5. Capture decision lineage**: Instrument your inference pipeline to log decision lineage for every high-stakes decision. Use asynchronous logging to minimize latency. Store lineage in an immutable audit store with role-based access controls.\n\n### Questions to Ask Your Team\n\n**6. Can we produce an evidence package in under 10 minutes?**: Run a drill. Pick a decision ID from last month. Ask your team to produce the full evidence package. If it takes longer than 10 minutes, identify the bottleneck and fix it.\n\n**7. Do we have clear ownership for objectives and constraints?**: For each objective and constraint, identify the owner, the approvers, and the escalation path for changes. Document this in a RACI matrix: who drafts, who approves, who deploys, who can override, who audits, and who is on point during an incident.\n\n**8. Are we logging enough to answer \"why did this happen?\"**: Review a sample of recent decisions. For each, ask: Can we explain why the model produced this output? Can we identify which constraint was violated, if any? Can we show whether a human intervened? If the answer is no, increase logging depth.\n\n**9. How do we handle GenAI-specific traceability?**: For GenAI systems, ensure you are versioning prompt templates, retrieval policies, guardrail rules, and human review criteria. Log the prompt, retrieved context, model response, guardrail interventions, and human review outcomes for each decision.\n\n**10. What are our data privacy and retention policies?**: Work with Legal and Privacy to define what you are allowed to log, for how long, and who can access it. Document how you handle subject access requests and deletion while maintaining audit integrity.\n\n### Post-Launch Metrics\n\n**11. Track traceability completeness**: Measure the percentage of decisions with complete lineage. Set a target, such as 99% for high-stakes decisions. Alert when completeness drops below the target.\n\n**12. Measure audit response time**: Track mean time to produce audit evidence. Set a target, such as under 10 minutes. Report this metric to your steering committee quarterly. Use it to justify investment in traceability infrastructure and to demonstrate ROI in reduced audit labor and faster dispute resolution.\n\n## Adoption and Change Management\n\nTraceability requires behavior change across teams. Update your SDLC to include release gates that check for versioned objectives and constraints. Require steering committee review for high-risk changes. Run quarterly audit drills to practice evidence retrieval. Tie team incentives and SLOs to traceability completeness. Define \"stop-the-line\" criteria: if traceability completeness drops below your target, pause deployments until the issue is resolved.\n\n## Example: Traceable AI Decision Logging Framework\n\nThe following code demonstrates how to implement traceable AI decision logging in Python. It covers versioning of objectives and constraints, KPI logging with slicing, and decision lineage capture. The code uses secure Colab secrets loading and avoids hardcoding sensitive information.\n\nThis block securely loads required API keys from Colab secrets for downstream integrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Securely load required API keys from Colab secrets for downstream integrations (if needed)\nimport os\nfrom google.colab import userdata\nfrom google.colab.userdata import SecretNotFoundError\n\n# List of required API keys (add more as needed for your integrations)\nkeys = [\"OPENAI_API_KEY\", \"ANTHROPIC_API_KEY\"]\nmissing = []\nfor k in keys:\n    value = None\n    try:\n        value = userdata.get(k)\n    except SecretNotFoundError:\n        pass\n\n    os.environ[k] = value if value is not None else \"\"\n\n    if not os.environ[k]:\n        missing.append(k)\n\nif missing:\n    raise EnvironmentError(f\"Missing keys: {', '.join(missing)}. Add them in Colab â†’ Settings â†’ Secrets.\")\n\nprint(\"All keys loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This block defines data classes for versioned artifacts, KPI records, and decision lineage, then simulates a traceable decision workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Traceable AI Decision Logging Framework\n\nimport uuid\nimport datetime\nfrom typing import Dict, Any, List, Optional\n\n# Lightweight logging for runtime behavior\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n# --- Data Classes for Versioned Artifacts ---\n\nclass VersionedArtifact:\n    \"\"\"\n    Base class for versioned artifacts (objectives, constraints, models, etc.).\n    Args:\n        name (str): Name of the artifact.\n        version (str): Version identifier (e.g., 'v1.0.0').\n        owner (str): Responsible party for the artifact.\n        changelog (str): Description of changes in this version.\n        timestamp (datetime): When this version was created.\n        approvers (List[str]): List of approvers for this version.\n        risk_rating (str): Risk rating (e.g., 'low', 'medium', 'high').\n    \"\"\"\n    def __init__(self, name: str, version: str, owner: str, changelog: str,\n                 approvers: List[str], risk_rating: str):\n        self.name = name\n        self.version = version\n        self.owner = owner\n        self.changelog = changelog\n        self.timestamp = datetime.datetime.utcnow()\n        self.approvers = approvers\n        self.risk_rating = risk_rating\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Serialize artifact to dictionary for logging or storage.\"\"\"\n        return {\n            \"name\": self.name,\n            \"version\": self.version,\n            \"owner\": self.owner,\n            \"changelog\": self.changelog,\n            \"timestamp\": self.timestamp.isoformat(),\n            \"approvers\": self.approvers,\n            \"risk_rating\": self.risk_rating\n        }\n\nclass Objective(VersionedArtifact):\n    \"\"\"\n    Represents a business objective for an AI system.\n    Args:\n        optimization_goal (str): What the system is optimizing (e.g., 'minimize false negatives').\n    \"\"\"\n    def __init__(self, name: str, version: str, owner: str, changelog: str,\n                 approvers: List[str], risk_rating: str, optimization_goal: str):\n        super().__init__(name, version, owner, changelog, approvers, risk_rating)\n        self.optimization_goal = optimization_goal\n\n    def to_dict(self) -> Dict[str, Any]:\n        d = super().to_dict()\n        d[\"optimization_goal\"] = self.optimization_goal\n        return d\n\nclass Constraint(VersionedArtifact):\n    \"\"\"\n    Represents a constraint policy for an AI system.\n    Args:\n        constraint_type (str): Type of constraint (e.g., 'fairness', 'latency').\n        threshold (Any): The threshold value for the constraint.\n    \"\"\"\n    def __init__(self, name: str, version: str, owner: str, changelog: str,\n                 approvers: List[str], risk_rating: str, constraint_type: str, threshold: Any):\n        super().__init__(name, version, owner, changelog, approvers, risk_rating)\n        self.constraint_type = constraint_type\n        self.threshold = threshold\n\n    def to_dict(self) -> Dict[str, Any]:\n        d = super().to_dict()\n        d[\"constraint_type\"] = self.constraint_type\n        d[\"threshold\"] = self.threshold\n        return d\n\n# --- KPI Logging ---\n\nclass KPIRecord:\n    \"\"\"\n    Represents a logged KPI for a specific model/objective/constraint version.\n    Args:\n        kpi_name (str): Name of the KPI (e.g., 'accuracy').\n        value (float): Value of the KPI.\n        slice_by (Optional[Dict[str, Any]]): Slicing information (e.g., {'region': 'EU'}).\n        model_version (str): Model version.\n        objective_version (str): Objective version.\n        constraint_version (str): Constraint version.\n        timestamp (datetime): When the KPI was logged.\n    \"\"\"\n    def __init__(self, kpi_name: str, value: float, slice_by: Optional[Dict[str, Any]],\n                 model_version: str, objective_version: str, constraint_version: str):\n        self.kpi_name = kpi_name\n        self.value = value\n        self.slice_by = slice_by or {}\n        self.model_version = model_version\n        self.objective_version = objective_version\n        self.constraint_version = constraint_version\n        self.timestamp = datetime.datetime.utcnow()\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"kpi_name\": self.kpi_name,\n            \"value\": self.value,\n            \"slice_by\": self.slice_by,\n            \"model_version\": self.model_version,\n            \"objective_version\": self.objective_version,\n            \"constraint_version\": self.constraint_version,\n            \"timestamp\": self.timestamp.isoformat()\n        }\n\n# --- Decision Lineage Logging ---\n\nclass DecisionLineage:\n    \"\"\"\n    Captures the full lineage for a single AI decision.\n    Args:\n        decision_id (str): Unique identifier for the decision.\n        input_data_ref (str): Reference to input data (hashed or tokenized if sensitive).\n        feature_set_version (str): Version of the feature set used.\n        model_version (str): Model version used.\n        objective_version (str): Objective version in effect.\n        constraint_version (str): Constraint version in effect.\n        output (Any): Model output (e.g., score, class).\n        human_override (Optional[Dict[str, Any]]): If a human overrode the decision, details.\n        timestamp (datetime): When the decision was made.\n    \"\"\"\n    def __init__(self, input_data_ref: str, feature_set_version: str, model_version: str,\n                 objective_version: str, constraint_version: str, output: Any,\n                 human_override: Optional[Dict[str, Any]] = None):\n        self.decision_id = str(uuid.uuid4())\n        self.input_data_ref = input_data_ref\n        self.feature_set_version = feature_set_version\n        self.model_version = model_version\n        self.objective_version = objective_version\n        self.constraint_version = constraint_version\n        self.output = output\n        self.human_override = human_override\n        self.timestamp = datetime.datetime.utcnow()\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"decision_id\": self.decision_id,\n            \"input_data_ref\": self.input_data_ref,\n            \"feature_set_version\": self.feature_set_version,\n            \"model_version\": self.model_version,\n            \"objective_version\": self.objective_version,\n            \"constraint_version\": self.constraint_version,\n            \"output\": self.output,\n            \"human_override\": self.human_override,\n            \"timestamp\": self.timestamp.isoformat()\n        }\n\n# --- Example Usage: Simulate a Traceable Decision ---\n\ndef simulate_decision(input_data: Dict[str, Any],\n                      feature_set_version: str,\n                      model_version: str,\n                      objective: Objective,\n                      constraint: Constraint,\n                      kpi_logger: List[KPIRecord],\n                      human_override: Optional[Dict[str, Any]] = None) -> DecisionLineage:\n    \"\"\"\n    Simulate a model decision and log its full lineage.\n    Args:\n        input_data (Dict[str, Any]): Input features for the decision.\n        feature_set_version (str): Version of the feature set.\n        model_version (str): Model version.\n        objective (Objective): Objective in effect.\n        constraint (Constraint): Constraint in effect.\n        kpi_logger (List[KPIRecord]): List to append KPI logs.\n        human_override (Optional[Dict[str, Any]]): Human override details, if any.\n    Returns:\n        DecisionLineage: The full lineage record for the decision.\n    \"\"\"\n    # Purpose: Simulate a model output (e.g., credit approval score)\n    # NOTE: Replace this with your actual model inference logic\n    score = sum(input_data.values()) % 100 / 100  # Dummy score between 0 and 1\n\n    # Log a KPI (e.g., utility metric) for this decision slice\n    kpi = KPIRecord(\n        kpi_name=\"approval_score\",\n        value=score,\n        slice_by={\"region\": input_data.get(\"region\", \"unknown\")},\n        model_version=model_version,\n        objective_version=objective.version,\n        constraint_version=constraint.version\n    )\n    kpi_logger.append(kpi)\n    logging.info(f\"KPI logged: {kpi.to_dict()}\")\n\n    # Hash or tokenize input reference for privacy (here, just a placeholder)\n    input_data_ref = f\"hash_{hash(str(input_data))}\"\n\n    # Create the decision lineage record\n    lineage = DecisionLineage(\n        input_data_ref=input_data_ref,\n        feature_set_version=feature_set_version,\n        model_version=model_version,\n        objective_version=objective.version,\n        constraint_version=constraint.version,\n        output={\"score\": score, \"approved\": score > constraint.threshold},\n        human_override=human_override\n    )\n    logging.info(f\"Decision lineage captured: {lineage.to_dict()}\")\n    return lineage\n\n# --- Example: Putting It All Together ---\n\n# Purpose: Demonstrate a full traceable decision workflow\n\n# Define versioned objective and constraint\nobjective = Objective(\n    name=\"Credit Default Minimization\",\n    version=\"v1.2.0\",\n    owner=\"Product Lead\",\n    changelog=\"Updated optimization to minimize false negatives.\",\n    approvers=[\"Head of Product\", \"Risk Officer\"],\n    risk_rating=\"high\",\n    optimization_goal=\"minimize false negatives\"\n)\n\nconstraint = Constraint(\n    name=\"Fairness Constraint\",\n    version=\"v2.0.1\",\n    owner=\"Compliance Lead\",\n    changelog=\"Lowered disparate impact threshold to 0.8.\",\n    approvers=[\"Chief Compliance Officer\"],\n    risk_rating=\"high\",\n    constraint_type=\"fairness\",\n    threshold=0.8  # Example: minimum approval rate ratio for protected group\n)\n\n# Simulate a decision event\nkpi_logger = []\ninput_data = {\"income\": 50000, \"age\": 35, \"region\": \"EU\"}\nfeature_set_version = \"fs_v3.1\"\nmodel_version = \"model_v5.0\"\n\n# Simulate a human override (optional)\nhuman_override = {\n    \"overridden_by\": \"Senior Reviewer\",\n    \"reason_code\": \"manual_appeal_approved\",\n    \"policy_version\": constraint.version\n}\n\n# Run the simulation\ndecision_lineage = simulate_decision(\n    input_data=input_data,\n    feature_set_version=feature_set_version,\n    model_version=model_version,\n    objective=objective,\n    constraint=constraint,\n    kpi_logger=kpi_logger,\n    human_override=human_override\n)\n\n# Output the full evidence package for audit/retrieval\nevidence_package = {\n    \"objective\": objective.to_dict(),\n    \"constraint\": constraint.to_dict(),\n    \"kpis\": [k.to_dict() for k in kpi_logger],\n    \"decision_lineage\": decision_lineage.to_dict()\n}\n\n# Print the evidence package (in real systems, store in immutable audit store)\nimport pprint\npprint.pprint(evidence_package)"
      ]
    }
  ],
  "metadata": {
    "title": "Make AI Decisions Traceable: Version Objectives, KPIs, Constraints",
    "description": "Get a practical framework to version objectives, align constraints, and log KPIs, so every AI decision has a verifiable audit trail, faster audits, and lower compliance risk.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}