{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Mastering Domain-Specific LLM Customization: Techniques and Tools Unveiled\n\n**Description:** Discover how to tailor Large Language Models for specific domains using Retrieval-Augmented Generation, fine-tuning, and prompt engineering to boost relevance and accuracy.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n\nIn the rapidly evolving field of Generative AI, deploying and maintaining scalable, secure, and production-ready solutions is crucial for AI builders. This tutorial will guide you through the process of taking a prototype to production, focusing on deploying, optimizing, and maintaining AI applications using frameworks like LangChain, Hugging Face, and ChromaDB. We will cover the entire journey, from setting up the environment to implementing a human-in-the-loop feedback system for continuous improvement.\n\n# Setup & Installation\n\nTo begin, we need to install the necessary libraries. Ensure you are using a virtual environment to avoid dependency conflicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install core libraries for LLM customization\n# transformers: Hugging Face library for pre-trained models and fine-tuning\n# langchain: Framework for building LLM-powered applications with RAG capabilities\n# chromadb: Vector database for efficient similarity search and retrieval\n\n!pip install transformers>=4.30.0\n!pip install langchain>=0.1.0\n!pip install chromadb>=0.4.0\n\n# Note: Consider using a virtual environment to avoid dependency conflicts\n# Run: python -m venv llm_env && source llm_env/bin/activate (Linux/Mac)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step-by-Step Walkthrough\n\n## Data Collection and Preparation\n\nFirst, we need to load and preprocess our domain-specific data. This step ensures that our model is trained on clean and relevant data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Purpose: Load and preprocess domain-specific data for LLM training\nimport pandas as pd\nimport logging\n\n# Configure logging for tracking data processing steps\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef preprocess_domain_data(file_path, text_column='text'):\n    \"\"\"\n    Load and preprocess domain-specific dataset for LLM training.\n    \n    Args:\n        file_path (str): Path to the CSV file containing domain data\n        text_column (str): Name of the column containing text data\n    \n    Returns:\n        pd.DataFrame: Preprocessed dataframe with cleaned text\n    \n    Raises:\n        FileNotFoundError: If the specified file doesn't exist\n        KeyError: If the text column is not found in the dataset\n    \"\"\"\n    try:\n        # Load the dataset from CSV file\n        logger.info(f\"Loading dataset from {file_path}\")\n        data = pd.read_csv(file_path)\n        \n        # Validate that the text column exists\n        if text_column not in data.columns:\n            raise KeyError(f\"Column '{text_column}' not found in dataset\")\n        \n        # Remove rows with missing text values to ensure data quality\n        initial_rows = len(data)\n        data = data.dropna(subset=[text_column])\n        logger.info(f\"Removed {initial_rows - len(data)} rows with missing text\")\n        \n        # Normalize text: convert to lowercase and remove leading/trailing whitespace\n        data[text_column] = data[text_column].apply(lambda x: x.lower().strip())\n        \n        # Remove duplicate entries to prevent model overfitting on repeated examples\n        data = data.drop_duplicates(subset=[text_column])\n        logger.info(f\"Final dataset size: {len(data)} rows\")\n        \n        return data\n    \n    except FileNotFoundError:\n        logger.error(f\"File not found: {file_path}\")\n        raise\n    except Exception as e:\n        logger.error(f\"Error during preprocessing: {str(e)}\")\n        raise\n\n# Example usage\ndata = preprocess_domain_data('domain_specific_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training and Fine-Tuning\n\nWith our data prepared, we can now fine-tune a pre-trained model using Hugging Face Transformers. This section will guide you through the setup and execution of the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Purpose: Fine-tune a pre-trained model on domain-specific data\nfrom transformers import (\n    Trainer, \n    TrainingArguments, \n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    DataCollatorWithPadding\n)\nfrom datasets import Dataset\nimport torch\n\ndef prepare_and_train_model(train_data, eval_data, num_labels=2, model_name='bert-base-uncased'):\n    \"\"\"\n    Prepare datasets, tokenize, and fine-tune a pre-trained model.\n    \n    Args:\n        train_data (pd.DataFrame): Training dataset with 'text' and 'label' columns\n        eval_data (pd.DataFrame): Evaluation dataset with 'text' and 'label' columns\n        num_labels (int): Number of classification labels\n        model_name (str): Hugging Face model identifier\n    \n    Returns:\n        Trainer: Trained model trainer object\n    \n    Raises:\n        ValueError: If datasets are empty or missing required columns\n    \"\"\"\n    # Validate input data\n    if train_data.empty or eval_data.empty:\n        raise ValueError(\"Training or evaluation data is empty\")\n    \n    # Load tokenizer for the specified pre-trained model\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    def tokenize_function(examples):\n        \"\"\"\n        Tokenize text examples with padding and truncation.\n        \n        Args:\n            examples (dict): Batch of text examples\n        \n        Returns:\n            dict: Tokenized examples with input_ids, attention_mask, etc.\n        \"\"\"\n        return tokenizer(examples['text'], truncation=True, max_length=512)\n    \n    # Convert pandas DataFrames to Hugging Face Dataset format\n    train_dataset = Dataset.from_pandas(train_data[['text', 'label']])\n    eval_dataset = Dataset.from_pandas(eval_data[['text', 'label']])\n    \n    # Apply tokenization to all examples in the datasets\n    train_dataset = train_dataset.map(tokenize_function, batched=True)\n    eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n    \n    # Load pre-trained model with classification head for domain-specific task\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name, \n        num_labels=num_labels\n    )\n    \n    # Data collator handles dynamic padding to the longest sequence in each batch\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    # Define training hyperparameters\n    training_args = TrainingArguments(\n        output_dir='./results',              # Directory to save model checkpoints\n        num_train_epochs=3,                  # Number of complete passes through training data\n        per_device_train_batch_size=16,      # Batch size per GPU/CPU (adjust based on memory)\n        per_device_eval_batch_size=32,       # Larger batch for evaluation (no gradients needed)\n        learning_rate=2e-5,                  # Learning rate (2e-5 is standard for BERT fine-tuning)\n        weight_decay=0.01,                   # L2 regularization to prevent overfitting\n        evaluation_strategy=\"epoch\",         # Evaluate after each epoch\n        save_strategy=\"epoch\",               # Save checkpoint after each epoch\n        load_best_model_at_end=True,         # Load best model based on evaluation metric\n        metric_for_best_model=\"f1\",          # Use F1 score to determine best model\n        logging_dir='./logs',                # Directory for training logs\n        logging_steps=100,                   # Log metrics every 100 steps\n        warmup_steps=500,                    # Gradual learning rate warmup for stability\n        fp16=torch.cuda.is_available(),      # Use mixed precision training if GPU available\n    )\n    \n    # Initialize Trainer with model, datasets, and training configuration\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,     # Function to compute evaluation metrics\n    )\n    \n    # Start training process\n    trainer.train()\n    \n    return trainer\n\n# Example usage (assuming train_data and eval_data are prepared DataFrames)\n# trainer = prepare_and_train_model(train_data, eval_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a comprehensive guide on fine-tuning these models, see our article on [mastering fine-tuning of large language models with Hugging Face](/blog/44830763/mastering-fine-tuning-of-large-language-models-with-hugging-face).\n\n## Evaluation and Optimization\n\nAfter training, it's essential to evaluate the model's performance and optimize it for deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Purpose: Compute evaluation metrics for model performance assessment\nfrom sklearn.metrics import (\n    accuracy_score, \n    precision_recall_fscore_support,\n    confusion_matrix,\n    classification_report\n)\nimport numpy as np\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef compute_metrics(pred):\n    \"\"\"\n    Compute comprehensive evaluation metrics for model predictions.\n    \n    This function calculates accuracy, precision, recall, and F1 score\n    to assess model performance on domain-specific tasks.\n    \n    Args:\n        pred (EvalPrediction): Object containing predictions and labels\n            - predictions: Model output logits (shape: [batch_size, num_labels])\n            - label_ids: Ground truth labels (shape: [batch_size])\n    \n    Returns:\n        dict: Dictionary containing computed metrics:\n            - accuracy: Overall classification accuracy\n            - f1: F1 score (harmonic mean of precision and recall)\n            - precision: Ratio of true positives to predicted positives\n            - recall: Ratio of true positives to actual positives\n    \n    Raises:\n        ValueError: If predictions and labels have mismatched shapes\n    \"\"\"\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    \n    if len(preds) != len(labels):\n        raise ValueError(f\"Predictions ({len(preds)}) and labels ({len(labels)}) length mismatch\")\n    \n    num_classes = len(np.unique(labels))\n    average_method = 'binary' if num_classes == 2 else 'weighted'\n    \n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, \n        preds, \n        average=average_method,\n        zero_division=0\n    )\n    \n    acc = accuracy_score(labels, preds)\n    \n    logger.info(\"\\nClassification Report:\")\n    logger.info(classification_report(labels, preds))\n    \n    return {\n        'accuracy': float(acc),\n        'f1': float(f1),\n        'precision': float(precision),\n        'recall': float(recall)\n    }\n\ndef evaluate_model_performance(trainer, test_dataset):\n    \"\"\"\n    Perform comprehensive model evaluation on test dataset.\n    \n    Args:\n        trainer (Trainer): Trained model trainer object\n        test_dataset (Dataset): Test dataset for evaluation\n    \n    Returns:\n        dict: Evaluation metrics and confusion matrix\n    \"\"\"\n    eval_results = trainer.evaluate(test_dataset)\n    \n    predictions = trainer.predict(test_dataset)\n    preds = predictions.predictions.argmax(-1)\n    labels = predictions.label_ids\n    \n    cm = confusion_matrix(labels, preds)\n    \n    logger.info(f\"\\nConfusion Matrix:\\n{cm}\")\n    logger.info(f\"Evaluation Results: {eval_results}\")\n    \n    return {\n        'metrics': eval_results,\n        'confusion_matrix': cm\n    }\n\n# Example usage\n# results = evaluate_model_performance(trainer, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Incorporating Human-in-the-Loop Feedback\n\nTo ensure continuous improvement, we implement a human-in-the-loop feedback system. This system allows for expert corrections and active learning strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Purpose: Implement human-in-the-loop feedback system for continuous model improvement\nimport json\nfrom datetime import datetime\nfrom typing import List, Dict, Any\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass FeedbackLoop:\n    \"\"\"\n    Human-in-the-loop feedback system for iterative model refinement.\n    \n    This class manages the collection of expert feedback, stores corrections,\n    and facilitates model retraining with improved data.\n    \"\"\"\n    \n    def __init__(self, feedback_file='feedback_log.json'):\n        \"\"\"\n        Initialize feedback loop system.\n        \n        Args:\n            feedback_file (str): Path to JSON file for storing feedback history\n        \"\"\"\n        self.feedback_file = feedback_file\n        self.feedback_history = []\n        \n    def collect_feedback(self, model_input: str, model_output: str, \n                        expert_correction: str, confidence_score: float) -> Dict[str, Any]:\n        \"\"\"\n        Collect and store expert feedback on model predictions.\n        \n        Args:\n            model_input (str): Original input text to the model\n            model_output (str): Model's predicted output\n            expert_correction (str): Expert's corrected output\n            confidence_score (float): Model's confidence in its prediction (0-1)\n        \n        Returns:\n            dict: Feedback entry with metadata\n        \"\"\"\n        feedback_entry = {\n            'timestamp': datetime.now().isoformat(),\n            'input': model_input,\n            'model_output': model_output,\n            'expert_correction': expert_correction,\n            'confidence_score': confidence_score,\n            'requires_retraining': model_output != expert_correction\n        }\n        \n        self.feedback_history.append(feedback_entry)\n        self._save_feedback()\n        \n        logger.info(f\"Feedback collected: {'Correction needed' if feedback_entry['requires_retraining'] else 'Confirmed correct'}\")\n        \n        return feedback_entry\n    \n    def _save_feedback(self):\n        \"\"\"Save feedback history to JSON file for persistence.\"\"\"\n        try:\n            with open(self.feedback_file, 'w') as f:\n                json.dump(self.feedback_history, f, indent=2)\n        except IOError as e:\n            logger.error(f\"Failed to save feedback: {str(e)}\")\n    \n    def get_correction_dataset(self, min_confidence_threshold: float = 0.7) -> List[Dict[str, str]]:\n        \"\"\"\n        Extract corrections for model retraining, prioritizing low-confidence errors.\n        \n        Args:\n            min_confidence_threshold (float): Only include corrections where model\n                                             confidence was above this threshold\n                                             (indicates systematic errors)\n        \n        Returns:\n            list: Training examples with corrected labels\n        \"\"\"\n        corrections = [\n            {\n                'text': entry['input'],\n                'label': entry['expert_correction'],\n                'original_prediction': entry['model_output'],\n                'confidence': entry['confidence_score']\n            }\n            for entry in self.feedback_history\n            if entry['requires_retraining'] and entry['confidence_score'] >= min_confidence_threshold\n        ]\n        \n        logger.info(f\"Extracted {len(corrections)} high-confidence corrections for retraining\")\n        \n        return corrections\n    \n    def active_learning_selection(self, predictions: List[Dict[str, Any]], \n                                  uncertainty_threshold: float = 0.5,\n                                  sample_size: int = 100) -> List[Dict[str, Any]]:\n        \"\"\"\n        Select uncertain predictions for expert review using active learning.\n        \n        This method identifies predictions where the model is least confident,\n        prioritizing them for human review to maximize learning efficiency.\n        \n        Args:\n            predictions (list): List of model predictions with confidence scores\n            uncertainty_threshold (float): Confidence threshold below which to flag\n            sample_size (int): Maximum number of samples to select for review\n        \n        Returns:\n            list: Selected samples for expert review\n        \"\"\"\n        uncertain_samples = [\n            pred for pred in predictions\n            if pred['confidence'] < uncertainty_threshold\n        ]\n        \n        logger.info(f\"Selected {min(len(uncertain_samples), sample_size)} samples for active learning review\")\n        \n        return uncertain_samples[:sample_size]\n\n# Example usage\n# feedback_loop = FeedbackLoop()\n# feedback_entry = feedback_loop.collect_feedback(\"input text\", \"model output\", \"corrected output\", 0.6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n\nIn this tutorial, we have walked through the process of deploying, optimizing, and maintaining a GenAI application. From setting up the environment and preparing data to fine-tuning models and incorporating human feedback, each step is crucial for building robust AI solutions. By following these guidelines, AI builders can ensure their applications are not only effective but also continuously improving. Consider exploring further extensions such as integrating additional data sources or deploying models using cloud platforms for scalability."
      ]
    }
  ],
  "metadata": {
    "title": "Mastering Domain-Specific LLM Customization: Techniques and Tools Unveiled",
    "description": "Discover how to tailor Large Language Models for specific domains using Retrieval-Augmented Generation, fine-tuning, and prompt engineering to boost relevance and accuracy.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}