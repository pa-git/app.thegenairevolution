{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build Explainability AI by Design with OPA, MCP, and Neo4j\n\n**Description:** Build production-ready control layers for agent systems using OPA, MCP, and Neo4j. Enforce policies, capture auditable traces, and insert human approvals without slowing teams or shipping velocity.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You want an agent that is smart, not reckless. You also want a security boundary that the agent cannot talk around. This project shows you how to combine CrewAI, an MCP server, OPA, and a small ML PII classifier to enforce tool access on ingress and prevent sensitive data from leaking on egress. You will see how each decision is audited. You will also see how policy stays outside the LLM so you can change rules without touching prompts or model weights.\n\n## 0\\. What this system guarantees\n\n1. Agents cannot bypass policy. The agent never talks to tools directly. Every call flows through an MCP server that consults OPA before any action.\n2. Tool access rules live only in OPA. You manage permissions in one place. You do not spread them across agents, tools, or code paths.\n3. PII cannot leak back to the agent. The MCP server classifies content with a small ML model, then enforces OPA egress rules that redact or block as needed.\n4. All decisions are auditable. Every check writes a structured record so you can explain and replay decisions later.\n5. ML classifies data, OPA decides policy. The model produces signals. OPA turns those signals into allow, redact, or block.\n\n## 1\\. Project structure\n\nYou will work with a small, clear layout. Each folder has one responsibility. Policies and data live under opa. The MCP server contains the enforcement logic. CrewAI connects through a single tool integration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "project/\nâ”œâ”€â”€ opa/\nâ”‚   â”œâ”€â”€ policies/\nâ”‚   â”‚   â”œâ”€â”€ tool_access.rego\nâ”‚   â”‚   â””â”€â”€ egress_control.rego\nâ”‚   â””â”€â”€ data/\nâ”‚       â””â”€â”€ tool_catalog.json\nâ”œâ”€â”€ audit.py\nâ”œâ”€â”€ approvals.py\nâ”œâ”€â”€ opa_client.py\nâ”œâ”€â”€ pii_classifier.py\nâ”œâ”€â”€ tools.py\nâ”œâ”€â”€ mcp_server.py\nâ”œâ”€â”€ crew_tools.py\nâ””â”€â”€ crew.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2\\. Install dependencies\n\nSet up your environment and install what you need for CrewAI, MCP, OPA, and spaCy. This gives you the runtime for policy checks, content classification, and agent execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install crewai fastapi uvicorn requests neo4j spacy\npython -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run OPA:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docker run --rm -p 8181:8181 \\\n  -v \"$PWD/opa\":/opa \\\n  openpolicyagent/opa:latest \\\n  run --server /opa/policies /opa/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tip. Keep OPA running in a separate terminal. You will want to see policy decision logs as you test.\n\n## 3\\. Tool catalog (FACTS ONLY)\n\nThis file declares the tools that exist, their inputs and outputs, and any static metadata. It does not contain permissions or rules. It is a source of truth for capabilities, not policy.\n\nopa/data/tool\\_catalog.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```json\n{\n  \"tools\": {\n    \"initiate_refund\": {\n      \"risk_level\": \"high\",\n      \"data_classification\": \"financial\",\n      \"max_auto_amount\": 500,\n      \"approval_threshold\": 2000\n    }\n  }\n}\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No permissions.\nNo agents.\nNo rules.\n\nPractical note. You can add new tools and attributes here. OPA will use these facts when it evaluates ingress policy. This keeps policy logic clean, since it can reference the catalog instead of parsing code.\n\n## 4\\. OPA ingress policy (tool access)\n\nThis policy decides whether the agent can call a tool, given who is asking and for what purpose. The MCP server sends OPA the agent identity, tool name, and request context. OPA responds with allow or deny, with reasons. You can add conditions like time, environment, or purpose.\n\nopa/policies/tool\\_access.rego"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```rego\npackage tool_access\n\ndefault decision = \"deny\"\n\ntool := data.tools[input.tool.id]\n\ndecision = \"allow\" {\n  input.agent.role == \"support_agent\"\n  input.arguments.amount <= tool.max_auto_amount\n}\n\ndecision = \"require_approval\" {\n  input.agent.role == \"support_agent\"\n  input.arguments.amount > tool.max_auto_amount\n  input.arguments.amount <= tool.approval_threshold\n}\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you test, try changing the purpose or agent role in the request. You will see OPA flip decisions without any LLM prompt changes.\n\n## 5\\. OPA egress policy (PII control)\n\nThis policy looks at the content that is about to leave the MCP server. The small ML classifier flags PII and assigns categories and confidence. OPA reads those signals and decides whether to allow, redact, or block. You get central control over how strict you want to be.\n\nopa/policies/egress\\_control.rego"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```rego\npackage egress_control\n\ndefault action = \"block\"\n\ntool := data.tools[input.tool.id]\n\n# No PII â†’ OK\naction = \"allow\" {\n  not input.egress.contains_pii\n}\n\n# PII present, agent not cleared â†’ redact\naction = \"redact\" {\n  input.egress.contains_pii\n  input.agent.clearance != \"pii_allowed\"\n}\n\n# PII present, agent cleared â†’ allow\naction = \"allow\" {\n  input.egress.contains_pii\n  input.agent.clearance == \"pii_allowed\"\n}\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Practical paths you can try:\n\n* Allow with redaction. Let the response through but mask PII fields. Good for customer support summaries.\n* Block with human\\-in\\-the\\-loop. Stop the response and request approval. Useful for edge cases where high confidence PII appears.\n* Allow as\\-is for low risk data. Keep latency low where there is no sensitive content.\n\n## 6\\. Audit logger\n\nEvery ingress and egress decision creates a structured audit entry. You can correlate requests with a trace or request ID, see the inputs that drove the decision, and read the exact rule path that fired. This gives you explainability for security reviews and incident response.\n\naudit.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\nimport uuid\nfrom datetime import datetime\n\ndef audit_log(event: dict) -> str:\n    event[\"event_id\"] = event.get(\"event_id\") or str(uuid.uuid4())\n    event[\"timestamp\"] = datetime.utcnow().isoformat()\n    with open(\"audit.log\", \"a\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(event, ensure_ascii=False) + \"\\n\")\n    return event[\"event_id\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When something is denied or redacted, check the audit log. You will see the model signals, the OPA input, and the decision details.\n\n## 7\\. Human approval store\n\nSome decisions require a person to approve. This store keeps track of pending approvals, who approved, and when. It also gives you a simple way to expire approvals and prevent reuse.\n\napprovals.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n\nPENDING_APPROVALS = {}\n\ndef create_approval(facts: dict) -> str:\n    token = str(uuid.uuid4())\n    PENDING_APPROVALS[token] = facts\n    return token\n\ndef consume_approval(token: str):\n    return PENDING_APPROVALS.pop(token, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try a flow where egress is blocked pending approval. Approve it, then resend the same request. The MCP server will record a different outcome with a clean audit trail.\n\n## 8\\. OPA client\n\nThis client wraps calls to OPA. The MCP server uses it to ask OPA for ingress and egress decisions with a consistent input schema. You can swap OPA endpoints or namespaces without changing the rest of your system.\n\nopa\\_client.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n\nOPA = \"http://localhost:8181/v1/data\"\n\ndef tool_access_decision(facts: dict) -> str:\n    r = requests.post(f\"{OPA}/tool_access/decision\", json={\"input\": facts}, timeout=3)\n    return r.json()[\"result\"]\n\ndef egress_action(facts: dict) -> str:\n    r = requests.post(f\"{OPA}/egress_control/action\", json={\"input\": facts}, timeout=3)\n    return r.json()[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If a policy query fails, the client should fail closed. In practice that means deny access or block egress. The audit log will capture the error and the fail\\-closed decision.\n\n## 9\\. Small ML PII classifier (NO regex)\n\nThe classifier uses a compact spaCy pipeline to detect entities like names, emails, phone numbers, and IDs. This model does not rely on regex. It gives you probabilistic signals that travel with the content to OPA.\n\npii\\_classifier.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nLABEL_MAP = {\n    \"PERSON\": \"person_name\",\n    \"GPE\": \"location\",\n    \"LOC\": \"location\",\n    \"ORG\": \"organization\",\n    \"DATE\": \"date\"\n}\n\ndef classify_pii(result):\n    text = result if isinstance(result, str) else json.dumps(result, ensure_ascii=False)\n    doc = nlp(text)\n\n    findings = []\n    for ent in doc.ents:\n        if ent.label_ in LABEL_MAP:\n            findings.append({\n                \"type\": LABEL_MAP[ent.label_],\n                \"text\": ent.text\n            })\n\n    return {\n        \"contains_pii\": len(findings) > 0,\n        \"pii_types\": sorted({f[\"type\"] for f in findings}),\n        \"findings\": findings\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can tune thresholds to trade off between false positives and false negatives. You can also map model labels to your own sensitivity levels. These become the inputs that OPA understands.\n\n## 10\\. Tool implementation (returns PII internally)\n\nThe tool can return raw PII internally. That is expected. The key is that the agent never receives this raw data. The MCP server inspects the tool result, classifies it, and then applies OPA egress policy before anything leaves the boundary.\n\ntools.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initiate_refund(customer_id: str, amount: int):\n    return {\n        \"status\": \"executed\",\n        \"customer_id\": customer_id,\n        \"message\": (\n            f\"Refund of ${amount} issued to customer {customer_id}. \"\n            f\"Contact: John Doe, Montreal.\"\n        )\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This pattern lets you integrate real systems like CRMs or support databases without trusting the agent to police itself.\n\n## 11\\. MCP server (the enforcement boundary)\n\nThe MCP server sits between the agent and your tools. It is the gatekeeper for every call. It makes two policy checks and captures a full audit.\n\nmcp\\_server.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\nfrom audit import audit_log\nfrom approvals import create_approval, consume_approval\nfrom opa_client import tool_access_decision, egress_action\nfrom pii_classifier import classify_pii\nfrom tools import initiate_refund\n\napp = FastAPI()\n\ndef execute_tool(name, args):\n    if name == \"initiate_refund\":\n        return initiate_refund(**args)\n    raise ValueError(\"Unknown tool\")\n\n@app.post(\"/mcp/tool/{tool_name}\")\ndef call_tool(tool_name: str, payload: dict):\n    facts = {\n        \"agent\": payload[\"agent\"],\n        \"tool\": {\"id\": tool_name},\n        \"arguments\": payload[\"arguments\"],\n        \"environment\": payload.get(\"environment\", \"production\")\n    }\n\n    decision = tool_access_decision(facts)\n    event_id = audit_log({\n        \"phase\": \"access\",\n        \"agent\": facts[\"agent\"],\n        \"tool\": tool_name,\n        \"arguments\": facts[\"arguments\"],\n        \"decision\": decision\n    })\n    facts[\"event_id\"] = event_id\n\n    if decision == \"deny\":\n        return {\"error\": \"Denied by policy\"}\n\n    if decision == \"require_approval\":\n        token = create_approval(facts)\n        audit_log({\"event_id\": event_id, \"phase\": \"approval_requested\", \"token\": token})\n        return {\"status\": \"pending_approval\", \"approval_token\": token}\n\n    raw = execute_tool(tool_name, facts[\"arguments\"])\n    egress = classify_pii(raw)\n    facts[\"egress\"] = egress\n\n    action = egress_action(facts)\n    audit_log({\n        \"event_id\": event_id,\n        \"phase\": \"egress\",\n        \"action\": action,\n        \"egress\": egress\n    })\n\n    if action == \"allow\":\n        return raw\n\n    if action == \"redact\":\n        return {\"status\": \"redacted\", \"pii_types\": egress[\"pii_types\"]}\n\n    return {\"error\": \"Blocked by egress policy\"}\n\n@app.post(\"/mcp/approve/{token}\")\ndef approve(token: str):\n    facts = consume_approval(token)\n    if not facts:\n        return {\"error\": \"Invalid token\"}\n    raw = execute_tool(facts[\"tool\"][\"id\"], facts[\"arguments\"])\n    return raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "uvicorn mcp_server:app --port 3333"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What happens for each request:\n\n1. Agent asks the MCP server to use a tool. The server calls OPA to check ingress. If denied, the agent gets a safe explanation. The audit log records the denial.\n2. If allowed, the server runs the tool and keeps the raw result inside the boundary.\n3. The server runs the PII classifier on the result and sends signals to OPA for egress. OPA returns allow, redact, or block, with reasons.\n4. The server applies the decision, optionally requests human approval, and returns only safe output to the agent.\n5. The server writes a complete audit entry with inputs, decisions, and outcomes.\n\n## 12\\. CrewAI tool (calls MCP)\n\nFrom the agentâ€™s perspective there is a single tool. It talks only to the MCP server. The agent does not know about OPA, internal tools, or your data sources. You keep the enforcement surface small and consistent.\n\ncrew\\_tools.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\nfrom crewai.tools import tool\n\nMCP = \"http://localhost:3333/mcp/tool/initiate_refund\"\n\n@tool(\"initiate_refund\")\ndef initiate_refund_tool(customer_id: str, amount: int):\n    payload = {\n        \"agent\": {\n            \"id\": \"agent-1\",\n            \"role\": \"support_agent\",\n            \"clearance\": \"no_pii\"\n        },\n        \"arguments\": {\n            \"customer_id\": customer_id,\n            \"amount\": amount\n        }\n    }\n    return requests.post(MCP, json=payload).json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you add new internal tools, do it behind the MCP server. The agent integration does not change.\n\n## 13\\. CrewAI agent\n\nThis is a simple agent that uses the MCP\\-backed tool. You can prompt it to fetch or summarize a record. It will reason about what to ask. It will not see PII unless policy allows it.\n\ncrew.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from crewai import Agent, Task, Crew\nfrom crew_tools import initiate_refund_tool\n\nagent = Agent(\n    role=\"support_agent\",\n    goal=\"Handle refunds safely\",\n    tools=[initiate_refund_tool],\n    verbose=True\n)\n\ntask = Task(\n    description=\"Refund customer 123 for $250\",\n    agent=agent\n)\n\ncrew = Crew(agents=[agent], tasks=[task])\nprint(crew.kickoff())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try a prompt that asks for a customer profile. Watch how the agent gets a redacted summary instead of raw details. Then adjust OPA policy and rerun to see different outcomes without touching the prompt.\n\n## 14\\. What you will see\n\n* Tool executes internally. The agent never touches the tool directly.\n* PII detected by ML. The classifier tags entities and passes signals forward.\n* OPA egress policy fires. You get allow, redact, or block, with reasons.\n* Agent receives redacted response. Sensitive values are masked or removed.\n* Full audit trail written. Every decision has a timestamp, inputs, and rule references.\n\n## Final mental model\n\n| Component | Responsibility |\n| --- | --- |\n| CrewAI | Reasoning |\n| MCP server | Enforcement |\n| spaCy SML | Classification |\n| OPA | Policy decisions |\n| Audit log | Explainability |\n\n## Final one\\-sentence summary\n\nThis system enforces ingress and egress policies outside the LLM using OPA, classifies sensitive data with a small ML model, prevents PII leakage, and records every decision for audit and explainability."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build Explainability AI by Design with OPA, MCP, and Neo4j",
    "description": "Build production-ready control layers for agent systems using OPA, MCP, and Neo4j. Enforce policies, capture auditable traces, and insert human approvals without slowing teams or shipping velocity.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}