{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build a Local LLM API with Ollama and the OpenAI SDK\n\n**Description:** Build a Local LLM API fast: keep OpenAI SDK code, change base_url and dummy key, stream with cancel, iterate offline.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Use Ollama for This Problem\n\nOllama provides a local, OpenAI-compatible API server that lets you iterate on GenAI applications without cloud latency, API costs, or internet dependency. For builders prototyping chat completions, streaming responses, and benchmarking model performance, Ollama offers a drop-in replacement for the OpenAI SDKâ€”just swap the `base_url` and use a placeholder API key. This tutorial focuses on connecting the OpenAI Python SDK to a local Ollama server, implementing streaming with early cancellation, and measuring basic latency metrics.\n\n**Prerequisites:** Ollama installed and running locally (`ollama serve`), Python 3.8+, and the OpenAI SDK (`openai>=1.30`). This guide assumes macOS or Linux; Windows users should use WSL or Docker. You'll need at least 8GB RAM for 7B models.\n\n**What you'll build:** A Python script that points the OpenAI SDK to Ollama, streams a chat completion with early stop, and measures time-to-first-token and throughput for local model evaluation.\n\n## Core Concepts for This Use Case\n\n**OpenAI-compatible endpoint:** Ollama exposes `/v1/chat/completions` and `/v1/models` endpoints that mirror OpenAI's API contract, allowing SDK reuse without code changes beyond configuration.\n\n**Streaming and cancellation:** The SDK's `stream=True` returns an iterator; you can break early and call `.close()` to stop generation mid-response, saving compute and time during interactive dev loops.\n\n**Local benchmarking:** Measuring latency (time-to-first-token, total duration) and throughput (tokens/sec) helps compare models and quantization levels on your hardware before committing to a production choice.\n\n## Install Dependencies\n\nRun this cell first to install the OpenAI SDK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up the OpenAI Client for Ollama\n\nConfigure the SDK to point to your local Ollama server. Use environment variables for flexibility across environments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom openai import OpenAI\n\n# Point to local Ollama server; override via environment if needed\nBASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"http://localhost:11434/v1\")\nAPI_KEY = os.getenv(\"OPENAI_API_KEY\", \"ollama\")  # Placeholder for local use\n\nclient = OpenAI(base_url=BASE_URL, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why this works:** Ollama doesn't enforce authentication locally, so any non-empty string satisfies the SDK's required `api_key` parameter.\n\n## Verify the Connection\n\nCheck that Ollama is running and the model is available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n\n# List available models via OpenAI-compatible endpoint\nresponse = requests.get(f\"{BASE_URL.replace('/v1', '')}/v1/models\")\nmodels = response.json()\nprint(\"Available models:\", [m[\"id\"] for m in models.get(\"data\", [])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If this fails, ensure `ollama serve` is running in another terminal and you've pulled a model (e.g., `ollama pull llama3.1:8b`).\n\n## Stream a Completion with Early Cancellation\n\nStream tokens as they're generated and stop after the first complete sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_with_cancel(client, model=\"llama3.1:8b\"):\n    \"\"\"\n    Stream a chat completion and cancel after the first sentence.\n    \n    Args:\n        client: OpenAI client instance\n        model: Model identifier (default: llama3.1:8b)\n    \"\"\"\n    stream = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": \"Explain gradient descent in two lines.\"}],\n        stream=True,\n        temperature=0.2,\n        max_tokens=80,\n    )\n    \n    sentence_endings = {\".\", \"!\", \"?\"}\n    buffer = []\n    \n    for chunk in stream:\n        token = chunk.choices[0].delta.content or \"\"\n        print(token, end=\"\", flush=True)\n        buffer.append(token)\n        \n        # Stop after first sentence\n        if any(token.endswith(end) for end in sentence_endings):\n            break\n    \n    stream.close()  # Release server resources\n    print(\"\\n[Stream canceled after first sentence]\")\n\nstream_with_cancel(client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why cancel early:** During development, you often need just enough output to verify behaviorâ€”stopping generation saves time and compute.\n\n## Measure Latency and Throughput\n\nBenchmark a model's performance on your hardware:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n\ndef benchmark_completion(client, model, prompt):\n    \"\"\"\n    Run a completion and measure duration and throughput.\n    \n    Args:\n        client: OpenAI client instance\n        model: Model identifier\n        prompt: User prompt string\n        \n    Returns:\n        Tuple of (duration_seconds, response_text)\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"Be concise and concrete.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    \n    start = time.perf_counter()\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=0.3,\n        max_tokens=256,\n    )\n    duration = time.perf_counter() - start\n    \n    text = response.choices[0].message.content or \"\"\n    usage = getattr(response, \"usage\", None)\n    \n    if usage and usage.completion_tokens:\n        tokens_per_sec = usage.completion_tokens / duration\n        print(f\"{model}: {duration:.2f}s, {usage.completion_tokens} tokens, {tokens_per_sec:.1f} tok/s\")\n    else:\n        chars_per_sec = len(text) / duration\n        print(f\"{model}: {duration:.2f}s, ~{len(text)} chars, ~{chars_per_sec:.0f} char/s\")\n    \n    return duration, text\n\n# Example: benchmark a single model\nbenchmark_completion(\n    client,\n    \"llama3.1:8b\",\n    \"Give three bullet points on pros/cons of vector databases.\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key metrics:** Time-to-first-token (TTFT) and tokens-per-second help you choose models and quantization levels that fit your latency budget.\n\n## Measure Streaming Metrics\n\nTrack time-to-first-token separately from total duration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def measure_streaming(client, model):\n    \"\"\"\n    Measure TTFT and total streaming duration.\n    \n    Args:\n        client: OpenAI client instance\n        model: Model identifier\n    \"\"\"\n    start = time.perf_counter()\n    first_token_time = None\n    total_chars = 0\n    \n    stream = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": \"Summarize Raft consensus in 3 bullets.\"}],\n        stream=True,\n        max_tokens=180,\n        temperature=0.2,\n    )\n    \n    for chunk in stream:\n        token = chunk.choices[0].delta.content or \"\"\n        if token and first_token_time is None:\n            first_token_time = time.perf_counter() - start\n        total_chars += len(token)\n    \n    duration = time.perf_counter() - start\n    stream.close()\n    \n    print(f\"{model}: TTFT={first_token_time:.2f}s, total={duration:.2f}s, chars={total_chars}\")\n\nmeasure_streaming(client, \"llama3.1:8b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why TTFT matters:** Low TTFT improves perceived responsiveness in chat UIs; total duration reflects overall throughput.\n\n## Compare Multiple Models\n\nPull additional models and benchmark them side-by-side:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run these in your terminal before executing the cell below\nollama pull mistral:7b-instruct\nollama pull qwen2.5:7b-instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = [\"llama3.1:8b\", \"mistral:7b-instruct\", \"qwen2.5:7b-instruct\"]\nprompt = \"Give three bullet points on pros/cons of vector databases.\"\n\nfor model in models:\n    print(f\"\\n--- {model} ---\")\n    benchmark_completion(client, model, prompt)\n    measure_streaming(client, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Comparison tips:** Run each model 3 times and average results; first runs may be slower due to model loading.\n\n## Run and Evaluate End-to-End\n\nCombine all steps into a single workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def full_workflow(client, model=\"llama3.1:8b\"):\n    \"\"\"\n    Complete workflow: verify connection, stream with cancel, benchmark.\n    \"\"\"\n    print(\"1. Verifying connection...\")\n    response = requests.get(f\"{BASE_URL.replace('/v1', '')}/v1/models\")\n    print(f\"   Models available: {len(response.json().get('data', []))}\")\n    \n    print(\"\\n2. Streaming with early cancel...\")\n    stream_with_cancel(client, model)\n    \n    print(\"\\n3. Benchmarking completion...\")\n    benchmark_completion(client, model, \"Explain CAP theorem in 2 sentences.\")\n    \n    print(\"\\n4. Measuring streaming metrics...\")\n    measure_streaming(client, model)\n    \n    print(\"\\nâœ“ Workflow complete\")\n\nfull_workflow(client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Limitations and Considerations\n\n**Unsupported OpenAI features:** Ollama does not support the Assistants API, vision models, DALLÂ·E, or advanced function-calling. JSON mode and tool calls have limited supportâ€”test thoroughly.\n\n**Error handling:** Add timeouts and retries for production use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\nimport time\n\nclient = OpenAI(base_url=BASE_URL, api_key=API_KEY, timeout=30.0)\n\ndef safe_completion(client, model, messages, retries=3):\n    for attempt in range(retries):\n        try:\n            return client.chat.completions.create(model=model, messages=messages)\n        except Exception as e:\n            if attempt == retries - 1:\n                raise\n            time.sleep(2 ** attempt)  # Exponential backoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Model tags:** Use instruct-tuned variants (e.g., `mistral:7b-instruct`) for chat; base models may produce poor conversational output.\n\n**Hardware impact:** Quantization (e.g., Q4 vs Q8) trades accuracy for speed; benchmark on your target hardware to find the right balance.\n\n## Conclusion\n\nYou've configured the OpenAI SDK to use Ollama as a local inference server, implemented streaming with early cancellation, and built a basic benchmarking harness to measure latency and throughput. This setup accelerates your dev loop by eliminating cloud dependencies and API costs while maintaining SDK compatibility.\n\n**Next steps:**\n- Explore [Creating Custom Modelfiles in Ollama](/article/creating-custom-modelfiles-ollama) to standardize system prompts and parameters across your team\n- Learn [Deploying Ollama for Team Access](/article/deploying-ollama-team-access) to share models over your local network with authentication"
      ]
    }
  ],
  "metadata": {
    "title": "How to Build a Local LLM API with Ollama and the OpenAI SDK",
    "description": "Build a Local LLM API fast: keep OpenAI SDK code, change base_url and dummy key, stream with cancel, iterate offline.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}