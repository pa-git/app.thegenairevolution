{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build Prompt Ops with Feature Flags, A/B Tests, and Auto Rollback\n\n**Description:** Ship prompt updates safely using feature flags, Git versioning, and A/B tests. Learn rollout strategies, production metrics, and automatic rollback rules that prevent regressions and reduce cost.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prompt changes are production changes. A new instruction can break a parser, double your token cost, or introduce hallucinations that surface only under specific user inputs. Yet most teams still treat prompts like config tweaks, deploying them with a Git push and hoping for the best. That approach works until it doesn't, and when it fails, you lose user trust, waste budget, or violate compliance rules.\n\nThis guide shows you how to ship prompt updates safely using feature flags, Git versioning, and A/B tests. You'll learn how to version prompts as code artifacts, wire feature flags to control rollout, run canary and A/B experiments with real production traffic, define automated rollback rules based on metrics, and validate the entire pipeline end-to-end. By the end, you'll have a working Prompt Ops architecture that prevents regressions, reduces cost, and scales across multiple variants and models.\n\n## Why Feature Flags Matter for Prompt Deployments\n\nFeature flags decouple deployment from release. You can merge a new prompt to main, deploy it to production, and activate it for 1% of traffic without touching infrastructure. If the variant degrades quality or spikes cost, you flip the flag and traffic instantly reverts to the stable baseline. No rollback deploy, no cache purge, no downtime.\n\nFlags also enable safe experimentation. You can run A/B tests comparing prompt variants on real users, measure business metrics like task success or cost per request, and promote the winner automatically. Without flags, you're forced to choose between risky big-bang releases or slow, manual canary processes that require custom routing logic in every service.\n\nThe key benefit is risk reduction. Prompts affect user-facing behavior in ways that unit tests can't catch. A prompt that works in staging may fail in production due to input distribution shift, edge cases in user queries, or unexpected model behavior under load. Flags let you test in production incrementally, observe real outcomes, and roll back instantly when something breaks.\n\n## Building the Prompt Ops Pipeline\n\nPrompt Ops works when prompt selection is a first-class runtime decision. Your app should choose a prompt variant per request using deterministic assignment. Everything else follows from that. Git gives you versioned prompt artifacts, flags control routing, and telemetry evaluates outcomes. If you want a practical walkthrough of building prompt-driven chains and reliable LLM workflows, see our [guide to building LangChain LLM workflows](/article/langchain-101-build-your-first-real-llm-application-step-by-step).\n\n### Store Prompts as Versioned Artifacts in Git\n\nTreat prompts like code. Store them in a Git repository with clear structure, version control, and review workflows. Each prompt should be a file or directory containing the template, metadata, and any associated schemas or examples.\n\nA typical repository structure looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts/\n  customer_support/\n    v1.txt\n    v2.txt\n    metadata.yaml\n  data_extraction/\n    v1.txt\n    v2.txt\n    metadata.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each prompt file contains the full template, including system instructions, user message format, and any tool definitions. The metadata file tracks version history, risk level, and deployment status.\n\nMetadata should include:\n\n- Prompt ID and version number\n- Author and reviewer\n- Risk level (low, medium, high)\n- Deployment status (draft, canary, production, archived)\n- Associated feature flag key\n- Git commit SHA\n- Model name and parameters (temperature, max tokens)\n\nRisk level helps you decide rollout strategy. Low-risk changes (typo fixes, minor wording tweaks) can ramp quickly. High-risk changes (new tool calls, refusal behavior changes, output format shifts) require longer canary periods and stricter rollback rules.\n\nUse pull requests for all prompt changes. Require code review from someone who understands the downstream impact. Reviewers should check for:\n\n- Ambiguous instructions that could confuse the model\n- Missing constraints that allow unsafe outputs\n- Schema changes that break parsers\n- Verbosity increases that spike token cost\n- Tool call changes that affect function routing\n\nTag each merged PR with a semantic version (e.g., `customer_support_v2.1.0`). This tag becomes the artifact identifier you reference in feature flags and telemetry.\n\n### Make Prompt Templates Explicit and Testable\n\nPrompts should be deterministic given the same inputs. Avoid dynamic instructions that change based on runtime state unless you explicitly version those state-dependent branches.\n\nIf the prompt must produce JSON, include a strict schema and require the model to output only JSON. If tool calls are allowed, define tool usage rules clearly. If a downstream parser expects certain keys, document them. Your goal is to reduce ambiguity and increase testability. For step-by-step instructions on crafting reliable prompts and structured outputs, check out our [prompt engineering with LLM APIs guide](/article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-4).\n\nExample: A data extraction prompt should specify the exact JSON structure, required fields, and validation rules. This makes it possible to write unit tests that verify the prompt produces valid outputs for known inputs.\n\n### Wire Feature Flags to Control Prompt Selection\n\nFeature flags control which prompt variant each request receives. The flag evaluation happens at runtime, before you call the LLM. Your application queries the flag service with a user or request identifier, receives a variant assignment, loads the corresponding prompt, and proceeds with the LLM call.\n\nHere's a minimal implementation pattern in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import hashlib\nfrom typing import Dict\n\ndef select_prompt_variant(\n    user_id: str,\n    flag_key: str,\n    flag_service,\n    prompt_registry: Dict[str, str]\n) -> str:\n    # Evaluate feature flag with deterministic user context\n    variant = flag_service.get_variant(\n        flag_key=flag_key,\n        context={\"user_id\": user_id}\n    )\n    \n    # Load prompt template for assigned variant\n    prompt_template = prompt_registry.get(variant, prompt_registry[\"baseline\"])\n    \n    # Log assignment for telemetry\n    log_prompt_assignment(\n        user_id=user_id,\n        flag_key=flag_key,\n        variant=variant,\n        prompt_id=prompt_template\n    )\n    \n    return prompt_template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function takes a user ID, queries the flag service, retrieves the assigned variant, loads the corresponding prompt from a registry, and logs the assignment. The registry maps variant names to prompt file paths or content hashes.\n\nChoose a feature flag vendor based on your constraints. LaunchDarkly and Split offer enterprise features like audit logs, advanced targeting, and experimentation analytics. PostHog provides open-source options with built-in product analytics. Statsig focuses on experimentation and statistical rigor. If you need self-hosting or strict data residency, consider Unleash or Flagsmith.\n\nKey requirements for prompt flags:\n\n- Deterministic bucketing (same user always gets same variant during experiment)\n- Percentage rollouts (start at 1%, ramp to 100%)\n- Targeting rules (canary to internal users, beta testers, or specific cohorts)\n- Instant kill switch (revert to baseline without deploy)\n- Audit logs (who changed what, when)\n\n### Implement Canary Rollout with Cohort Targeting\n\nStart every prompt change with a canary. Deploy the new variant to 1-5% of traffic, monitor key metrics for 15-30 minutes, and expand gradually if metrics stay healthy.\n\nUse cohort targeting to control who sees the canary. Internal employees, beta users, or low-risk segments are good starting points. This lets you catch obvious failures before they reach your entire user base.\n\nA typical canary schedule:\n\n- 1% for 30 minutes\n- 5% for 1 hour\n- 10% for 2 hours\n- 25% for 4 hours\n- 50% for 8 hours\n- 100% if all metrics remain stable\n\nIf any metric degrades during a stage, pause the rollout and investigate. If the degradation exceeds your rollback threshold, revert immediately.\n\n### Fetch Prompts at Runtime with Caching and Fallback\n\nYour application needs to load the selected prompt at runtime. You have two main options: bake prompts into the container image or fetch them from an artifact store.\n\nBaking prompts into the image is simple and eliminates runtime dependencies, but requires a new deploy for every prompt change. This defeats the purpose of feature flags.\n\nFetching from an artifact store (S3, GCS, artifact registry) decouples prompt updates from deployments. You can update a prompt, push it to the store, and activate it via flag without restarting services.\n\nKey considerations for runtime fetching:\n\n- Cache prompts in memory after first fetch to avoid latency on every request\n- Use ETags or version hashes to detect changes and invalidate cache\n- Implement a fallback to an embedded stable prompt if the fetch fails or times out\n- Warm the cache on service startup to avoid cold-start latency\n- Handle multi-region deployments by replicating artifacts or using a CDN\n\nExample caching pattern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\nfrom functools import lru_cache\nfrom typing import Optional\n\n@lru_cache(maxsize=128)\ndef fetch_prompt(prompt_id: str, version: str) -> str:\n    try:\n        response = requests.get(\n            f\"https://artifacts.example.com/prompts/{prompt_id}/{version}\",\n            timeout=2.0\n        )\n        response.raise_for_status()\n        return response.text\n    except requests.RequestException:\n        # Fallback to embedded stable prompt\n        return get_embedded_prompt(prompt_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function fetches a prompt from an artifact store, caches it in memory, and falls back to an embedded version if the fetch fails. The cache ensures low latency after the first request.\n\n### Instrument Telemetry for Every LLM Call\n\nYou can't manage what you don't measure. Every LLM request should emit structured telemetry that ties the outcome back to the prompt variant, user, and request context.\n\nLog these fields for every request:\n\n- Prompt ID and version (Git commit SHA or semantic version)\n- Feature flag key and assigned variant\n- User or session identifier\n- Model name and parameters (temperature, max tokens, top_p)\n- Input token count and output token count\n- Latency (time to first token, total completion time)\n- HTTP status code or error type\n- Downstream success indicator (parser succeeded, tool call executed, user accepted output)\n- Cost estimate (tokens multiplied by model pricing)\n\nSend this data to your observability stack (Datadog, Honeycomb, Grafana, or your data warehouse). Build dashboards that show per-variant metrics in real time. You need to detect regressions within minutes, not hours.\n\nExample telemetry event:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```json\n{\n  \"timestamp\": \"2025-05-15T10:23:45Z\",\n  \"prompt_id\": \"customer_support_v2.1.0\",\n  \"flag_key\": \"customer_support_prompt\",\n  \"variant\": \"new_instruction\",\n  \"user_id\": \"user_12345\",\n  \"model\": \"gpt-4\",\n  \"temperature\": 0.7,\n  \"input_tokens\": 150,\n  \"output_tokens\": 80,\n  \"latency_ms\": 1200,\n  \"status\": \"success\",\n  \"parser_valid\": true,\n  \"cost_usd\": 0.0042\n}\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This event captures everything you need to compute success rate, cost per request, and latency distributions per variant.\n\n## Running A/B Tests and Defining Rollback Rules\n\nCanaries tell you if a prompt breaks. A/B tests tell you if it improves outcomes. Once your canary is stable, run a randomized experiment to measure the impact on business metrics.\n\n### Design A/B Tests with Clear Primary Metrics\n\nPick one primary metric before you start the test. This is the metric you'll use to decide whether to promote the new prompt. Common primary metrics:\n\n- Task success rate (user accepted output, downstream job completed)\n- Cost per successful request\n- User satisfaction score (thumbs up/down, CSAT)\n- Latency (p50, p95, p99)\n\nAlso define guardrail metrics. These are metrics that must not degrade, even if the primary metric improves. Examples:\n\n- Refusal rate (model refuses to answer valid requests)\n- Hallucination rate (model makes unsupported claims)\n- Schema validation failure rate\n- Token cost per request\n\nIf any guardrail metric degrades significantly, stop the test and roll back, even if the primary metric looks good.\n\n### Randomize Assignment and Run for Sufficient Duration\n\nUse the feature flag service to randomly assign users to control (baseline prompt) or treatment (new prompt). Ensure assignment is deterministic per user, so the same user always sees the same variant during the experiment.\n\nRun the test long enough to collect sufficient data. A common mistake is stopping too early because the results look good. You need enough samples to detect real differences and account for daily or weekly patterns in user behavior.\n\nMinimum guidelines:\n\n- Run for at least 3-7 days to cover weekday and weekend traffic\n- Collect at least 1,000 samples per variant (more if the effect size is small)\n- Use statistical tests (t-test, chi-square, or Bayesian methods) to determine significance\n- Avoid peeking at results repeatedly, which inflates false positive rates\n\nIf your traffic is too low for a full A/B test, consider interleaving (show both variants to the same user in sequence) or rely on offline evaluation plus a staged rollout with close monitoring.\n\n### Define Automated Rollback Rules Based on Metrics\n\nAutomated rollback prevents bad prompts from staying live. Define hard thresholds that trigger an immediate revert to baseline. These rules run continuously during canary and A/B phases.\n\nExample rollback triggers:\n\n- Success rate drops more than 5 percentage points compared to baseline\n- p95 latency increases by more than 50%\n- Cost per request increases by more than 30%\n- Hallucination rate increases by more than 10 percentage points\n- Error rate exceeds 2%\n\nImplement these rules as a control loop that queries your metrics backend every 5-15 minutes, compares the new variant to baseline, and calls the feature flag API to disable the variant if any threshold is breached.\n\nHere's a conceptual implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\nfrom typing import Dict, List\n\ndef rollback_control_loop(\n    flag_key: str,\n    variant: str,\n    baseline: str,\n    metrics_client,\n    flag_service,\n    thresholds: Dict[str, float],\n    check_interval_seconds: int = 300\n):\n    while True:\n        # Fetch metrics for both variants\n        variant_metrics = metrics_client.get_metrics(variant)\n        baseline_metrics = metrics_client.get_metrics(baseline)\n        \n        # Check each threshold\n        for metric_name, max_delta in thresholds.items():\n            variant_value = variant_metrics.get(metric_name, 0)\n            baseline_value = baseline_metrics.get(metric_name, 0)\n            delta = variant_value - baseline_value\n            \n            if delta > max_delta:\n                # Trigger rollback\n                flag_service.disable_variant(flag_key, variant)\n                alert_team(f\"Rollback triggered: {metric_name} delta {delta} exceeds {max_delta}\")\n                return\n        \n        time.sleep(check_interval_seconds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This loop runs continuously, fetches metrics for the new variant and baseline, compares them against thresholds, and disables the variant if any threshold is exceeded. In production, run this as a cron job, Kubernetes CronJob, or serverless function.\n\nAdjust check intervals based on traffic volume and telemetry lag. High-traffic services can check every 5 minutes. Low-traffic services may need 30-60 minute windows to collect enough samples. Account for metric export delays (OpenTelemetry batching, backend ingestion lag) when setting intervals.\n\n### Automate Promotion When Metrics Improve\n\nRollback rules handle failures. Promotion rules handle success. Define criteria for automatically promoting a variant to 100% traffic when it outperforms baseline.\n\nExample promotion criteria:\n\n- Primary metric improves by at least 5% with statistical significance (p < 0.05)\n- All guardrail metrics remain stable (no degradation beyond noise)\n- Variant has been live for at least 48 hours\n- No incidents or manual interventions during the test\n\nWhen these criteria are met, the control loop calls the flag API to ramp the variant to 100% and archives the baseline. This closes the loop: you ship a prompt, test it, measure outcomes, and promote or roll back automatically.\n\n### Measure Hallucination and Output Quality\n\nHallucination is hard to define, but you can start with operational signals.\n\n- Citation mismatch. If your system uses retrieval, check whether claims reference retrieved sources.\n- Tool call mismatch. If the model claims it executed an action, verify tool logs.\n- Unsupported numeric claims. Flag outputs with numbers not present in context.\n\nThese heuristics are imperfect, but they are useful for trend detection across variants. For a deeper dive into reducing hallucinations with semantic search and vector stores, see our [ultimate guide to vector store retrieval for RAG systems](/article/rag-101-build-an-index-run-semantic-search-and-use-langchain-to-automate-it).\n\nYou can also use LLM-as-judge to evaluate outputs. Prompt a separate model to score the output for relevance, accuracy, or adherence to instructions. This adds cost and latency, so run it on a sample of requests rather than every call.\n\nOperational definitions for common metrics:\n\n- Success rate: Percentage of requests where the output passes schema validation and downstream processing succeeds (e.g., parser extracts required fields, user accepts suggestion).\n- Cost per request: Total tokens (input plus output) multiplied by model pricing. For multi-step agents or tool calls, sum tokens across all LLM invocations in the request.\n- Hallucination rate: Percentage of outputs flagged by citation checks, tool call verification, or LLM-as-judge scoring below a threshold.\n\nWithout clear definitions, teams build inconsistent dashboards and can't compare results across experiments.\n\n## Validation and Next Steps\n\nBefore you trust this pipeline in production, validate each component end-to-end.\n\n### Test the Full Pipeline with Synthetic Traffic\n\nSend synthetic requests through your application with known inputs and expected outputs. Verify that:\n\n- Feature flag evaluation returns the correct variant for each user\n- Prompt fetching retrieves the right version and caches it correctly\n- Deterministic bucketing assigns the same user to the same variant on repeated requests\n- Telemetry logs all required fields (prompt ID, variant, tokens, latency, success)\n- Rollback rules trigger when you inject a bad metric value\n\nUse a staging environment that mirrors production configuration. Test edge cases like cache misses, artifact fetch timeouts, and flag service outages. Confirm that fallback prompts load correctly when external dependencies fail.\n\n### Force a Regression and Verify Rollback\n\nDeploy a prompt variant that you know will degrade metrics. For example, add an instruction that increases verbosity, which will spike token cost. Enable the variant for a small percentage of traffic and confirm that:\n\n- Telemetry shows the cost increase within your check interval\n- The rollback control loop detects the threshold breach\n- The flag service disables the variant automatically\n- Traffic reverts to baseline without manual intervention\n\nThis test proves your safety net works. If rollback doesn't trigger, debug the control loop, metric queries, or threshold configuration before relying on it for real experiments.\n\n### Scale to Multiple Variants and Models\n\nOnce the baseline pipeline works, extend it to support multiple concurrent experiments and model comparisons.\n\nRun multiple A/B tests in parallel by using separate feature flags for different prompts or user flows. Ensure flags don't overlap (same user shouldn't be in multiple experiments for the same feature).\n\nCompare models by treating the model name as a variant dimension. For example, test GPT-4 vs Claude vs Llama on the same prompt and measure cost, latency, and quality. Use the same telemetry and rollback rules.\n\nManage prompt complexity by organizing prompts into families (customer support, data extraction, code generation) and versioning each family independently. This prevents a single Git repository from becoming a bottleneck.\n\n### Promote Winners and Clean Up Experiments\n\nWhen an A/B test concludes and you've chosen a winner, promote it to 100% traffic and clean up the experiment artifacts.\n\nSteps to close an experiment:\n\n- Ramp the winning variant to 100% via the feature flag\n- Archive the losing variant in Git (tag it as `archived` in metadata)\n- Remove the feature flag or convert it to a kill switch (keeps the flag but sets it to 100% winner, allowing instant rollback if needed)\n- Document the experiment outcome (primary metric delta, guardrail results, decision rationale)\n- Tag a Git release with the stable prompt version\n\nThis prevents flag debt (hundreds of unused flags cluttering your codebase) and keeps the prompt repository clean.\n\n### Integrate with CI/CD Pipelines\n\nAutomate prompt validation and deployment by integrating with your CI/CD system.\n\nOn every pull request:\n\n- Run linters to check prompt structure (required sections, max length, forbidden phrases)\n- Execute unit tests with known inputs and expected outputs\n- Validate metadata completeness (prompt ID, version, risk level, flag key)\n- Require approval from a designated code owner\n\nOn merge to main:\n\n- Build a prompt artifact (zip file, Docker image, or registry entry)\n- Upload the artifact to your storage backend (S3, GCS, artifact registry)\n- Create or update the feature flag with the new variant\n- Trigger a canary rollout to 1% traffic\n\nThis pipeline ensures every prompt change is reviewed, tested, and deployed safely without manual steps.\n\n### Consider Multi-Armed Bandits for Continuous Optimization\n\nA/B tests measure a fixed set of variants. Multi-armed bandits dynamically allocate traffic to the best-performing variant as data accumulates, reducing the cost of exploration.\n\nBandits work well when:\n\n- You have many variants to test (5+)\n- You want to minimize regret (traffic sent to suboptimal variants)\n- The reward signal is fast and reliable (immediate user feedback)\n\nBandits are risky when:\n\n- Reward signals are noisy or delayed (e.g., user retention measured days later)\n- Variants have non-stationary performance (quality drifts over time)\n- You need rigorous statistical guarantees (bandits optimize for reward, not inference)\n\nIf you use bandits, keep guardrail metrics and rollback rules active. Bandits can exploit measurement artifacts or drift into unsafe regions if left unchecked.\n\nPrompt Ops is not a one-time setup. It's a continuous practice of versioning, testing, measuring, and iterating. The architecture described here gives you the foundation to ship prompt changes safely, learn from production data, and scale your GenAI systems with confidence."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build Prompt Ops with Feature Flags, A/B Tests, and Auto Rollback",
    "description": "Ship prompt updates safely using feature flags, Git versioning, and A/B tests. Learn rollout strategies, production metrics, and automatic rollback rules that prevent regressions and reduce cost.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}