{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Deploy DeepSeek-R1 Locally with Ollama, MongoDB, and a Chat UI\n\n**Description:** Build a private DeepSeek-R1 chatbot with Ollama, MongoDB, and chat UIâ€”no external APIs. Deployment steps for local setups or AWS.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Stack?\n\nYou'll use Ollama to run DeepSeek-R1 locally, MongoDB to persist conversations, and Hugging Face Chat UI for a web interface. This combination gives you full control over data, avoids API costs, and lets you iterate quickly without external dependencies.\n\n**Hardware scope:** 8B models need ~8â€“12 GB RAM; 14B needs ~16â€“24 GB; 32B needs ~30â€“50 GB. A 4-core CPU with 16 GB RAM is a practical starting point for 8B. GPU acceleration is optional but improves latencyâ€”see the GPU section below for setup.\n\n**What you'll build:** A local chat interface that streams responses from DeepSeek-R1, persists conversations in MongoDB, and runs on a single VM. You'll validate end-to-end streaming, test conversation retrieval, and configure basic security.\n\n## Prerequisites\n\n- Ubuntu 22.04 or 24.04 VM with at least 16 GB RAM\n- Root or sudo access\n- Python 3.10+ and Node.js 18+ installed\n- Docker installed for MongoDB\n\nInstall system dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update package lists and install curl, git, and Docker prerequisites\nsudo apt update && sudo apt install -y curl git ca-certificates gnupg lsb-release\n\n# Add Docker's official GPG key and repository\nsudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n\n# Install Docker and start the service\nsudo apt update && sudo apt install -y docker-ce docker-ce-cli containerd.io\nsudo systemctl enable --now docker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install Node.js 18 LTS:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add NodeSource repository for Node.js 18.x\ncurl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\n\n# Install Node.js and npm\nsudo apt install -y nodejs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install and Configure Ollama\n\nOllama serves models via a local REST API. Install it and verify connectivity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and install Ollama binary\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Start Ollama as a background service\nsudo systemctl enable --now ollama\n\n# Verify Ollama is running and reachable\ncurl http://localhost:11434/api/tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see a JSON response listing available models (empty initially).\n\nPull the DeepSeek-R1 8B model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List available DeepSeek-R1 tags to confirm the exact version\nollama list | grep deepseek-r1\n\n# Pull the 8B quantized model (adjust tag if needed)\nollama pull deepseek-r1:8b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This downloads ~5â€“6 GB. Verify the model loads:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Send a test prompt and confirm streaming response\ncurl -X POST http://localhost:11434/api/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\":\"deepseek-r1:8b\",\"prompt\":\"Explain recursion in one sentence.\",\"stream\":false}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should receive a JSON object with a `response` field containing the model's answer.\n\n**Model size guidance:** 1.5B runs in 2â€“4 GB RAM; 7â€“8B needs ~8â€“12 GB; 14B needs ~16â€“24 GB; 32B needs ~30â€“50 GB; 70B requires serious RAM/GPU. Start with 8B for quality vs. latency balance, then scale up if your hardware allows. For a deeper dive into selecting the best LLM for your application, including performance and hardware considerations, see our guide on [how to pick an LLM](/article/how-to-choose-an-ai-model-for-your-app-speed-cost-reliability).\n\n## Step 2: Run MongoDB with Docker\n\nMongoDB stores conversation history. Run it in a container with a persistent volume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a directory for MongoDB data persistence\nmkdir -p ~/mongodb_data\n\n# Start MongoDB container with volume mount and default port\ndocker run -d \\\n  --name mongodb \\\n  --restart unless-stopped \\\n  -v ~/mongodb_data:/data/db \\\n  -p 127.0.0.1:27017:27017 \\\n  mongo:7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify MongoDB is running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check container status\ndocker ps | grep mongodb\n\n# Test connection with mongosh (install if needed: sudo apt install -y mongodb-mongosh)\nmongosh --eval \"db.adminCommand('ping')\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see `{ ok: 1 }`.\n\n**Optional: Enable authentication**\n\nFor production, create an admin user and application user:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to MongoDB shell\ndocker exec -it mongodb mongosh\n\n# Inside mongosh, create admin user\nuse admin\ndb.createUser({\n  user: \"admin\",\n  pwd: \"CHANGE_THIS_PASSWORD\",\n  roles: [\"root\"]\n})\n\n# Create application user with read/write on chatdb\nuse chatdb\ndb.createUser({\n  user: \"chatapp\",\n  pwd: \"CHANGE_THIS_APP_PASSWORD\",\n  roles: [{ role: \"readWrite\", db: \"chatdb\" }]\n})\nexit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Restart MongoDB with auth enabled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop and remove the existing container\ndocker stop mongodb && docker rm mongodb\n\n# Start with authentication required\ndocker run -d \\\n  --name mongodb \\\n  --restart unless-stopped \\\n  -v ~/mongodb_data:/data/db \\\n  -p 127.0.0.1:27017:27017 \\\n  -e MONGO_INITDB_ROOT_USERNAME=admin \\\n  -e MONGO_INITDB_ROOT_PASSWORD=CHANGE_THIS_PASSWORD \\\n  mongo:7 --auth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Update your connection string to include credentials:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mongodb://chatapp:CHANGE_THIS_APP_PASSWORD@localhost:27017/chatdb?authSource=chatdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Set Up Hugging Face Chat UI\n\nClone the Chat UI repository and configure it to use Ollama and MongoDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the Hugging Face Chat UI repository\ngit clone https://github.com/huggingface/chat-ui.git\ncd chat-ui\n\n# Install dependencies\nnpm install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a `.env.local` file with the following configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MongoDB connection string (use authenticated URI if you enabled auth)\nMONGODB_URL=mongodb://localhost:27017/chatdb\n\n# Ollama API base URL\nOLLAMA_BASE_URL=http://localhost:11434\n\n# Model configuration for Chat UI\nMODELS=`[\n  {\n    \"name\": \"deepseek-r1:8b\",\n    \"displayName\": \"DeepSeek-R1 8B\",\n    \"description\": \"Local reasoning model via Ollama\",\n    \"parameters\": {\n      \"temperature\": 0.7,\n      \"max_new_tokens\": 2048\n    },\n    \"endpoints\": [{\n      \"type\": \"ollama\",\n      \"url\": \"http://localhost:11434\",\n      \"ollamaName\": \"deepseek-r1:8b\"\n    }]\n  }\n]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key configuration notes:**\n\n- `MONGODB_URL`: Connection string for MongoDB. Use `chatdb` as the database name.\n- `OLLAMA_BASE_URL`: Points to your local Ollama instance.\n- `MODELS`: JSON array defining available models. The `type: \"ollama\"` tells Chat UI to use Ollama's API format.\n\nStart the development server:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the Chat UI dev server on port 3000\nnpm run dev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The UI will be available at `http://localhost:3000`. Open it in a browser and confirm the model appears in the dropdown.\n\n## Step 4: Validate End-to-End Streaming\n\nTest that the UI streams responses from Ollama and persists conversations in MongoDB.\n\n**UI test:**\n\n1. Open `http://localhost:3000` in a browser.\n2. Select \"DeepSeek-R1 8B\" from the model dropdown.\n3. Send a prompt: \"Explain how binary search works.\"\n4. Confirm tokens stream in real-time and the response completes.\n\n**Python validation script:**\n\nInstall Python dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install requests pymongo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create `validate.py`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\nimport pymongo\nimport time\n\n# Test Ollama API directly\nollama_url = \"http://localhost:11434/api/generate\"\npayload = {\n    \"model\": \"deepseek-r1:8b\",\n    \"prompt\": \"What is the capital of France?\",\n    \"stream\": False\n}\n\nprint(\"Testing Ollama API...\")\nresponse = requests.post(ollama_url, json=payload)\nif response.status_code == 200:\n    print(\"Ollama response:\", response.json().get(\"response\", \"\")[:100])\nelse:\n    print(\"Ollama error:\", response.status_code, response.text)\n\n# Test MongoDB connectivity and write a test document\nprint(\"\\nTesting MongoDB...\")\nclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\ndb = client[\"chatdb\"]\ncollection = db[\"test_conversations\"]\n\ntest_doc = {\n    \"timestamp\": time.time(),\n    \"prompt\": \"Test prompt\",\n    \"response\": \"Test response\"\n}\nresult = collection.insert_one(test_doc)\nprint(\"Inserted document ID:\", result.inserted_id)\n\n# Retrieve and verify\nretrieved = collection.find_one({\"_id\": result.inserted_id})\nprint(\"Retrieved document:\", retrieved)\n\nclient.close()\nprint(\"\\nValidation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "python validate.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see successful Ollama and MongoDB interactions.\n\n## Step 5: Production Deployment\n\nFor production, build the UI, run it with a process manager, and add a reverse proxy with HTTPS.\n\n**Build the UI:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create optimized production build\nnpm run build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Run with PM2:**\n\nInstall PM2 globally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo npm install -g pm2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create `ecosystem.config.js`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```javascript\nmodule.exports = {\n  apps: [{\n    name: \"chat-ui\",\n    script: \"npm\",\n    args: \"start\",\n    env: {\n      NODE_ENV: \"production\",\n      PORT: 3000\n    },\n    instances: 1,\n    autorestart: true,\n    watch: false,\n    max_memory_restart: \"1G\"\n  }]\n};\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Start the app:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the UI with PM2\npm2 start ecosystem.config.js\n\n# Save PM2 process list and enable startup script\npm2 save\npm2 startup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reverse proxy with Caddy:**\n\nInstall Caddy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo apt install -y debian-keyring debian-archive-keyring apt-transport-https\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/gpg.key' | sudo gpg --dearmor -o /usr/share/keyrings/caddy-stable-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/caddy-stable-archive-keyring.gpg] https://dl.cloudsmith.io/public/caddy/stable/deb/debian any-version main\" | sudo tee /etc/apt/sources.list.d/caddy-stable.list\nsudo apt update && sudo apt install -y caddy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create `/etc/caddy/Caddyfile`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "your-domain.com {\n    reverse_proxy localhost:3000\n    \n    # Enable automatic HTTPS via Let's Encrypt\n    tls your-email@example.com\n    \n    # Add basic auth (generate hash with: caddy hash-password)\n    basicauth / {\n        admin $2a$14$HASHED_PASSWORD_HERE\n    }\n    \n    # Security headers\n    header {\n        Strict-Transport-Security \"max-age=31536000; includeSubDomains; preload\"\n        X-Content-Type-Options \"nosniff\"\n        X-Frame-Options \"DENY\"\n    }\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a hashed password:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "caddy hash-password"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paste the output into the Caddyfile, then reload:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo systemctl reload caddy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your UI is now accessible at `https://your-domain.com` with automatic TLS and basic auth.\n\n## Optional: GPU Acceleration\n\nIf you have an NVIDIA GPU, enable it for faster inference.\n\n**Install NVIDIA drivers:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install NVIDIA drivers (adjust version as needed)\nsudo apt install -y nvidia-driver-535\n\n# Reboot to load drivers\nsudo reboot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After reboot, verify:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Install nvidia-container-toolkit:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add NVIDIA container toolkit repository\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\n# Install toolkit and restart Docker\nsudo apt update && sudo apt install -y nvidia-container-toolkit\nsudo systemctl restart docker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Test GPU access in Docker:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see GPU info.\n\n**Configure Ollama to use GPU:**\n\nOllama detects GPUs automatically. Verify by checking logs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo journalctl -u ollama -f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see messages indicating GPU initialization.\n\n**Validate performance:**\n\nRun the same prompt with and without GPU and compare latency. GPU should deliver 2â€“5x faster token generation for 8B models.\n\n## Firewall and Network Security\n\nRestrict access to internal services:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Allow SSH, HTTP, and HTTPS\nsudo ufw allow 22/tcp\nsudo ufw allow 80/tcp\nsudo ufw allow 443/tcp\n\n# Deny external access to Ollama and MongoDB\nsudo ufw deny 11434/tcp\nsudo ufw deny 27017/tcp\n\n# Enable firewall\nsudo ufw enable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For cloud VMs, configure security groups to allow only your IP on port 22 and public access on 80/443.\n\n## Troubleshooting\n\n**Ollama not responding:**\n\nCheck service status and logs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo systemctl status ollama\nsudo journalctl -u ollama -n 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Restart if needed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo systemctl restart ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Chat UI can't connect to Ollama:**\n\nVerify `OLLAMA_BASE_URL` in `.env.local` matches the running instance. Test with curl:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "curl http://localhost:11434/api/tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**MongoDB connection errors:**\n\nCheck container logs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docker logs mongodb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify the connection string format and credentials if auth is enabled.\n\n**High latency:**\n\nIf latency is high, downgrade model size or ensure you have adequate RAM and fast storage. Understanding when to use smaller models versus larger ones can help you optimize both cost and performanceâ€”explore our analysis on [small vs large language models](/article/small-language-models-vs-large-language-models-when-to-use-each-2) for practical scenarios.\n\n**DeepSeek-R1 \"thinking\" tokens visible:**\n\nDeepSeek-R1 may output reasoning tokens. To suppress them, add a system prompt in the UI settings or filter the stream in middleware. Check Chat UI documentation for custom prompt templates.\n\n## Next Steps\n\n- **Add authentication:** Integrate OAuth or JWT-based auth for multi-user access.\n- **Enable observability:** Add structured logging with Winston or Pino, and scrape logs for latency metrics.\n- **Scale with Docker Compose:** Create a `docker-compose.yml` with services for Ollama, MongoDB, and Chat UI for reproducible deployments.\n- **Optimize prompts:** To ensure your most important instructions aren't missed in long prompts, check out our tips on [placing critical info in long prompts](/article/lost-in-the-middle-placing-critical-info-in-long-prompts).\n- **Deploy to cloud:** Use Terraform or cloud-init scripts to automate VM provisioning and service setup on AWS, GCP, or Azure.\n\nYou now have a fully local, cost-free chat interface powered by DeepSeek-R1, with persistent conversations and production-ready deployment options."
      ]
    }
  ],
  "metadata": {
    "title": "How to Deploy DeepSeek-R1 Locally with Ollama, MongoDB, and a Chat UI",
    "description": "Build a private DeepSeek-R1 chatbot with Ollama, MongoDB, and chat UIâ€”no external APIs. Deployment steps for local setups or AWS.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}