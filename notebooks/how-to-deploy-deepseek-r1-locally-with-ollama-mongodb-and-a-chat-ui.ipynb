{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Deploy DeepSeek-R1 Locally with Ollama, MongoDB, and a Chat UI\n\n**Description:** Build a private DeepSeek-R1 chatbot with Ollama, MongoDB, and chat UIâ€”no external APIs. Deployment steps for local setups or AWS.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By now, you have probably heard of DeepSeek R1, the open source large language model from Chinese startup DeepSeek. Its release made headlines and even coincided with a selloff in several AI related stocks in U.S. markets. Many reviews have already covered how impressive the model is, so you do not need one more. You will focus on one of the most useful angles instead. You can download the model and run it on your own machine.\n\nWhy would you do that? Maybe you do not want to send data to a third party API. Or you want to control costs. Running locally lets you fine tune the model and customize everything to your stack.\n\nThe good news. Deploying DeepSeek R1 on your own hardware is straightforward. Here is how to do it in 2025\\.\n\n## Get a machine: AWS EC2 instance\n\nFirst, pick a machine to run DeepSeek R1\\. If you are experimenting or building a personal chatbot, your local computer can work. If you are building for production, you might prefer dedicated servers. If you want to get started quickly, a cloud instance is the fastest route.\n\nFor a lightweight start, an AWS EC2 CPU instance is enough for the 1\\.5B parameter variant. In the original example, an m5\\.2xlarge was used. You can still use it. You can also choose a newer generation instance like m7i.2xlarge or m7g.2xlarge to improve price performance.\n\nIf you want faster responses or plan to try larger variants, use a GPU instance. A g6\\.xlarge or g5\\.xlarge is a good baseline. These give you an NVIDIA GPU with enough VRAM for 7B class models at practical quantization levels.\n\nTo launch an EC2 instance, follow the official AWS guide: AWS EC2 Getting Started Guide. <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html>\n\n## Set up your machine\n\nYou will install the essentials needed to run DeepSeek R1\\. Start with a fresh Ubuntu LTS image to keep things clean. Ubuntu 24\\.04 LTS is a solid default in late 2025\\.\n\nConnect to your instance over SSH using AWSâ€™s official steps: Connecting to Your Linux Instance Using SSH. [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect\\-linux\\-inst\\-ssh.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect-linux-inst-ssh.html)\n\n### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update system packages\nsudo apt update && sudo apt upgrade -y\n\n# Install dependencies\nsudo apt install -y curl git\n\n# Install Ollama\n# This script will create, enable, and start the Ollama systemd service\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Install Node.js and npm (for Chat UI)\ncurl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\nsudo apt install -y nodejs\n\n# Restart shell session to apply changes\nexec bash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download and serve DeepSeek R1 on Ollama\n\nChoose the DeepSeek R1 model variant based on your resources and performance needs. In this walkthrough, you will use DeepSeek R1 1\\.5B, the smallest version available in many community runtimes.\n\nThe 1\\.5B model has 1\\.5 billion parameters. It runs well on consumer hardware or modest cloud instances. It offers lower compute demands and delivers solid results for everyday chat and coding use cases.\n\nAs of late 2025, you can also choose larger variants in Ollama and similar runtimes. For example, several 7B and 8B options are available. Larger models respond more coherently and reason better, but they require more memory. Here is a quick rule of thumb to help you decide:\n\n* CPU only with 16 to 32 GB RAM. Use 1\\.5B or a quantized 7B model.\n* Single mid range GPU with 16 to 24 GB VRAM. Use 7B or 8B quantized models comfortably.\n* High VRAM GPUs. Consider larger models if you need stronger reasoning and can tolerate higher cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.5B version (smallest, lightweight, suitable for low-resource setups)\nollama pull deepseek-r1:1.5b\n\n# 8B version (mid-range, balances performance and resource usage)\nollama pull deepseek-r1:8b\n\n# 14B version (higher accuracy, requires more compute power)\nollama pull deepseek-r1:14b\n\n# 32B version (powerful, best for advanced tasks, needs high-end hardware)\nollama pull deepseek-r1:32b\n\n# 70B version (largest, highest performance, very resource-intensive)\nollama pull deepseek-r1:70b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the download completes, list installed models to confirm the load."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "$ ollama list\nNAME                ID              SIZE      MODIFIED\ndeepseek-r1:1.5b    a42b25d8c10a    1.1 GB    2 seconds ago"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ollama serves on [http://127\\.0\\.0\\.1:11434](http://127.0.0.1:11434) by default. Check service health with the commands below. Keep this API URL handy. You will use it in your chat UI configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if Ollama is running and list downloaded models\ncurl http://127.0.0.1:11434/api/tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see output listing the models available on your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{\n   \"models\":[\n      {\n         \"name\":\"deepseek-r1:1.5b\",\n         \"model\":\"deepseek-r1:1.5b\",\n         \"modified_at\":\"2025-02-01T17:05:07.520024256Z\",\n         \"size\":1117322599,\n         \"digest\":\"a42b25d8c10a841bd24724309898ae851466696a7d7f3a0a408b895538ccbc96\",\n         \"details\":{\n            \"parent_model\":\"\",\n            \"format\":\"gguf\",\n            \"family\":\"qwen2\",\n            \"families\":[\n               \"qwen2\"\n            ],\n            \"parameter_size\":\"1.8B\",\n            \"quantization_level\":\"Q4_K_M\"\n         }\n      }\n   ]\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the model with a simple generate call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "curl -X POST http://127.0.0.1:11434/api/generate -d '{\n  \"model\": \"deepseek-r1:1.5b\",\n  \"prompt\": \"What is Ollama?\",\n  \"num_predict\": 100,\n  \"stream\": false\n}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the chat interface\n\nDeepSeek R1 is running. Next, add a chat UI so you can talk to your model from a browser.\n\n### Install MongoDB\n\nThe chat UI stores conversation history in a MongoDB instance. This is required for it to function properly. The simplest path is a local MongoDB container with a persistent volume. Docker makes this easy and repeatable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo snap install docker\nsudo docker run -d -p 27017:27017 -v mongo-chat-ui:/data --name mongo-chat-ui mongo:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When MongoDB is running, the database is available at:\nmongodb://localhost:27017\n\nYou will add this URL to your chat UI configuration file (.env.local).\n\n### Download and install Clone Chat UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Clone Chat UI\ngit clone https://github.com/huggingface/chat-ui.git\ncd chat-ui\n\n#Install Dependencies\nnpm install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Chat UI\n\nUpdate your .env.local file with the following values.\n\n* MongoDB URL: mongodb://localhost:27017\\. This stores chat history.\n* Ollama Endpoint: [http://127\\.0\\.0\\.1:11434](http://127.0.0.1:11434). This is your local Ollama API.\n* Ollama Model Name: deepseek\\-r1:1\\.5b. Replace this with the exact model tag you installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a .env.local file:\nnano .env.local"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can tweak parameters to suit your hardware and latency goals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MONGODB_URL=mongodb://localhost:27017\nMODELS=`[\n  {\n    \"name\": \"DeepSeek-R1\",\n    \"chatPromptTemplate\": \"<s>{{#each messages}}{{#ifUser}}[INST] {{content}} [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}</s> {{/ifAssistant}}{{/each}}\",\n    \"parameters\": {\n      \"temperature\": 0.3,\n      \"top_p\": 0.95,\n      \"max_new_tokens\": 1024,\n      \"stop\": [\"</s>\"]\n    },\n    \"endpoints\": [\n      {\n        \"type\": \"ollama\",\n        \"url\" : \"http://127.0.0.1:11434\",\n        \"ollamaName\" : \"deepseek-r1:1.5b\" \n      }\n    ]\n  }\n]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you finish, save and exit. Use CTRL\\+X, then Y, then ENTER.\n\n## Use your very own DeepSeek R1 chatbot\n\nYou are ready to use your DeepSeek R1 chatbot.\n\n### Start Chat UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the Chat UI in development mode, making it accessible on the network\n$ npm run dev -- --host 0.0.0.0\n\n# The output confirms the server is running and displays the accessible port\n> chat-ui@0.9.4 dev\n> vite dev --host 0.0.0.0\n\n\n  VITE v5.4.14  ready in 1122 ms\n\n  âžœ  Local:   http://localhost:5173/\n  âžœ  Network: http://100.00.00.000:5173/\n  âžœ  Network: http://100.00.0.0:5173/\n  âžœ  press h + enter to show help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are running on an AWS EC2 instance, open the UI port in the instance security group. You can do this in the AWS Console under EC2 â†’ Security Groups â†’ Inbound Rules. You can also do it with the AWS CLI.\n\nFor a public deployment, consider adding a reverse proxy with HTTPS and enabling authentication. You will protect both your Ollama endpoint and the chat UI.\n\n### Access your chatbot\n\nOpen your machineâ€™s public address and port in a browser. You should see the chat interface.\n\n<img src='http://thegenairevolution.com/wp-content/uploads/2025/01/image-2-1024x644.png' alt='' title='' width='1024' height='644' /><img src='http://thegenairevolution.com/wp-content/uploads/2025/01/image-3-1024x534.png' alt='' title='' width='1024' height='534' />\n\n## Conclusion\n\nYou now have a powerful AI model running under your control, on your own machine, and inside your own security perimeter.\n\n### Recap\n\n* You set up the environment by installing Ollama, MongoDB, and required dependencies.\n* You downloaded and configured DeepSeek R1 to run locally.\n* You set up a Chat UI and linked it to MongoDB and Ollama.\n* You ensured network access by opening the needed ports on AWS.\n* You accessed the chatbot from your browser.\n* Optional. You picked a GPU instance for faster responses and larger models.\n\nYour locally hosted DeepSeek R1 chatbot is now up and running."
      ]
    }
  ],
  "metadata": {
    "title": "How to Deploy DeepSeek-R1 Locally with Ollama, MongoDB, and a Chat UI",
    "description": "Build a private DeepSeek-R1 chatbot with Ollama, MongoDB, and chat UIâ€”no external APIs. Deployment steps for local setups or AWS.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}