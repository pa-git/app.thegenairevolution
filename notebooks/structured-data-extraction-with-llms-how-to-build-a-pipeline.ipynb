{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WyKIxoQp9NM"
      },
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n",
        "\n",
        "**Title:** Structured Data Extraction with LLMs: How to Build a Pipeline\n",
        "\n",
        "**Description:** Build a reliable structured data extraction pipeline using LLMs, LangChain, and OpenAI functionsâ€”JSON schemas, deterministic outputs, zero hallucinations, for production.\n",
        "\n",
        "---\n",
        "\n",
        "*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvsRvA-Kp9NP"
      },
      "source": [
        "High-quality structured data unlocks downstream analytics, automation, and search. This guide shows you exactly how to turn messy text into validated JSON using LLMs, OpenAI function calling, LangChain, and Pydanticâ€”no training data, no brittle regex. You'll build and run a Colab-ready, deterministic extraction pipeline that converts raw text into validated JSON. If you're dealing with unpredictable input, understanding [tokenization pitfalls and invisible characters](/article/tokenization-pitfalls-invisible-characters-that-break-prompts-and-rag-2) can help you avoid subtle extraction bugs.\n",
        "\n",
        "## Why This Approach Works\n",
        "\n",
        "**Function Calling Enforces Structure**  \n",
        "OpenAI function calling forces the model to return JSON matching your schema. No free-form text, no hallucinated fields. The model outputs only what you define.\n",
        "\n",
        "**Pydantic Validates At The Boundary**  \n",
        "Pydantic models enforce types, required fields, and constraints at runtime. Invalid payloads fail fast with clear error messages, ensuring downstream systems receive clean data.\n",
        "\n",
        "**LangChain Orchestrates Composable Pipelines**  \n",
        "LangChain chains combine prompts, models, and parsers into testable, reusable pipelines. You can swap models, extend schemas, or add retry logic without rewriting core extraction logic.\n",
        "\n",
        "**Deterministic Output With Temperature Zero**  \n",
        "Setting temperature to zero eliminates randomness. The same input produces the same output every time, making extraction predictable and testable.\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Define Schema**: Use Pydantic models to specify the structure of extracted data (e.g., event name, date, outcome).\n",
        "2. **Convert To Function Spec**: Transform the Pydantic model into an OpenAI function definition.\n",
        "3. **Bind Function To Model**: Attach the function spec to the LLM so it knows to return structured JSON.\n",
        "4. **Create Prompt**: Write a strict system prompt instructing the model to extract only explicit information.\n",
        "5. **Build Chain**: Compose prompt, model, and output parser into a LangChain pipeline.\n",
        "6. **Invoke And Validate**: Run the chain on input text and validate the output with Pydantic.\n",
        "\n",
        "## Setup & Installation\n",
        "\n",
        "Run this cell at the top of your Colab notebook to install all required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqJLm14Np9NQ",
        "outputId": "75c2e1c4-2400-4f26-ea19-7833689e2bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain>=0.2 in /usr/local/lib/python3.12/dist-packages (1.0.2)\n",
            "Requirement already satisfied: langchain-openai>=0.1 in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: langchain-community>=0.2 in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-text-splitters>=0.0.1 in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.14.2)\n",
            "Requirement already satisfied: html2text in /usr/local/lib/python3.12/dist-packages (2025.4.15)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.2) (1.0.1)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.2) (1.0.2)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai>=0.1) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai>=0.1) (0.12.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2) (3.13.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2) (2.11.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2) (0.4.38)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2) (2.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.2) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.2) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain>=0.2) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain>=0.2) (25.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain>=0.2) (3.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain>=0.2) (1.0.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain>=0.2) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain>=0.2) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community>=0.2) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community>=0.2) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community>=0.2) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community>=0.2) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.1) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.1) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.1) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.1) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.1) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community>=0.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community>=0.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community>=0.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community>=0.2) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community>=0.2) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai>=0.1) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community>=0.2) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community>=0.2) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain>=0.2) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain>=0.2) (1.11.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.2) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \"langchain>=0.2\" \"langchain-openai>=0.1\" \"langchain-community>=0.2\" \"langchain-text-splitters>=0.0.1\" pydantic python-dotenv beautifulsoup4 html2text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQcDlCH7p9NR"
      },
      "source": [
        "Set your OpenAI API key as an environment variable before running the notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35nHhtSEp9NR",
        "outputId": "f22b9318-b7ca-4741-9100-887770c4f1c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All required API keys found.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "required_keys = [\"OPENAI_API_KEY\"]\n",
        "missing = [k for k in required_keys if not os.getenv(k)]\n",
        "if missing:\n",
        "    raise EnvironmentError(\n",
        "        f\"Missing required environment variables: {', '.join(missing)}\\n\"\n",
        "        \"Please set them before running the notebook. Example:\\n\"\n",
        "        \"  export OPENAI_API_KEY='your-key-here'\"\n",
        "    )\n",
        "\n",
        "print(\"All required API keys found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQismq3Qp9NR"
      },
      "source": [
        "## Step-by-Step Implementation\n",
        "\n",
        "### Step 1: Initialize The LLM\n",
        "\n",
        "Load environment variables and initialize the OpenAI model with temperature zero for deterministic output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scop_Rxyp9NR",
        "outputId": "d6b01c0b-0f4f-4ca3-8cd9-c96c75e5a3b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM ready: client=<openai.resources.chat.completions.completions.Completions object at 0x79d56d546b10> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x79d56d547a10> root_client=<openai.OpenAI object at 0x79d4ccdaed50> root_async_client=<openai.AsyncOpenAI object at 0x79d4da138500> model_name='gpt-4o-mini' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Use gpt-4o-mini for cost-effective, fast extraction with function calling support\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "print(\"LLM ready:\", llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpBzo57rp9NS"
      },
      "source": [
        "### Step 2: Define Pydantic Models\n",
        "\n",
        "Create Pydantic models to define the structure of extracted events. Field descriptions guide the LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kG9noDDcp9NS"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Event(BaseModel):\n",
        "    \"\"\"\n",
        "    Represents a single event extracted from text.\n",
        "    \"\"\"\n",
        "    name: str = Field(..., description=\"The explicit event name or title extracted verbatim from the text.\")\n",
        "    date: Optional[str] = Field(None, description=\"The explicit date as written in the text (ISO if present, else raw).\")\n",
        "    outcome: Optional[str] = Field(None, description=\"The explicit outcome/result stated in the text, if any.\")\n",
        "\n",
        "class Extracted(BaseModel):\n",
        "    \"\"\"\n",
        "    Wrapper model for a list of extracted events.\n",
        "    \"\"\"\n",
        "    events: List[Event] = Field(default_factory=list, description=\"All events explicitly mentioned in the text.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jSXAvtop9NS"
      },
      "source": [
        "### Step 3: Convert Schema To OpenAI Function Spec\n",
        "\n",
        "Transform the Pydantic model into an OpenAI function definition so the model knows to return structured JSON:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ikxVU30_p9NT"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "extract_fn = convert_to_openai_function(Extracted)\n",
        "\n",
        "functions = [extract_fn]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioOpmG9_p9NT"
      },
      "source": [
        "### Step 4: Create A Strict System Prompt\n",
        "\n",
        "Write a system prompt that instructs the model to extract only explicit information and avoid hallucination:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Q7f43Seqp9NT"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a precise information extractor.\n",
        "- Extract only information explicitly present in the text.\n",
        "- Do not infer, guess, or add missing details.\n",
        "- If a field is not explicitly present, set it to null.\n",
        "- If no events are present, return an empty list.\n",
        "- Preserve original wording where reasonable.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYSTEM_PROMPT),\n",
        "        (\"human\", \"{text}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSETmDYdp9NT"
      },
      "source": [
        "### Step 5: Bind Function To Model And Set Up Parsers\n",
        "\n",
        "Bind the function spec to the LLM and configure output parsers to extract structured data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hOn6totNp9NT"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers.openai_functions import (\n",
        "    JsonKeyOutputFunctionsParser,\n",
        "    JsonOutputFunctionsParser,\n",
        ")\n",
        "\n",
        "# Bind the function spec to the LLM\n",
        "model_with_fn = llm.bind(functions=functions)\n",
        "\n",
        "# Parser that returns only the \"events\" key from the function arguments\n",
        "events_only_parser = JsonKeyOutputFunctionsParser(key_name=\"events\")\n",
        "\n",
        "# Parser that returns the entire function arguments payload\n",
        "full_args_parser = JsonOutputFunctionsParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDbgeqdCp9NT"
      },
      "source": [
        "### Step 6: Build LangChain Pipelines\n",
        "\n",
        "Compose the prompt, model, and parsers into reusable chains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-fiYIKoDp9NU"
      },
      "outputs": [],
      "source": [
        "# Chain that returns only the list of events\n",
        "events_chain = prompt | model_with_fn | events_only_parser\n",
        "\n",
        "# Chain that returns the full structured payload for validation\n",
        "full_chain = prompt | model_with_fn | full_args_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWPqYSMcp9NU"
      },
      "source": [
        "## Run And Validate\n",
        "\n",
        "### Basic Extraction\n",
        "\n",
        "Test the pipeline on a simple input string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3IMxMvcp9NU",
        "outputId": "8a3706d8-556f-4ce1-a5e8-b9d263fc2caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'music festival', 'date': 'June 15th', 'outcome': None}, {'name': 'tech conference', 'date': 'July 20th', 'outcome': None}]\n"
          ]
        }
      ],
      "source": [
        "text = \"I attended a music festival on June 15th and a tech conference on July 20th.\"\n",
        "events = events_chain.invoke({\"text\": text})\n",
        "print(events)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCrmgs0xp9NU"
      },
      "source": [
        "### Validate With Pydantic\n",
        "\n",
        "Use the full chain to extract the payload and validate it with Pydantic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxgpB2nZp9NU",
        "outputId": "fde3bd5a-b8b1-4d01-cf86-df0256bdfead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "events=[Event(name='music festival', date='June 15th', outcome=None), Event(name='tech conference', date='July 20th', outcome=None)]\n",
            "music festival June 15th None\n",
            "tech conference July 20th None\n"
          ]
        }
      ],
      "source": [
        "payload = full_chain.invoke({\"text\": text})\n",
        "validated = Extracted.model_validate(payload)\n",
        "print(validated)\n",
        "for ev in validated.events:\n",
        "    print(ev.name, ev.date, ev.outcome)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrkD_5uLp9NU"
      },
      "source": [
        "### Edge Case: No Events\n",
        "\n",
        "Verify the extractor returns an empty list when no events are present:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSlWN1Zgp9NU",
        "outputId": "072514e2-99b9-4d47-ee87-fa14dfbeb34b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "text_irrelevant = \"This is irrelevant text with no events.\"\n",
        "empty = events_chain.invoke({\"text\": text_irrelevant})\n",
        "print(empty)\n",
        "assert empty == [], \"Extractor should not hallucinate events.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnRhJDsMp9NU"
      },
      "source": [
        "### Edge Case: Missing Fields\n",
        "\n",
        "Test extraction when some fields are missing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eIDeAI5p9NU",
        "outputId": "f56fa904-d15f-490e-c6a2-1357c3d4b829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'Launch Day', 'date': None, 'outcome': None}, {'name': 'Demo Night', 'date': None, 'outcome': None}]\n"
          ]
        }
      ],
      "source": [
        "text_partial = \"Our team hosted Launch Day and later Demo Night.\"\n",
        "partial = events_chain.invoke({\"text\": text_partial})\n",
        "print(partial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-YCOtcdp9NV"
      },
      "source": [
        "### Determinism Check\n",
        "\n",
        "Run the same input multiple times and confirm outputs are identical:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Xj9tuOD5p9NV"
      },
      "outputs": [],
      "source": [
        "same1 = events_chain.invoke({\"text\": text})\n",
        "same2 = events_chain.invoke({\"text\": text})\n",
        "assert same1 == same2, \"Outputs should be identical with temperature=0.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3lNDQTOp9NV"
      },
      "source": [
        "### Real-World Data Extraction\n",
        "\n",
        "Load a Wikipedia page and extract events from real content:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQGkMXupp9NV",
        "outputId": "4de1f303-3234-455f-8abb-c7527f1522bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 4 events\n",
            "{'name': 'Apollo 11 mission', 'date': 'July 20, 1969', 'outcome': 'astronauts Neil Armstrong and Buzz Aldrin landed their Apollo Lunar Module on the Moon and walked on the lunar surface, while Michael Collins remained in lunar orbit in the command and service module, and all three landed safely on Earth in the Pacific Ocean on July 24.'}\n",
            "{'name': 'Apollo 1 cabin fire', 'date': '1967', 'outcome': 'killed the entire crew during a prelaunch test.'}\n",
            "{'name': 'Apollo 13 landing', 'date': None, 'outcome': \"had to be aborted after an oxygen tank exploded en route to the Moon, crippling the CSM. The crew barely managed a safe return to Earth by using the Lunar Module as a 'lifeboat' on the return journey.\"}\n",
            "{'name': 'Apollo 17 mission', 'date': 'December 1972', 'outcome': None}\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Apollo_program\")\n",
        "docs = loader.load()\n",
        "page_text = docs[0].page_content[:10000]\n",
        "\n",
        "real_events = events_chain.invoke({\"text\": page_text})\n",
        "print(f\"Extracted {len(real_events)} events\")\n",
        "for e in real_events[:5]:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57b6LGemp9NV"
      },
      "source": [
        "### Chunking Long Documents\n",
        "\n",
        "For longer documents, split text into overlapping chunks, extract per chunk, then merge and deduplicate events. If you notice models missing details or hallucinating as context grows, our deep dive on [context rot and LLM memory limits](/article/context-rot-why-llms-forget-as-their-memory-grows-3) explains why this happens and how to mitigate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POoIlJ7up9NV",
        "outputId": "c7c4f077-dd4d-4473-fe81-8ba9e265e793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged events: 150\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=400)\n",
        "chunks = splitter.split_text(docs[0].page_content)\n",
        "\n",
        "all_events = []\n",
        "for ch in chunks:\n",
        "    all_events.extend(events_chain.invoke({\"text\": ch}))\n",
        "\n",
        "# Deduplicate by (name, date) tuple\n",
        "unique = {(e[\"name\"], e.get(\"date\")): e for e in all_events}\n",
        "chr = list(unique.values())\n",
        "print(f\"Merged events: {len(merged_events)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPfx-Ovhp9NV"
      },
      "source": [
        "## Constraints And Performance\n",
        "\n",
        "**Token Limits**: gpt-4o-mini supports up to 128k tokens input. For documents over 4k characters, chunk the text to avoid context window issues.\n",
        "\n",
        "**Cost**: gpt-4o-mini costs approximately $0.15 per 1M input tokens and $0.60 per 1M output tokens. A typical extraction call uses 500â€“2000 tokens.\n",
        "\n",
        "**Latency**: Expect 1â€“3 seconds per extraction call depending on input size and API load.\n",
        "\n",
        "For high-volume jobs, control prompt size, reduce chunk overlap, and prefer cheaper models like gpt-4o-mini for extraction. If you're evaluating which model best fits your pipeline's speed, cost, and reliability needs, see our practical guide on [how to choose an LLM for your application](/article/how-to-choose-an-ai-model-for-your-app-speed-cost-reliability).\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "You've built a deterministic, validated extraction pipeline that converts raw text into structured JSON using OpenAI function calling, Pydantic, and LangChain. The system enforces schema compliance, eliminates hallucination, and produces repeatable results.\n",
        "\n",
        "**Key Design Choices**:\n",
        "- **Function calling** forces the model to return only schema-compliant JSON.\n",
        "- **Pydantic validation** catches invalid payloads at runtime.\n",
        "- **LangChain orchestration** makes the pipeline composable, testable, and extensible.\n",
        "- **Temperature zero** ensures deterministic output.\n",
        "\n",
        "**Next Steps**:\n",
        "- Add retry logic with exponential backoff for production reliability.\n",
        "- Extend the schema with new fields like location or confidence scores.\n",
        "- Deploy the pipeline as a REST API using FastAPI.\n",
        "- Parallelize extraction across multiple documents with asyncio and rate limiting.\n",
        "- Add observability with structured logging and input/output hashing to track performance and avoid logging PII."
      ]
    }
  ],
  "metadata": {
    "title": "Structured Data Extraction with LLMs: How to Build a Pipeline",
    "description": "Build a reliable structured data extraction pipeline using LLMs, LangChain, and OpenAI functionsâ€”JSON schemas, deterministic outputs, zero hallucinations, for production.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}