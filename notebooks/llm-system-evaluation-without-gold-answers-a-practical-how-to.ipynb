{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** LLM System Evaluation Without Gold Answers: A Practical How-To\n\n**Description:** Build a reference-free LLM grading system that scores open responses using rubrics, few-shots, and multi-judge consensus, plus validation, drift monitoring.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Approach Works\n\nManual evaluation is slow, subjective, and doesn't scale. When you're iterating on prompts, testing new models, or running A/B experiments, you need fast, consistent feedback. Reference-free LLM evaluation solves this by using a frontier model as a judge. It scores outputs against a rubric without needing gold-standard references, making it practical for open-ended tasks like summarization, creative writing, or customer support responses.\n\nThis guide builds a production-ready evaluator with structured outputs, few-shot anchoring, validation against human labels, and optional multi-judge consensus. You'll have a Colab-ready notebook and a minimal FastAPI service you can deploy today.\n\n## How It Works (High-Level Overview)\n\nThe system follows this flow:\n\n1. **Inputs**: Task instructions, a rubric, a question, and a candidate response.\n2. **Judge Prompt**: Combine rubric criteria and few-shot examples into a strict prompt.\n3. **Model Call**: Send the prompt to an LLM with low temperature and JSON mode enabled.\n4. **Parse & Validate**: Use Pydantic to enforce schema and catch malformed outputs.\n5. **Logging**: Write scores, rationales, and metadata to CSV for tracking.\n6. **Consensus (Optional)**: Run multiple judges with different seeds and average scores to reduce variance.\n7. **Validation**: Compare AI scores to human labels using Spearman correlation and Cohen's kappa.\n8. **Outputs**: Structured evaluation results, CSV logs, and validation metrics.\n\n## Setup & Installation\n\nRun this cell first to install all dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install openai anthropic pydantic tenacity transformers scipy scikit-learn fastapi uvicorn pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import all required libraries and verify they load correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport json\nimport time\nimport csv\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nfrom statistics import mean\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import spearmanr\nfrom sklearn.metrics import cohen_kappa_score\n\nfrom pydantic import BaseModel, Field, ValidationError\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nfrom openai import OpenAI\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom fastapi import FastAPI, HTTPException\n\nprint(\"All imports successful.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Securely load API keys using Colab's built-in secrets manager. This cell checks for required keys and raises an error if any are missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import userdata\nfrom google.colab.userdata import SecretNotFoundError\n\nkeys = [\"OPENAI_API_KEY\", \"ANTHROPIC_API_KEY\"]\nmissing = []\nfor k in keys:\n    value = None\n    try:\n        value = userdata.get(k)\n    except SecretNotFoundError:\n        pass\n\n    os.environ[k] = value if value is not None else \"\"\n\n    if not os.environ[k]:\n        missing.append(k)\n\nif missing:\n    raise EnvironmentError(f\"Missing keys: {', '.join(missing)}. Add them in Colab â†’ Settings â†’ Secrets.\")\n\nprint(\"All keys loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step-by-Step Implementation\n\n### Define the Rubric and Evaluation Schema\n\nUse Pydantic to enforce structure and catch malformed outputs early. This prevents silent failures and makes debugging easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CriterionScore(BaseModel):\n    \"\"\"\n    Represents a single criterion's score and rationale.\n\n    Args:\n        score (int): Score for the criterion (1-5).\n        rationale (str): Explanation for the score.\n\n    Raises:\n        ValidationError: If score is not in the range 1-5.\n    \"\"\"\n    score: int = Field(ge=1, le=5)\n    rationale: str\n\nclass EvaluationResult(BaseModel):\n    \"\"\"\n    Represents the full evaluation result for a response.\n\n    Args:\n        criteria (Dict[str, CriterionScore]): Scores and rationales for each criterion.\n        overall_score (float): Overall score (1-5).\n        summary_feedback (str): Summary feedback for the response.\n\n    Raises:\n        ValidationError: If any field is missing or out of range.\n    \"\"\"\n    criteria: Dict[str, CriterionScore]\n    overall_score: float = Field(ge=1, le=5)\n    summary_feedback: str\n\nRUBRIC = {\n    \"relevance\": \"How directly does the response address the prompt. 1 off-topic. 3 partially relevant. 5 fully on-point.\",\n    \"coherence\": \"How clear and logically structured is the response. 1 disorganized. 3 somewhat clear. 5 very clear.\",\n    \"depth\": \"How thorough and insightful is the response. 1 superficial. 3 adequate detail. 5 deep and well-supported.\",\n    \"creativity\": \"How original and valuable are the ideas. 1 generic. 3 somewhat original. 5 fresh and compelling.\"\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build the Judge Prompt\n\nConstruct a strict prompt that includes rubric criteria, few-shot examples, and the candidate response. This anchors the model's behavior and reduces variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_judge_prompt(task_instructions: str, rubric: dict, question: str, response: str, few_shots: List[dict]) -> str:\n    \"\"\"\n    Constructs a strict prompt for the LLM judge, including rubric and few-shots.\n\n    Args:\n        task_instructions (str): Instructions for the evaluation task.\n        rubric (dict): Rubric criteria and descriptions.\n        question (str): The input prompt/question.\n        response (str): The candidate response to evaluate.\n        few_shots (List[dict]): Few-shot examples with input, response, and evaluation.\n\n    Returns:\n        str: The complete prompt for the LLM.\n    \"\"\"\n    parts = []\n    parts.append(\"You are an impartial evaluator. Score the response using the rubric. Return strict JSON only.\")\n    parts.append(\"Rubric criteria. 1 to 5 integers. Provide a short rationale per criterion and a one-paragraph summary.\")\n    for name, desc in rubric.items():\n        parts.append(f\"- {name}: {desc}\")\n    parts.append(\"\\nExamples:\")\n    for ex in few_shots:\n        parts.append(\"Input: \" + ex[\"input\"])\n        parts.append(\"Response: \" + ex[\"response\"])\n        parts.append(\"Evaluation JSON: \" + json.dumps(ex[\"evaluation\"]))\n    parts.append(\"\\nNow evaluate the candidate.\")\n    parts.append(\"Input: \" + question)\n    parts.append(\"Response: \" + response)\n    parts.append(\"Return JSON with fields: criteria, overall_score, summary_feedback.\")\n    parts.append(\"criteria is an object with keys for each rubric criterion, each has fields score and rationale.\")\n    return \"\\n\".join(parts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Call the OpenAI API with Retries\n\nUse low temperature for determinism and enable JSON mode to ensure parsable outputs. Retries handle transient API errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\nclient = OpenAI()\n\nclass APIError(Exception):\n    \"\"\"Custom exception for API errors.\"\"\"\n    pass\n\n@retry(stop=stop_after_attempt(4), wait=wait_exponential(multiplier=0.5, min=0.5, max=8),\n       retry=retry_if_exception_type(APIError))\ndef call_openai(prompt: str, temperature: float = 0.2, max_tokens: int = 400, model: str = OPENAI_MODEL) -> str:\n    \"\"\"\n    Calls the OpenAI chat completion API with retries and error handling.\n\n    Args:\n        prompt (str): The prompt to send to the model.\n        temperature (float): Sampling temperature for determinism.\n        max_tokens (int): Maximum tokens to generate.\n        model (str): Model name to use.\n\n    Returns:\n        str: The model's response content.\n\n    Raises:\n        APIError: If the API call fails or returns empty content.\n    \"\"\"\n    try:\n        resp = client.chat.completions.create(\n            model=model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=temperature,\n            max_tokens=max_tokens,\n            response_format={\"type\": \"json_object\"}\n        )\n    except Exception as e:\n        raise APIError(str(e))\n    content = resp.choices[0].message.content\n    if not content:\n        raise APIError(\"Empty response\")\n    return content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parse and Validate the LLM Output\n\nPydantic validation catches schema violations and missing fields. This prevents downstream errors and makes debugging faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_evaluation(json_text: str) -> Optional[EvaluationResult]:\n    \"\"\"\n    Parses and validates the LLM's JSON output against the EvaluationResult schema.\n\n    Args:\n        json_text (str): The JSON string output from the LLM.\n\n    Returns:\n        Optional[EvaluationResult]: Parsed and validated result, or None if invalid.\n    \"\"\"\n    try:\n        data = json.loads(json_text)\n        if \"criteria\" not in data:\n            raise ValueError(\"Missing criteria\")\n        for k in RUBRIC.keys():\n            if k not in data[\"criteria\"]:\n                raise ValueError(f\"Missing criterion {k}\")\n        return EvaluationResult(**data)\n    except Exception as e:\n        print(f\"Validation error: {e}\")\n        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Few-Shot Examples\n\nFew-shots anchor the model's scoring behavior and reduce bias. Include examples that cover edge cases like verbosity without substance and concise but strong responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FEW_SHOTS = [\n    {\n        \"input\": \"Write a product tagline for a privacy-focused email app.\",\n        \"response\": \"Email, but better.\",\n        \"evaluation\": {\n            \"criteria\": {\n                \"relevance\": {\"score\": 3, \"rationale\": \"Mentions email but not privacy.\"},\n                \"coherence\": {\"score\": 4, \"rationale\": \"Clear and short.\"},\n                \"depth\": {\"score\": 1, \"rationale\": \"No specifics.\"},\n                \"creativity\": {\"score\": 2, \"rationale\": \"Generic phrasing.\"}\n            },\n            \"overall_score\": 2.5,\n            \"summary_feedback\": \"Too generic. Bring privacy into the message and add a twist.\"\n        }\n    },\n    {\n        \"input\": \"Write a product tagline for a privacy-focused email app.\",\n        \"response\": \"Your inbox, only yours. Private email for real life.\",\n        \"evaluation\": {\n            \"criteria\": {\n                \"relevance\": {\"score\": 5, \"rationale\": \"Directly emphasizes privacy.\"},\n                \"coherence\": {\"score\": 5, \"rationale\": \"Flows and reads well.\"},\n                \"depth\": {\"score\": 3, \"rationale\": \"Basic value but not features.\"},\n                \"creativity\": {\"score\": 4, \"rationale\": \"Memorable and on-brand.\"}\n            },\n            \"overall_score\": 4.25,\n            \"summary_feedback\": \"Clear privacy angle. Consider a more distinctive hook.\"\n        }\n    }\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate a Single Response\n\nThis function combines prompt building, API call, parsing, and optional repair. It returns a validated result or None if the output is malformed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_response(task_instructions: str, question: str, response: str,\n                      few_shots=FEW_SHOTS, temperature: float = 0.2, model: str = OPENAI_MODEL) -> Optional[EvaluationResult]:\n    \"\"\"\n    Evaluates a single candidate response using the LLM judge.\n\n    Args:\n        task_instructions (str): Instructions for the evaluation task.\n        question (str): The input prompt/question.\n        response (str): The candidate response to evaluate.\n        few_shots (list): Few-shot examples for the prompt.\n        temperature (float): Sampling temperature for the LLM.\n        model (str): Model name to use.\n\n    Returns:\n        Optional[EvaluationResult]: Validated evaluation result, or None if invalid.\n    \"\"\"\n    prompt = build_judge_prompt(task_instructions, RUBRIC, question, response, few_shots)\n    raw = call_openai(prompt, temperature=temperature, model=model)\n    parsed = parse_evaluation(raw)\n    if parsed is None:\n        repair_prompt = f\"Fix this JSON to match the schema. Return JSON only.\\n{raw}\"\n        try:\n            repaired = call_openai(repair_prompt, temperature=0.0, max_tokens=400, model=model)\n            parsed = parse_evaluation(repaired)\n        except Exception as e:\n            print(f\"Repair failed: {e}\")\n            parsed = None\n    return parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Evaluation with Logging\n\nEvaluate multiple items and write results to CSV. This logs every evaluation with timestamp, scores, and feedback for tracking over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUBRIC_VERSION = \"v1.0\"\nPROMPT_VERSION = \"v1.0\"\n\ndef evaluate_batch(items, task_instructions: str, outfile: str, model: str = OPENAI_MODEL) -> None:\n    \"\"\"\n    Evaluates a batch of items and writes results to a CSV file.\n\n    Args:\n        items (list): List of dicts with 'question' and 'response' keys.\n        task_instructions (str): Instructions for the evaluation task.\n        outfile (str): Output CSV file path.\n        model (str): Model name to use.\n\n    Returns:\n        None\n    \"\"\"\n    with open(outfile, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.DictWriter(f, fieldnames=[\n            \"timestamp\", \"model\", \"rubric_version\", \"prompt_version\",\n            \"question\", \"response\",\n            \"relevance\", \"coherence\", \"depth\", \"creativity\",\n            \"overall_score\", \"summary_feedback\"\n        ])\n        writer.writeheader()\n        for it in items:\n            q = it[\"question\"].strip()\n            r = it[\"response\"].strip()\n            if not r:\n                writer.writerow({\n                    \"timestamp\": datetime.utcnow().isoformat(),\n                    \"model\": model, \"rubric_version\": RUBRIC_VERSION, \"prompt_version\": PROMPT_VERSION,\n                    \"question\": q, \"response\": r,\n                    \"relevance\": \"\", \"coherence\": \"\", \"depth\": \"\", \"creativity\": \"\",\n                    \"overall_score\": \"\", \"summary_feedback\": \"Empty response\"\n                })\n                continue\n            result = evaluate_response(task_instructions, q, r, model=model)\n            if result is None:\n                writer.writerow({\n                    \"timestamp\": datetime.utcnow().isoformat(),\n                    \"model\": model, \"rubric_version\": RUBRIC_VERSION, \"prompt_version\": PROMPT_VERSION,\n                    \"question\": q, \"response\": r,\n                    \"relevance\": \"\", \"coherence\": \"\", \"depth\": \"\", \"creativity\": \"\",\n                    \"overall_score\": \"\", \"summary_feedback\": \"Invalid JSON\"\n                })\n                continue\n            row = {\n                \"timestamp\": datetime.utcnow().isoformat(),\n                \"model\": model,\n                \"rubric_version\": RUBRIC_VERSION,\n                \"prompt_version\": PROMPT_VERSION,\n                \"question\": q,\n                \"response\": r,\n                \"relevance\": result.criteria[\"relevance\"].score,\n                \"coherence\": result.criteria[\"coherence\"].score,\n                \"depth\": result.criteria[\"depth\"].score,\n                \"creativity\": result.criteria[\"creativity\"].score,\n                \"overall_score\": result.overall_score,\n                \"summary_feedback\": result.summary_feedback\n            }\n            writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Judge Consensus (Optional)\n\nRun multiple judges with different temperatures and average their scores. This reduces variance and mitigates idiosyncratic biases. For more on building robust multi-agent and consensus systems, see our guide to [multi-agent AI systems with CrewAI and YAML](/article/how-to-build-multi-agent-ai-systems-with-crewai-and-yaml-2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_with_consensus(task_instructions: str, question: str, response: str,\n                            judges=[{\"model\": \"gpt-4o-mini\", \"temperature\": 0.2},\n                                    {\"model\": \"gpt-4o-mini\", \"temperature\": 0.0},\n                                    {\"model\": \"gpt-4o-mini\", \"temperature\": 0.4}],\n                            few_shots=FEW_SHOTS) -> Optional[EvaluationResult]:\n    \"\"\"\n    Runs multiple LLM judges and averages their scores for consensus.\n\n    Args:\n        task_instructions (str): Instructions for the evaluation task.\n        question (str): The input prompt/question.\n        response (str): The candidate response to evaluate.\n        judges (list): List of judge configs (model, temperature).\n        few_shots (list): Few-shot examples for the prompt.\n\n    Returns:\n        Optional[EvaluationResult]: Consensus evaluation result, or None if all fail.\n    \"\"\"\n    results = []\n    for j in judges:\n        res = evaluate_response(task_instructions, question, response, few_shots, temperature=j[\"temperature\"], model=j[\"model\"])\n        if res:\n            results.append(res)\n    if not results:\n        return None\n    merged = {\n        \"criteria\": {},\n        \"overall_score\": mean([r.overall_score for r in results]),\n        \"summary_feedback\": max([r.summary_feedback for r in results], key=len)\n    }\n    for k in RUBRIC.keys():\n        merged[\"criteria\"][k] = {\n            \"score\": int(round(mean([r.criteria[k].score for r in results]))),\n            \"rationale\": max([r.criteria[k].rationale for r in results], key=len)\n        }\n    try:\n        return EvaluationResult(**merged)\n    except ValidationError:\n        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Validate\n\n### Evaluate a Small Dataset\n\nRun the evaluator on sample items and write results to CSV. This demonstrates the end-to-end flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_items = [\n    {\"question\": \"Write a headline for a privacy-first email app.\", \"response\": \"Email that minds its business.\"},\n    {\"question\": \"Write a headline for a privacy-first email app.\", \"response\": \"Advanced AI email assistant for sales teams.\"},\n    {\"question\": \"Write a headline for a privacy-first email app.\", \"response\": \"Your inbox, only yours.\"}\n]\n\nevaluate_batch(sample_items, \"Score headline quality.\", \"results.csv\")\nprint(\"Wrote results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect the first few rows of the output to verify structure and scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"results.csv\")\nprint(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validate Against Human Labels\n\nCompare AI scores to human labels using Spearman correlation and Cohen's kappa. This measures agreement and helps you tune the rubric or few-shots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validation_report(df: pd.DataFrame):\n    \"\"\"\n    Computes Spearman correlation and Cohen's kappa between human and AI scores.\n\n    Args:\n        df (pd.DataFrame): DataFrame with columns: human_* and ai_* for each criterion.\n\n    Returns:\n        dict: Metrics for each criterion and overall.\n    \"\"\"\n    metrics = {}\n    for k in [\"relevance\", \"coherence\", \"depth\", \"creativity\"]:\n        rho, p = spearmanr(df[f\"human_{k}\"], df[f\"ai_{k}\"], nan_policy=\"omit\")\n        metrics[f\"{k}_spearman\"] = round(rho, 3)\n        human_bins = df[f\"human_{k}\"].apply(lambda s: 0 if s <= 2 else 1 if s == 3 else 2)\n        ai_bins = df[f\"ai_{k}\"].apply(lambda s: 0 if s <= 2 else 1 if s == 3 else 2)\n        metrics[f\"{k}_kappa\"] = round(cohen_kappa_score(human_bins, ai_bins), 3)\n    rho_overall, _ = spearmanr(df[\"human_overall\"], df[\"ai_overall\"], nan_policy=\"omit\")\n    metrics[\"overall_spearman\"] = round(rho_overall, 3)\n    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load AI results and human labels, then compute validation metrics. Replace the example human labels with real annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ai = pd.read_csv(\"results.csv\")\nhuman = pd.DataFrame({\n    \"human_relevance\": [5, 2, 5],\n    \"human_coherence\": [4, 3, 5],\n    \"human_depth\": [3, 2, 3],\n    \"human_creativity\": [4, 2, 4],\n    \"human_overall\": [4.0, 2.2, 4.2]\n})\ndf = pd.concat([ai, human], axis=1)\nprint(validation_report(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detect Drift Over Time\n\nTrack score distributions and flag significant shifts. This helps you catch rubric drift, model updates, or data quality issues. To learn more about detecting and addressing model drift in production, check out our article on [context rot and LLM memory management](/article/context-rot-why-llms-forget-as-their-memory-grows-3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def drift_flags(scores: list[float], window: int = 200, z_thresh: float = 3.0):\n    \"\"\"\n    Detects drift in a rolling window of scores using z-score thresholding.\n\n    Args:\n        scores (list[float]): List of scores over time.\n        window (int): Size of the rolling window.\n        z_thresh (float): Z-score threshold for flagging drift.\n\n    Returns:\n        list: List of (index, z-score) tuples where drift is detected.\n    \"\"\"\n    if len(scores) < window * 2:\n        return []\n    flags = []\n    for i in range(window, len(scores)):\n        prev = np.array(scores[i-window:i])\n        mu, sigma = prev.mean(), prev.std() + 1e-6\n        z = (scores[i] - mu) / sigma\n        if abs(z) >= z_thresh:\n            flags.append((i, float(z)))\n    return flags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run drift detection on recent scores and print any flagged indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "recent_scores = df[\"overall_score\"].dropna().tolist()\nflags = drift_flags(recent_scores, window=2, z_thresh=2.0)\nif flags:\n    print(\"Drift detected at indices:\", flags)\nelse:\n    print(\"No drift detected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nYou now have a reference-free LLM evaluator with structured outputs, few-shot anchoring, Pydantic validation, and optional multi-judge consensus. The system logs every evaluation with metadata for tracking, validates against human labels, and detects drift over time.\n\nKey design decisions:\n\n1. **Low temperature and JSON mode** ensure deterministic, parsable outputs.\n2. **Pydantic validation** catches schema violations early and prevents silent failures.\n3. **Few-shot examples** anchor scoring behavior and reduce variance.\n4. **Multi-judge consensus** averages scores from multiple runs to mitigate bias.\n5. **Logging with versioning** enables drift detection and rubric iteration.\n\n### Next Steps\n\n1. **Deploy as an API**: Wrap the evaluator in a FastAPI service and deploy to a cloud provider.\n2. **Add caching**: Use a content-hash cache to avoid redundant API calls and reduce costs.\n3. **Weekly validation CI**: Automate validation against fresh human labels to catch rubric drift.\n4. **Monitoring dashboards**: Instrument metrics for Prometheus and build Grafana dashboards to track score distributions, latency, and error rates.\n\nFor a deeper dive into building robust prompts and ensuring reliable outputs, see our guide on [prompt engineering with LLM APIs](/article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-3). If you are unsure which LLM best fits your needs, our article on [how to pick an LLM for your app](/article/how-to-choose-an-ai-model-for-your-app-speed-cost-reliability) explores performance, efficiency, and pricing considerations."
      ]
    }
  ],
  "metadata": {
    "title": "LLM System Evaluation Without Gold Answers: A Practical How-To",
    "description": "Build a reference-free LLM grading system that scores open responses using rubrics, few-shots, and multi-judge consensus, plus validation, drift monitoring.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}