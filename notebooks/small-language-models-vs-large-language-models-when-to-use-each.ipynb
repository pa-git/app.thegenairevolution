{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Small Language Models vs Large Language Models: When to Use Each\n\n**Description:** Learn when small language models outperform large ones in production. This guide helps AI builders decide between SLMs and LLMs using quality thresholds, latency SLOs, and hybrid routing to cut inference cost while preserving accuracy and privacy.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Matters\n\nMost production GenAI systems overspend on compute and miss latency SLOs because teams default to the largest model available. But in practice, many tasksâ€”intent classification, schema extraction, summarization of structured dataâ€”don't require frontier reasoning. A 7B parameter model can match or exceed GPT-4 accuracy on narrow, well-defined problems while delivering 5â€“10Ã— lower latency and 20Ã— lower cost per request.\n\nThe core tradeoff is simple: **model size drives compute cost and latency, but task complexity determines the minimum capability required**. Understanding when a small language model (SLM) is sufficientâ€”and when to escalate to a large language model (LLM)â€”lets you hit p95 latency targets, control token spend, and satisfy compliance constraints without sacrificing quality.\n\nThis matters when:\n- Your p95 latency budget is under 200ms and LLM cold starts or token generation time push you over.\n- Token costs at scale (millions of requests/day) make GPT-4 class models economically unsustainable.\n- Privacy or compliance rules require on-premises or VPC-only inference, where hosting a 175B+ model is impractical.\n- Your task has a narrow input/output schema and can be validated programmatically, making SLM errors detectable and recoverable.\n\n## How It Works\n\n### Compute and Latency Scale with Parameters and Sequence Length\n\nEach forward pass through a transformer costs roughly **2 Ã— parameters Ã— tokens** FLOPs. A 7B model processing 512 tokens requires ~7 TFLOPs; a 70B model requires ~70 TFLOPsâ€”10Ã— more compute. On the same hardware, this translates directly to 10Ã— longer time-to-first-token (TTFT) and proportionally higher token generation latency.\n\nAttention mechanisms add **O(sequence_lengthÂ²)** memory and compute overhead. Doubling context from 2k to 4k tokens quadruples attention cost. The KV cacheâ€”which stores past token representationsâ€”grows linearly with sequence length and batch size, consuming GPU memory and limiting concurrency. Larger models amplify this: a 70B model's KV cache can exceed 40GB for a single 8k-token request, starving throughput.\n\n**Tail latency** (p95, p99) is driven by cold starts, KV cache evictions, and batch scheduling variance. SLMs fit entirely in a single GPU's memory, enabling faster cold starts and more predictable scheduling. LLMs often require multi-GPU inference with cross-device communication, adding 10â€“50ms per request and increasing p99 unpredictability.\n\n### Task Complexity Determines Minimum Capability\n\nNot all tasks require reasoning over ambiguous context or multi-step planning. **Structured tasks**â€”intent classification, entity extraction with a fixed schema, SQL generation from templates, summarization of tabular dataâ€”have narrow input distributions and deterministic success criteria. A fine-tuned 7B model can achieve >95% exact-match accuracy on these tasks because the solution space is constrained.\n\n**Open-ended tasks**â€”creative writing, complex multi-hop reasoning, ambiguous question answeringâ€”benefit from larger models' broader world knowledge and deeper reasoning. But even here, many requests fall into common patterns. A hybrid system can route simple queries to a SLM and escalate only when confidence is low or validation fails.\n\n### Specialization Closes the Gap\n\nSLMs reach task-specific quality thresholds through:\n- **Parameter-efficient fine-tuning (PEFT)**: LoRA adapters add <1% trainable parameters, letting you specialize a 7B model on 10kâ€“100k examples in hours on a single GPU. This is covered in depth in [Understanding LoRA and Parameter-Efficient Fine-Tuning](/article/understanding-lora-and-parameter-efficient-fine-tuning).\n- **Quantization**: INT8 or INT4 quantization reduces memory footprint by 2â€“4Ã—, enabling larger batch sizes and lower latency with <1% accuracy loss on many tasks. For practical guidance, see [A Practical Guide to Model Quantization](/article/practical-guide-to-model-quantization).\n- **Distillation**: Train a small model to mimic a large model's outputs, transferring task-specific behavior without the compute cost.\n\nA specialized 7B model often outperforms a general-purpose 70B model on narrow tasks because it has learned the exact input/output mapping without the noise of unrelated capabilities.\n\n### Privacy and Compliance Constraints Favor Smaller Models\n\nRegulatory requirements (GDPR, HIPAA, financial services) often prohibit sending data to third-party APIs. Hosting a 175B model on-premises requires 8Ã— A100 GPUs and complex orchestration. A quantized 7B model runs on a single GPU or even high-end CPUs, making VPC, edge, or air-gapped deployment feasible. For teams with strict data residency rules, SLMs are often the only practical option. Learn more about deployment tradeoffs in [Understanding Latency in LLM Inference](/article/understanding-latency-in-llm-inference).\n\n## What You Should Do\n\n### Define Task Profile and Acceptance Criteria\n\nCharacterize your task by:\n- **Input/output structure**: Fixed schema (JSON, SQL) or open-ended text?\n- **Quality threshold**: Exact-match accuracy, F1 score, or human eval pass rate?\n- **Latency SLO**: p95 target (e.g., <100ms TTFT, <500ms end-to-end)?\n- **Volume and cost**: Requests/day and acceptable cost per 1M tokens?\n\nExample: Intent classification for a chatbot with 20 intents, p95 <80ms, exact-match >95%, 10M requests/day. A fine-tuned 7B model is a strong candidate.\n\n### Implement SLM-First Hybrid Routing\n\nRoute requests to a SLM by default. Validate outputs programmatically (schema checks, confidence thresholds, rule-based heuristics). Escalate to a LLM only when validation fails or the task is flagged as high-risk.\n\nHere's a minimal routing heuristic in pseudocode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def route_request(input_text, task_type):\n    if task_type in [\"intent\", \"extraction\", \"sql\"]:\n        response = slm_inference(input_text)\n        if validate(response) and confidence(response) > 0.9:\n            return response\n    # Escalate to LLM\n    return llm_inference(input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This pattern is explored further in [Implementing Fallback Strategies for LLM Failures](/article/implementing-fallback-strategies-for-llm-failures). Cache validated SLM responses to avoid redundant LLM calls. Track escalation rate as a key metricâ€”if >20% of requests escalate, revisit SLM specialization or thresholds.\n\n### Specialize and Compress the SLM\n\nFine-tune a 7B model on 10kâ€“50k task-specific examples using LoRA. Apply INT8 quantization to reduce memory and improve throughput. Benchmark against your quality threshold and latency SLO. If the SLM meets both, deploy it as the primary model. If not, iterate on training data quality, prompt engineering, or escalation logic before scaling up model size.\n\n### Benchmark Under Realistic Load\n\nSimulate production traffic patterns: request rate, sequence length distribution, and concurrency. Measure:\n- **Tokens per request** (input + output) to estimate cost.\n- **p95 and p99 latency** to validate SLO compliance.\n- **Escalation rate** to quantify LLM fallback frequency.\n- **Throughput** (requests/sec) to size infrastructure.\n\nUse tools like Locust or k6 to generate load. Instrument with Prometheus and OpenTelemetry to track token counts and latency distributions in production. For a deeper dive into performance measurement, see [A Practical Guide to Benchmarking LLM Performance](/article/practical-guide-to-benchmarking-llm-performance).\n\n## Key Takeaways\n\nSmall language models outperform large ones when task complexity is low, latency and cost constraints are tight, and outputs can be validated programmatically. The decision hinges on three factors: compute cost scaling with parameters and sequence length, task-specific capability thresholds, and compliance requirements.\n\n**When to care:**\n- Your p95 latency target is <200ms and LLM token generation time exceeds it.\n- Token costs at scale make frontier models unsustainable.\n- Privacy or compliance rules require on-premises or VPC-only inference.\n- Your task has a narrow schema and deterministic success criteria.\n\nStart with a specialized SLM, validate outputs, and escalate to a LLM only when necessary. This hybrid approach delivers the quality of large models at the cost and latency of small ones."
      ]
    }
  ],
  "metadata": {
    "title": "Small Language Models vs Large Language Models: When to Use Each",
    "description": "Learn when small language models outperform large ones in production. This guide helps AI builders decide between SLMs and LLMs using quality thresholds, latency SLOs, and hybrid routing to cut inference cost while preserving accuracy and privacy.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}