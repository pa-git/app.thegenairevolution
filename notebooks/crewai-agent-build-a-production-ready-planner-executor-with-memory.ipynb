{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** CrewAI Agent: Build a Production-Ready Planner-Executor with Memory\n\n**Description:** Ship a production-ready CrewAI agent that plans tasks, validates tools, persists memory, and returns deterministic schema-validated JSON with automatic retries.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thought: I now can give a great answer\n\n---\n\n## Why This Approach Works\n\nIn production, LLM agents fail in predictable ways: they invent facts, forget past context, return inconsistent formats that break parsers, and call tools with wrong inputs. We'll explicitly engineer around those failure modes using planning, persistent memory, strict tool I/O schemas, and final JSON validation with automated retries. If you're curious about why LLMs tend to \"forget\" as their working memory grows, our article on [Context Rot - Why LLMs \"Forget\" as Their Memory Grows](/article/context-rot-why-llms-forget-as-their-memory-grows) provides a deeper explanation and practical mitigation strategies.\n\n**Key trade-offs:**\n\n- **Determinism vs flexibility**: We pin temperature to 0.0 for critical paths (planning, tool selection, finalization) to ensure reproducible outputs. This sacrifices creative variation but gains reliability and testability.\n- **Schema strictness vs ease of iteration**: Pydantic validation catches malformed outputs early, but requires upfront schema design. The payoff is fewer runtime surprises and cleaner debugging.\n- **Memory overhead vs context quality**: Persistent vector memory adds latency and storage, but prevents context loss across sessions and enables long-term learning.\n- **Custom orchestration vs framework magic**: We use CrewAI agents for role clarity but handle orchestration manually to maintain full control over validation, retries, and memory writes.\n\n---\n\n## How It Works (High-Level Overview)\n\n**Pipeline data flow:**\n\n1. **Input**: User goal (string)\n2. **Memory recall**: Query vector memory for relevant past context\n3. **Plan**: Planner agent generates a 3â€“7 step plan (validated JSON)\n4. **Execute loop**: For each step:\n   - Recall memory for step-specific context\n   - Executor agent selects one tool and arguments (validated JSON)\n   - Execute tool with schema validation\n   - Extract and persist memory writes\n5. **Finalize**: Finalizer agent assembles results, citations, and memory writes into a final answer (validated JSON)\n6. **Output**: FinalAnswer object with plan, results, answer text, citations, and memory writes\n\n**Core components:**\n\n- **VectorMemory (ChromaDB + OpenAI embeddings)**: Persistent storage and retrieval of facts\n- **Pydantic schemas**: Strict contracts for plans, tool calls, step results, and final answers\n- **LLM wrapper with retry logic**: Automatic JSON repair on validation failures\n- **Three agents (Planner, Executor, Finalizer)**: Role-based prompting for clarity\n- **Safe tools**: Tavily search, sandboxed calculator, HTTP GET JSON with timeout\n\n---\n\n## Setup & Installation\n\nRun this cell first to install dependencies with pinned versions for stability:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q chromadb==0.4.22 openai==1.12.0 pydantic==2.6.1 tavily-python==0.3.3 crewai==0.28.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your API keys securely in Colab. Use the secrets panel (key icon in left sidebar) to store `OPENAI_API_KEY` and `TAVILY_API_KEY`, then run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom google.colab import userdata\n\nos.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\nos.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")\nos.environ[\"OPENAI_MODEL\"] = \"gpt-4o-mini\"  # Pin model for determinism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step-by-Step Implementation\n\n### 1. Define Pydantic Schemas\n\nThese schemas enforce strict contracts for plans, tool calls, and outputs. Every LLM response must validate against one of these models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Literal, Optional, Dict, Any\nfrom pydantic import BaseModel, Field, HttpUrl\n\nclass PlanStep(BaseModel):\n    \"\"\"A single step in a plan.\"\"\"\n    id: int = Field(..., ge=1)\n    description: str\n    rationale: str\n\nclass Plan(BaseModel):\n    \"\"\"A plan consisting of multiple steps.\"\"\"\n    steps: List[PlanStep]\n\nclass SearchArgs(BaseModel):\n    \"\"\"Arguments for Tavily search tool.\"\"\"\n    query: str\n    max_results: int = Field(5, ge=1, le=10)\n\nclass CalcArgs(BaseModel):\n    \"\"\"Arguments for calculator tool.\"\"\"\n    expression: str\n\nclass HttpArgs(BaseModel):\n    \"\"\"Arguments for HTTP GET JSON tool.\"\"\"\n    url: HttpUrl\n    timeout: int = Field(10, ge=1, le=30)\n\nclass ToolCall(BaseModel):\n    \"\"\"Tool selection and arguments.\"\"\"\n    tool: Literal[\"tavily_search\", \"calculator\", \"http_get_json\"]\n    args: Dict[str, Any]\n\ndef validate_tool_args(tc: ToolCall):\n    \"\"\"Validate tool arguments against the correct schema.\"\"\"\n    if tc.tool == \"tavily_search\":\n        SearchArgs(**tc.args)\n    elif tc.tool == \"calculator\":\n        CalcArgs(**tc.args)\n    elif tc.tool == \"http_get_json\":\n        HttpArgs(**tc.args)\n    else:\n        raise ValueError(f\"Unknown tool: {tc.tool}\")\n\nclass StepResult(BaseModel):\n    \"\"\"Result of executing a plan step.\"\"\"\n    step_id: int\n    tool: str\n    input_args: Dict[str, Any]\n    output: Any\n    notes: Optional[str] = None\n    citations: List[str] = Field(default_factory=list)\n\nclass FinalAnswer(BaseModel):\n    \"\"\"Final answer schema for the agent.\"\"\"\n    plan: Plan\n    results: List[StepResult]\n    final_answer: str\n    citations: List[str] = Field(default_factory=list)\n    memory_writes: List[str] = Field(default_factory=list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Implement Safe Tools\n\nEach tool validates inputs and handles errors gracefully. The calculator uses AST parsing to prevent code injection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\nimport operator\nimport requests\nfrom tavily import TavilyClient\n\ntavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n\ndef tavily_search(args: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Perform a web search using Tavily.\"\"\"\n    params = SearchArgs(**args)\n    res = tavily_client.search(params.query, max_results=params.max_results)\n    items = []\n    for r in res.get(\"results\", []):\n        items.append({\n            \"title\": r.get(\"title\"),\n            \"url\": r.get(\"url\"),\n            \"content\": r.get(\"content\")\n        })\n    return {\"results\": items[:params.max_results]}\n\n# Allowed AST operations for safe calculator\n_ALLOWED_OPS = {\n    ast.Add: operator.add,\n    ast.Sub: operator.sub,\n    ast.Mult: operator.mul,\n    ast.Div: operator.truediv,\n    ast.Pow: operator.pow,\n    ast.Mod: operator.mod,\n    ast.USub: operator.neg,\n}\n\ndef _eval_expr(node):\n    \"\"\"Safely evaluate a mathematical expression AST node.\"\"\"\n    if isinstance(node, ast.Num):\n        return node.n\n    if isinstance(node, ast.Constant):\n        return node.value\n    if isinstance(node, ast.UnaryOp) and type(node.op) in _ALLOWED_OPS:\n        return _ALLOWED_OPS[type(node.op)](_eval_expr(node.operand))\n    if isinstance(node, ast.BinOp) and type(node.op) in _ALLOWED_OPS:\n        return _ALLOWED_OPS[type(node.op)](_eval_expr(node.left), _eval_expr(node.right))\n    raise ValueError(\"Unsupported expression\")\n\ndef calculator(args: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Safely evaluate a mathematical expression.\"\"\"\n    params = CalcArgs(**args)\n    tree = ast.parse(params.expression, mode=\"eval\")\n    value = _eval_expr(tree.body)\n    return {\"value\": value}\n\ndef http_get_json(args: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Fetch JSON from a URL with timeout.\"\"\"\n    params = HttpArgs(**args)\n    resp = requests.get(str(params.url), timeout=params.timeout)\n    resp.raise_for_status()\n    content_type = resp.headers.get(\"Content-Type\", \"\")\n    if \"application/json\" in content_type:\n        data = resp.json()\n    else:\n        data = {\"text\": resp.text[:2000]}  # Truncate to avoid memory bloat\n    return {\n        \"status\": resp.status_code,\n        \"headers\": dict(resp.headers),\n        \"data\": data\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Build LLM Wrapper with Retry Logic\n\nThis wrapper prompts the LLM for JSON output and automatically repairs malformed responses up to 3 times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\nimport logging\nfrom typing import Type, Any\nfrom openai import OpenAI\n\nlogging.basicConfig(level=logging.INFO)\nclient = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\nOPENAI_MODEL = os.environ[\"OPENAI_MODEL\"]\n\ndef chat(system: str, user: str, temperature: float = 0.0) -> str:\n    \"\"\"Send a chat completion request to OpenAI.\"\"\"\n    resp = client.chat.completions.create(\n        model=OPENAI_MODEL,\n        temperature=temperature,\n        messages=[\n            {\"role\": \"system\", \"content\": system},\n            {\"role\": \"user\", \"content\": user},\n        ],\n    )\n    return resp.choices[0].message.content\n\ndef json_prompt(model: Type[BaseModel], system: str, user: str, max_retries: int = 3) -> Any:\n    \"\"\"\n    Prompt LLM for JSON output conforming to a Pydantic schema, with repair loop.\n    Retries up to max_retries times if validation fails.\n    \"\"\"\n    hint = f\"Return ONLY valid JSON that conforms to this schema: {model.model_json_schema()}.\"\n    last_err = None\n    content = \"\"\n    for attempt in range(max_retries):\n        content = chat(system, f\"{user}\\n\\n{hint}\", temperature=0.0)\n        try:\n            data = json.loads(content)\n            return model.model_validate(data)\n        except Exception as e:\n            last_err = e\n            logging.warning(f\"JSON validation failed (attempt {attempt+1}): {e}\")\n            # Attempt repair\n            repair = chat(\n                \"You are a JSON repair tool. Fix to valid JSON exactly.\",\n                f\"Schema:\\n{model.model_json_schema()}\\n\\nInvalid JSON:\\n{content}\\n\\nError:\\n{last_err}\\n\\nReturn only corrected JSON.\"\n            )\n            try:\n                data = json.loads(repair)\n                return model.model_validate(data)\n            except Exception as e2:\n                last_err = e2\n                logging.warning(f\"JSON repair failed (attempt {attempt+1}): {e2}\")\n                continue\n    raise ValueError(f\"Failed to produce valid JSON after retries. Last error: {last_err}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Implement Persistent Vector Memory\n\nWe'll persist short-term and long-term memory into a Chroma DB directory. Long-term facts become retrievable context across sessions. We'll use OpenAI's text-embedding-3-small for cost-effective memory indexing and querying. For more on how the placement of critical information in prompts affects LLM recall, see our guide on [Lost in the Middle: Placing Critical Info in Long Prompts](/article/lost-in-the-middle-placing-critical-info-in-long-prompts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\nimport chromadb\nfrom chromadb.utils import embedding_functions\n\nclass VectorMemory:\n    \"\"\"\n    Persistent vector memory using ChromaDB and OpenAI embeddings.\n    Stores and retrieves facts across sessions.\n    \"\"\"\n    def __init__(self, path: str = \"./memory_db\", collection: str = \"crew_memory\"):\n        self.client = chromadb.PersistentClient(path=path)\n        self.embedding_fn = embedding_functions.OpenAIEmbeddingFunction(\n            api_key=os.environ[\"OPENAI_API_KEY\"],\n            model_name=\"text-embedding-3-small\"\n        )\n        self.collection = self.client.get_or_create_collection(\n            name=collection,\n            embedding_function=self.embedding_fn\n        )\n\n    def remember(self, text: str, meta: Dict[str, Any] | None = None) -> str:\n        \"\"\"Store a text and optional metadata in vector memory.\"\"\"\n        doc_id = str(uuid.uuid4())\n        self.collection.upsert(\n            documents=[text],\n            metadatas=[meta or {}],\n            ids=[doc_id],\n        )\n        return doc_id\n\n    def recall(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"Retrieve top-k relevant memories for a query.\"\"\"\n        res = self.collection.query(query_texts=[query], n_results=k)\n        results = []\n        for doc, meta, doc_id in zip(\n            res.get(\"documents\", [[]])[0],\n            res.get(\"metadatas\", [[]])[0],\n            res.get(\"ids\", [[]])[0]\n        ):\n            results.append({\"id\": doc_id, \"text\": doc, \"meta\": meta})\n        return results\n\ndef memory_to_context(mem: VectorMemory, query: str, k: int = 5) -> str:\n    \"\"\"Retrieve top-k relevant memory entries as context.\"\"\"\n    hits = mem.recall(query, k=k)\n    lines = [f\"- {h['text']}\" for h in hits]\n    return \"\\n\".join(lines) if lines else \"None\"\n\ndef extract_memory_writes(result: StepResult) -> List[str]:\n    \"\"\"Extract memory write candidates from a step result.\"\"\"\n    writes = []\n    if result.tool == \"tavily_search\":\n        for r in result.output.get(\"results\", []):\n            title, url = r.get(\"title\"), r.get(\"url\")\n            if title and url:\n                writes.append(f\"{title} -> {url}\")\n    if result.tool == \"http_get_json\":\n        url = result.input_args.get(\"url\")\n        status = result.output.get(\"status\")\n        if url and status == 200:\n            writes.append(f\"Fetched JSON OK from {url}\")\n    return writes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Define CrewAI Agents\n\nThese agents provide role-based prompting for planning, execution, and finalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from crewai import Agent\n\nplanner_agent = Agent(\n    role=\"Planner\",\n    goal=\"Decompose complex requests into minimal, sequential steps to solve the user goal.\",\n    backstory=\"You are a meticulous project planner who writes terse, unambiguous plans.\",\n    verbose=True,\n    allow_delegation=False,\n)\n\nexecutor_agent = Agent(\n    role=\"Executor\",\n    goal=\"For each step, choose one tool and specify exact arguments to complete the step.\",\n    backstory=\"You are precise and only call tools you truly need, with correct arguments.\",\n    verbose=True,\n    allow_delegation=False,\n)\n\nfinalizer_agent = Agent(\n    role=\"Finalizer\",\n    goal=\"Assemble the results and write a concise, accurate final answer with citations.\",\n    backstory=\"You summarize clearly, include sources, and keep to the requested schema.\",\n    verbose=True,\n    allow_delegation=False,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Implement Planner\n\nThe Planner agent generates a structured plan with 3â€“7 steps, each validated against the Plan schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PLAN_SYSTEM = (\n    \"You break down goals into numbered steps with minimal scope per step. \"\n    \"Prefer quick wins. Avoid redundant steps. Use 3-7 steps.\"\n)\n\ndef make_plan(user_goal: str, memory_context: str) -> Plan:\n    \"\"\"Prompt the Planner agent to create a Plan.\"\"\"\n    user = (\n        f\"User goal:\\n{user_goal}\\n\\n\"\n        f\"Relevant memory:\\n{memory_context}\\n\\n\"\n        \"Output a Plan with steps: each has id, description, rationale.\"\n    )\n    return json_prompt(Plan, PLAN_SYSTEM, user)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Implement Executor with Auto-Correction\n\nThe Executor selects and executes tools. If a tool fails, it retries with corrected arguments up to 3 times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TOOL_DOC = \"\"\"\nAvailable tools:\n- tavily_search(query: str, max_results: int=5)\n- calculator(expression: str)\n- http_get_json(url: http(s) URL, timeout: int=1..30)\n\"\"\"\n\nEXEC_SYSTEM = (\n    \"You are a strict tool selector. For the provided step, return exactly one tool to call and its args.\"\n)\n\ndef select_tool(step_text: str, context: str) -> ToolCall:\n    \"\"\"Prompt the Executor agent to select a tool and arguments.\"\"\"\n    user = (\n        f\"Step:\\n{step_text}\\n\\nContext:\\n{context}\\n\\n\"\n        f\"{TOOL_DOC}\\n\\nReturn a ToolCall.\"\n    )\n    return json_prompt(ToolCall, EXEC_SYSTEM, user)\n\ndef run_tool(tc: ToolCall) -> StepResult:\n    \"\"\"Validate and execute the selected tool.\"\"\"\n    validate_tool_args(tc)\n    if tc.tool == \"tavily_search\":\n        out = tavily_search(tc.args)\n        cits = [r[\"url\"] for r in out.get(\"results\", []) if r.get(\"url\")]\n    elif tc.tool == \"calculator\":\n        out = calculator(tc.args)\n        cits = []\n    elif tc.tool == \"http_get_json\":\n        out = http_get_json(tc.args)\n        cits = []\n    else:\n        raise ValueError(f\"Unknown tool: {tc.tool}\")\n    return StepResult(\n        step_id=0,\n        tool=tc.tool,\n        input_args=tc.args,\n        output=out,\n        citations=cits\n    )\n\ndef robust_execute(step_desc: str, context: str, max_attempts: int = 3) -> StepResult:\n    \"\"\"\n    Execute a step with automatic correction on tool failure.\n    Retries up to max_attempts times if tool execution fails.\n    \"\"\"\n    last_error = None\n    for attempt in range(max_attempts):\n        error_hint = f\"\\nPrevious error: {last_error}\" if last_error else \"\"\n        tc = select_tool(f\"{step_desc}{error_hint}\", context)\n        try:\n            return run_tool(tc)\n        except Exception as e:\n            last_error = str(e)\n            logging.warning(f\"Tool execution failed (attempt {attempt+1}): {last_error}\")\n            continue\n    raise RuntimeError(f\"Execution failed after retries. Last error: {last_error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Implement Finalizer\n\nThe Finalizer assembles results, citations, and memory writes into a validated FinalAnswer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FINALIZE_SYSTEM = (\n    \"You produce concise, accurate answers. Include citations whenever you relied on sources.\"\n)\n\ndef finalize(plan: Plan, results: List[StepResult], user_goal: str, memory_writes: List[str]) -> FinalAnswer:\n    \"\"\"Prompt the Finalizer agent to assemble the final answer.\"\"\"\n    user = (\n        f\"User goal:\\n{user_goal}\\n\\n\"\n        f\"Plan:\\n{plan.model_dump_json(indent=2)}\\n\\n\"\n        f\"Results:\\n[{', '.join(r.model_dump_json() for r in results)}]\\n\\n\"\n        \"Compose final_answer (concise but complete), gather unique citations, \"\n        \"and include memory_writes (as provided) in the output.\"\n    )\n    return json_prompt(FinalAnswer, FINALIZE_SYSTEM, user)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Orchestrate the Full Pipeline\n\nThis function runs the plannerâ€“executorâ€“finalizer pipeline with memory and logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_agent(user_goal: str) -> FinalAnswer:\n    \"\"\"\n    Run the full agent pipeline: plan, execute, finalize.\n    Returns a validated FinalAnswer with plan, results, answer, citations, and memory writes.\n    \"\"\"\n    mem = VectorMemory()\n    \n    # 1) Recall memory for planning\n    plan_context = memory_to_context(mem, user_goal, k=5)\n    plan = make_plan(user_goal, plan_context)\n    logging.info(f\"Plan created with {len(plan.steps)} steps\")\n\n    # 2) Execute steps with auto-correction\n    step_results: List[StepResult] = []\n    pending_writes: List[str] = []\n    for step in plan.steps:\n        ctx = memory_to_context(mem, f\"{user_goal} | {step.description}\", k=5)\n        sr = robust_execute(step.description, ctx)\n        sr.step_id = step.id\n        step_results.append(sr)\n        \n        # Memory writes\n        writes = extract_memory_writes(sr)\n        for w in writes:\n            mem.remember(w, meta={\"step_id\": step.id, \"goal\": user_goal})\n        pending_writes.extend(writes)\n        logging.info(f\"Step {step.id}: tool={sr.tool}, writes={writes}\")\n\n    # 3) Finalize\n    answer = finalize(plan, step_results, user_goal, pending_writes)\n    logging.info(\"Final answer assembled\")\n    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Run and Validate\n\nRun the agent with a realistic goal and print the validated JSON output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "goal = \"Compare the current price of Bitcoin to last week's average and compute the percentage change.\"\nresult = run_agent(goal)\nprint(result.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected output structure:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```json\n{\n  \"plan\": {\n    \"steps\": [\n      {\"id\": 1, \"description\": \"Search for current Bitcoin price\", \"rationale\": \"...\"},\n      {\"id\": 2, \"description\": \"Search for last week's average Bitcoin price\", \"rationale\": \"...\"},\n      {\"id\": 3, \"description\": \"Calculate percentage change\", \"rationale\": \"...\"}\n    ]\n  },\n  \"results\": [\n    {\"step_id\": 1, \"tool\": \"tavily_search\", \"input_args\": {...}, \"output\": {...}, \"citations\": [...]},\n    {\"step_id\": 2, \"tool\": \"tavily_search\", \"input_args\": {...}, \"output\": {...}, \"citations\": [...]},\n    {\"step_id\": 3, \"tool\": \"calculator\", \"input_args\": {...}, \"output\": {...}, \"citations\": []}\n  ],\n  \"final_answer\": \"Bitcoin is currently $X. Last week's average was $Y. The percentage change is Z%.\",\n  \"citations\": [\"https://...\", \"https://...\"],\n  \"memory_writes\": [\"Bitcoin price $X -> https://...\", \"Last week average $Y -> https://...\"]\n}\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Test memory persistence:**\n\nRun a second query that relies on the first query's memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "goal2 = \"What was the Bitcoin price I looked up earlier?\"\nresult2 = run_agent(goal2)\nprint(result2.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The agent should recall the Bitcoin price from memory without re-searching.\n\n**Test auto-correction:**\n\nForce a tool failure by sabotaging the input, then observe retry logs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This will fail and retry with corrected arguments\ngoal3 = \"Calculate the result of 10 divided by zero\"\ntry:\n    result3 = run_agent(goal3)\nexcept Exception as e:\n    print(f\"Expected failure: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Conclusion\n\nYou've built a production-oriented plannerâ€“executor agent with persistent memory, schema-validated JSON, and automatic error correction. The system decomposes goals into steps, selects and executes tools with strict validation, persists learnings across sessions, and assembles final answers with citations.\n\n**Key decisions:**\n\n- **Pydantic schemas** enforce strict contracts and catch errors early\n- **Persistent vector memory** prevents context loss and enables long-term learning\n- **Automatic retry loops** handle LLM hallucinations and tool failures gracefully\n- **Temperature 0.0** ensures deterministic, reproducible outputs for testing\n\nFor deterministic behavior, pin your OpenAI model (e.g., gpt-4o-mini) and run with temperature 0.0 in json_prompt and critical prompts. This reduces drift in tool selections and JSON structure and makes test snapshots reliable across runs and environments. If you're weighing the tradeoffs between deploying small versus large language models in production, our article [Small vs Large Language Models](/article/small-language-models-vs-large-language-models-when-to-use-each) explores when each is most effective, including cost and latency considerations.\n\n**Next steps:**\n\n- **Harden the service endpoint**: Wrap `run_agent` in a FastAPI route with rate limits, authentication, and request validation\n- **Add structured logging**: Use Python's logging module with JSON formatters and correlation IDs to trace plan â†’ tool calls â†’ retries â†’ memory writes\n- **Keep the tool catalog minimal**: Start with 3â€“5 tools and expand only when you have clear use cases. Each tool adds validation overhead and increases the chance of incorrect selections\n- **Integrate CrewAI hierarchical process**: Use CrewAI's Task and Crew to manage agent coordination if you need more complex workflows. See the optional snippet below for a starting point\n- **Monitor and version schemas**: Track schema changes in version control and test backward compatibility when updating Pydantic models\n- **Add OAuth tool integrations**: Extend tools to support authenticated APIs (e.g., Google Calendar, Slack) with secure token management\n\n**Optional: CrewAI hierarchical process**\n\nIf you want to use CrewAI's built-in task management, define tasks and a crew:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from crewai import Crew, Process, Task\n\nplan_task = Task(\n    description=\"Create a 3-7 step plan for the user goal with minimal steps.\",\n    expected_output=\"A JSON plan with steps: id, description, rationale.\",\n    agent=planner_agent,\n)\n\ncrew = Crew(\n    agents=[planner_agent, executor_agent, finalizer_agent],\n    tasks=[plan_task],\n    process=Process.hierarchical,\n    verbose=True,\n)\n\n# crew.kickoff()  # Use the validation+memory wrappers shown above for production output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This approach delegates orchestration to CrewAI but requires adapting the validation and memory logic to fit CrewAI's task output format."
      ]
    }
  ],
  "metadata": {
    "title": "CrewAI Agent: Build a Production-Ready Planner-Executor with Memory",
    "description": "Ship a production-ready CrewAI agent that plans tasks, validates tools, persists memory, and returns deterministic schema-validated JSON with automatic retries.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}