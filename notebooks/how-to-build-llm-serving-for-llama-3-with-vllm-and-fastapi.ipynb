{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build LLM Serving for Llama 3 with vLLM and FastAPI\n\n**Description:** Deploy production-grade LLM serving fast: self-host Llama 3 on vLLM with KV cache batching, FastAPI auth, rate limits, streaming, benchmarks.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What You'll Build\n\nA production-grade FastAPI gateway that sits in front of vLLM, adding authentication, rate limiting, streaming, and Prometheus metrics. By the end, you'll have a working endpoint on port 8080 proxying to vLLM on 8001, validated with curl and exposing metrics on `/metrics`. You'll measure time-to-first-token (TTFT) and tokens/sec under load, giving you a baseline for optimization.\n\nThis setup takes hours, not weeks, and unlocks the control you need to scale LLM inference in production.\n\n## Prerequisites\n\n- **Hardware**: NVIDIA GPU with 24+ GB VRAM (e.g., A100, L4, RTX 4090) for Llama 3 8B; 70B requires multi-GPU with NVLink\n- **Software**: CUDA 12.1+, Docker, Python 3.10+, Redis\n- **Model**: Meta Llama 3 8B Instruct (or 70B with tensor parallelism)\n- **Knowledge**: Basic Python, async/await, REST APIs, and familiarity with Docker\n\n## Why This Approach Works\n\nvLLM handles batching, paged attention, and GPU optimization. Your gateway adds the production layer: auth, rate limits, observability, and streaming. This separation keeps vLLM focused on inference speed while your gateway enforces policy and collects metrics. The result is a scalable, observable LLM service that integrates cleanly with existing infrastructure.\n\n## How It Works (High-Level Overview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Client â†’ FastAPI Gateway (auth, rate limit, validation, metrics) â†’ vLLM (OpenAI-compatible API) â†’ GPU\n                â†“\n         Prometheus scrapes /metrics\n         Redis tracks rate limits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gateway validates API keys, enforces per-key rate limits via Redis, proxies requests to vLLM's OpenAI-compatible endpoint, streams responses as Server-Sent Events (SSE), and exposes Prometheus metrics for TTFT, tokens/sec, and GPU utilization.\n\n## Step 1: Install vLLM and Launch the Server\n\nInstall vLLM with CUDA support:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install vllm==0.4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Launch vLLM with Llama 3 8B Instruct on a single GPU, setting max context to 8192 tokens and reserving 90% of GPU memory for inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vllm serve meta-llama/Meta-Llama-3-8B-Instruct \\\n  --host 0.0.0.0 \\\n  --port 8001 \\\n  --max-model-len 8192 \\\n  --gpu-memory-utilization 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hardware tuning presets**:\n- **A100 40GB**: `--gpu-memory-utilization 0.9 --max-model-len 8192` (supports ~16 concurrent requests)\n- **L4 24GB**: `--gpu-memory-utilization 0.85 --max-model-len 4096` (tighter memory, shorter context)\n- **RTX 4090 24GB**: `--gpu-memory-utilization 0.85 --max-model-len 4096` (similar to L4)\n- **70B multi-GPU**: `--tensor-parallel-size 4 --gpu-memory-utilization 0.9 --max-model-len 8192` (requires 4x A100 80GB with NVLink)\n\nVerify the server is running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "curl http://localhost:8001/v1/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see `meta-llama/Meta-Llama-3-8B-Instruct` in the response.\n\nFor a deeper dive into how to evaluate and select the right LLM for your applicationâ€”including considerations like context length, hardware efficiency, and costâ€”see our guide on [how to pick an LLM](/article/how-to-choose-an-ai-model-for-your-app-speed-cost-reliability).\n\n## Step 2: Set Up Redis for Rate Limiting\n\nStart Redis via Docker:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docker run -d -p 6379:6379 redis:7-alpine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify Redis is reachable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "redis-cli ping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see `PONG`.\n\n## Step 3: Build the FastAPI Gateway\n\nInstall dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install fastapi==0.110.0 uvicorn==0.27.0 httpx==0.27.0 \\\n  starlette-limiter==0.2.0 redis==5.0.1 prometheus-client==0.20.0 pynvml==11.5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create `app/main.py`. This file defines the FastAPI gateway with auth, rate limiting, streaming, and metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport time\nimport json\nimport asyncio\nfrom typing import AsyncGenerator, Dict, Any, Optional\n\nimport httpx\nfrom fastapi import FastAPI, HTTPException, Depends, Header, Request\nfrom fastapi.responses import StreamingResponse, JSONResponse, PlainTextResponse\nfrom prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST\nfrom starlette.middleware.cors import CORSMiddleware\nfrom starlette_limiter import Limiter\nfrom redis.asyncio import from_url\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"llm-gateway\")\n\nVLLM_URL = os.getenv(\"VLLM_URL\", \"http://localhost:8001\")\nALLOWED_KEYS = set(os.getenv(\"API_KEYS\", \"dev-key-1,dev-key-2\").split(\",\"))\nREDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n\napp = FastAPI(title=\"LLM Gateway\", version=\"1.0.0\")\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"]\n)\n\nredis = from_url(REDIS_URL, encoding=\"utf-8\", decode_responses=True)\nlimiter = Limiter(\n    key_func=lambda request: request.headers.get(\"x-api-key\", \"unknown\"),\n    storage=redis\n)\n\nREQS = Counter(\"llm_requests_total\", \"Total LLM requests\", [\"route\"])\nTTFT = Histogram(\"llm_ttft_seconds\", \"Time to first token\", buckets=(0.05, 0.1, 0.2, 0.5, 1, 2, 5))\nTOKENS = Counter(\"llm_output_tokens_total\", \"Output tokens total\")\nTPS = Histogram(\"llm_tps\", \"Tokens per second\", buckets=(5, 10, 20, 40, 80, 160, 320))\nGPU_UTIL = Gauge(\"gpu_utilization_percent\", \"GPU utilization percent\")\nGPU_MEM = Gauge(\"gpu_memory_used_mb\", \"GPU memory used MB\")\n\nasync def require_key(x_api_key: Optional[str] = Header(default=None)):\n    if x_api_key is None or x_api_key not in ALLOWED_KEYS:\n        logger.warning(\"Unauthorized access attempt with API key: %s\", x_api_key)\n        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n\nasync def stream_vllm(payload: Dict[str, Any]) -> AsyncGenerator[bytes, None]:\n    url = f\"{VLLM_URL}/v1/chat/completions\"\n    payload = {**payload, \"stream\": True}\n    t0 = time.perf_counter()\n    first = True\n    token_count = 0\n    async with httpx.AsyncClient(timeout=None) as client:\n        async with client.stream(\"POST\", url, json=payload) as r:\n            async for line in r.aiter_lines():\n                if not line:\n                    continue\n                if line.startswith(\"data:\"):\n                    data = line[5:].strip()\n                    if data == \"[DONE]\":\n                        break\n                    if first:\n                        ttft = time.perf_counter() - t0\n                        TTFT.observe(ttft)\n                        logger.info(\"TTFT observed: %.3fs\", ttft)\n                        first = False\n                    try:\n                        obj = json.loads(data)\n                        delta = obj[\"choices\"][0].get(\"delta\", {}).get(\"content\", \"\")\n                        token_count += len(delta.split())\n                    except Exception as e:\n                        logger.debug(\"Failed to parse SSE data: %s\", e)\n                    yield (f\"{line}\\n\").encode(\"utf-8\")\n    elapsed = max(time.perf_counter() - t0, 1e-6)\n    TOKENS.inc(token_count)\n    TPS.observe(token_count / elapsed)\n    logger.info(\"Streamed %d tokens in %.2fs (%.2f TPS)\", token_count, elapsed, token_count / elapsed)\n\n@app.post(\"/v1/chat/completions\")\n@limiter.limit(\"60/minute\")\nasync def chat_completions(req: Request, key=Depends(require_key)):\n    body = await req.json()\n    REQS.labels(route=\"/v1/chat/completions\").inc()\n    return StreamingResponse(stream_vllm(body), media_type=\"text/event-stream\")\n\n@app.get(\"/healthz\")\nasync def health():\n    return {\"status\": \"ok\"}\n\n@app.get(\"/metrics\")\nasync def metrics():\n    try:\n        import pynvml\n        h = pynvml.nvmlDeviceGetHandleByIndex(0)\n        util = pynvml.nvmlDeviceGetUtilizationRates(h)\n        mem = pynvml.nvmlDeviceGetMemoryInfo(h)\n        GPU_UTIL.set(util.gpu)\n        GPU_MEM.set(mem.used / (1024 * 1024))\n    except Exception as e:\n        logger.debug(\"GPU metrics update failed: %s\", e)\n    return PlainTextResponse(generate_latest(), media_type=CONTENT_TYPE_LATEST)\n\n@app.on_event(\"startup\")\nasync def startup():\n    try:\n        import pynvml\n        pynvml.nvmlInit()\n        logger.info(\"NVML initialized for GPU metrics.\")\n    except Exception as e:\n        logger.warning(\"NVML initialization failed: %s\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run the Gateway\n\nStart the gateway with Uvicorn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "uvicorn app.main:app --host 0.0.0.0 --port 8080 --workers 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: For production with multiple workers, enable Prometheus multiprocess mode by setting `PROMETHEUS_MULTIPROC_DIR` and using `MultiProcessCollector`. Single-worker mode is sufficient for initial testing.\n\n## Step 5: Test the Gateway\n\nSend a streaming request with a valid API key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "curl -X POST http://localhost:8080/v1/chat/completions \\\n  -H \"x-api-key: dev-key-1\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Explain vector databases in one paragraph.\"}],\n    \"max_tokens\": 256,\n    \"temperature\": 0.2,\n    \"stream\": true\n  }'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see a stream of `data:` lines with incremental tokens.\n\nTest authentication by omitting the API key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "curl -X POST http://localhost:8080/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n    \"max_tokens\": 50\n  }'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should receive a `401 Unauthorized` response.\n\nTest rate limiting by sending 61 requests in quick succession:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in {1..61}; do\n  curl -X POST http://localhost:8080/v1/chat/completions \\\n    -H \"x-api-key: dev-key-1\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": \"Hi\"}], \"max_tokens\": 10}' &\ndone\nwait"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After 60 requests, subsequent requests should return `429 Rate limit exceeded`.\n\nCheck Prometheus metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "curl http://localhost:8080/metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Look for `llm_requests_total`, `llm_ttft_seconds`, `llm_output_tokens_total`, `llm_tps`, `gpu_utilization_percent`, and `gpu_memory_used_mb`. Verify that `llm_requests_total` increments with each request and `llm_ttft_seconds` records TTFT observations.\n\nIf you're working with especially long prompts, be aware of position bias and how models may miss critical detailsâ€”our article on [placing critical info in long prompts](/article/lost-in-the-middle-placing-critical-info-in-long-prompts) offers practical advice to mitigate these issues.\n\n## Step 6: Benchmark Throughput and Latency\n\nCreate `tools/quick_bench.py` to measure concurrent throughput and TTFT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\nimport time\nimport httpx\nimport os\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"quick_bench\")\n\nGATEWAY = os.getenv(\"GATEWAY\", \"http://localhost:8080\")\nHEADERS = {\n    \"x-api-key\": os.getenv(\"API_KEY\", \"dev-key-1\"),\n    \"Content-Type\": \"application/json\"\n}\nPAYLOAD = {\n    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"In one paragraph, explain vector databases.\"}],\n    \"max_tokens\": 256,\n    \"temperature\": 0.2,\n    \"stream\": True\n}\n\nasync def run_once():\n    t0 = time.perf_counter()\n    first = True\n    tokens = 0\n    async with httpx.AsyncClient(timeout=None) as c:\n        async with c.stream(\"POST\", f\"{GATEWAY}/v1/chat/completions\", json=PAYLOAD, headers=HEADERS) as r:\n            async for line in r.aiter_lines():\n                if line and line.startswith(\"data:\"):\n                    d = line[5:].strip()\n                    if d == \"[DONE]\":\n                        break\n                    if first:\n                        ttft = time.perf_counter() - t0\n                        logger.info(\"TTFT: %.3fs\", ttft)\n                        first = False\n                    tokens += len(d.split())\n    elapsed = time.perf_counter() - t0\n    logger.info(\"Tokens: %d, TPS: %.2f\", tokens, tokens / elapsed)\n\nasync def main(iters: int = 8):\n    tasks = [run_once() for _ in range(iters)]\n    await asyncio.gather(*tasks)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the benchmark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "python tools/quick_bench.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see TTFT and tokens/sec logged for each request. On an A100 40GB with Llama 3 8B, expect TTFT < 0.2s and throughput > 100 tokens/sec per request under light load.\n\nRemember, LLM context is not infiniteâ€”if you notice increased hallucinations or inconsistent responses as context grows, you may be running into [context rot](/article/context-rot-why-llms-forget-as-their-memory-grows-3), where models \"forget\" earlier information as their memory window expands.\n\n## Step 7: Tune vLLM for Your Workload\n\nAdjust `--max-model-len` based on your prompt and output length requirements. Increase cautiously: KV cache grows roughly linearly with sequence length and batch size. Start with `--gpu-memory-utilization 0.9` and adjust based on OOM events and throughput measurements.\n\nFor multi-GPU setups with 70B models, add `--tensor-parallel-size N` where N is the number of GPUs. Ensure NVLink is enabled for optimal inter-GPU bandwidth.\n\nRe-run benchmarks after each change to measure the impact on TTFT and tokens/sec.\n\n## Conclusion\n\nYou've built a production-grade LLM gateway with authentication, rate limiting, streaming, and observability. You can now measure TTFT, tokens/sec, and GPU utilization, giving you the data to optimize for your workload. Key trade-offs: single-worker mode simplifies metrics but limits concurrency; FP8 quantization can boost throughput but may reduce quality; longer context increases memory pressure.\n\n**Next steps**:\n- Enable Prometheus multiprocess mode for multi-worker deployments\n- Add input validation with Pydantic models to enforce max_tokens and temperature ranges\n- Integrate TLS and rotate API keys via a secret manager\n- Deploy behind NGINX or Envoy with SSE-compatible config (disable buffering, set `Cache-Control: no-cache`)\n- Explore FP8 quantization with `--quantization fp8` for higher throughput on supported GPUs"
      ]
    }
  ],
  "metadata": {
    "title": "How to Build LLM Serving for Llama 3 with vLLM and FastAPI",
    "description": "Deploy production-grade LLM serving fast: self-host Llama 3 on vLLM with KV cache batching, FastAPI auth, rate limits, streaming, benchmarks.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}