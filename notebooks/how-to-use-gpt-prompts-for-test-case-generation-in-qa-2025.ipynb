{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Use GPT Prompts for Test Case Generation in QA [2025]\n\n**Description:** Generate comprehensive, reliable tests in minutes using proven GPT prompts, BDD templates, and tool integrationsâ€”cut flakiness, expand coverage, accelerate releases.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generating test scenarios from acceptance criteria often produces shallow, happy-path-only outputs that ignore edge cases, security risks, and format requirements. This happens because instruction dilution and missing delimiters cause format contamination and incomplete coverage.\n\nThis guide shows you a five-part prompt scaffold to generate schema-compliant Gherkin test suites from acceptance criteria. You'll see a before/after comparison where a naive prompt yields 3 generic scenarios, and a structured prompt yields 12 schema-valid scenarios with boundary and OWASP coverage.\n\n## What Problem Are We Solving?\n\nWhen you prompt a language model to generate test scenarios, you typically get:\n\n- Only happy-path cases with no boundary or negative tests.\n- Outputs that ignore acceptance criteria or blend them into prose.\n- Gherkin syntax errors that break parsers and runners.\n- Missing security or edge-case coverage.\n\nFor example, a naive prompt like \"Generate test scenarios for this user story\" produces 3 generic scenarios focused on success flows. A structured prompt with delimiters, explicit counts, and schema constraints produces 12 parseable scenarios covering happy paths, boundaries, negatives, and OWASP risks.\n\n## What's Actually Happening Under the Hood?\n\nFour mechanisms explain why unstructured prompts fail:\n\n- **Recency and salience bias.** Models prioritize instructions near the generation task. If acceptance criteria appear early and the generation instruction appears late, the model may ignore the criteria.\n- **Instruction dilution.** Mixing requirements, format rules, and constraints in a single block reduces compliance. The model treats everything as suggestions rather than hard rules.\n- **Format contamination.** Without delimiters, the model blends instructions into the output. You get explanatory text instead of clean Gherkin.\n- **Exploration vs. compliance tension.** Models default to creative, varied outputs. Without explicit constraints (counts, tags, schema fields), they skip edge cases and security scenarios.\n\nThe following diagram shows how proper ordering and delimiters prevent contamination:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\nflowchart LR\n    A[System: Role + Style] --> B[Context: User Story + Criteria]\n    B --> C[Task: Coverage + Counts]\n    C --> D[Format: Gherkin Schema]\n    D --> E[Constraints: Negative + OWASP + Boundaries]\n    E --> F[Output: Tests]\n    B -. no delimiters .-> F\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Fix It: Structured Gherkin Outputs with Delimiters and Schema Constraints\n\nUse a five-part scaffold to control output quality:\n\n1. **Role (system message).** Assign a QA engineer persona with strict terminology.\n2. **Context (user message).** Provide the user story and acceptance criteria, delimited with `---CRITERIA---` and `---END CRITERIA---`.\n3. **Task.** Specify coverage targets and counts (e.g., \"Generate 12 scenarios: 4 happy, 4 boundary, 2 negative, 2 OWASP\").\n4. **Format.** Define the Gherkin schema with required fields (Feature, Scenario, Given/When/Then, tags).\n5. **Constraints.** Enforce output-only blocks, no explanations, and explicit tagging (e.g., `@security @owasp-a01`).\n\n### Before: Naive Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Generate test scenarios for a login feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output (3 scenarios, all happy paths):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```gherkin\nScenario: User logs in successfully\n  Given the user is on the login page\n  When the user enters valid credentials\n  Then the user is redirected to the dashboard\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### After: Structured Prompt\n\nThis example demonstrates the five-part scaffold with delimiters, explicit counts, and schema enforcement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "System: You are a QA engineer. Use strict Gherkin syntax. No explanations.\n\nUser:\n---CRITERIA---\nAC-1: Users must log in with email and password.\nAC-2: Invalid credentials show an error.\nAC-3: Account locks after 5 failed attempts.\n---END CRITERIA---\n\nGenerate 12 scenarios:\n- 4 happy paths\n- 4 boundary cases (empty fields, special characters, max length)\n- 2 negative cases (wrong password, locked account)\n- 2 OWASP cases (SQL injection, brute force)\n\nFormat:\nFeature: [name]\nScenario: [description]\n  Given [precondition]\n  When [action]\n  Then [outcome]\nTags: @type @owasp-id (if applicable)\n\nOutput only Gherkin. No commentary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output (12 scenarios, schema-compliant, with boundary and security coverage):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```gherkin\nFeature: User Login\n\n@happy\nScenario: Valid login redirects to dashboard\n  Given the user is on the login page\n  When the user enters valid email and password\n  Then the user is redirected to the dashboard\n\n@boundary\nScenario: Empty email field shows validation error\n  Given the user is on the login page\n  When the user submits with an empty email field\n  Then an error message \"Email is required\" is displayed\n\n@negative\nScenario: Locked account prevents login\n  Given the user account is locked after 5 failed attempts\n  When the user enters valid credentials\n  Then an error message \"Account locked\" is displayed\n\n@security @owasp-a03\nScenario: SQL injection in email field is sanitized\n  Given the user is on the login page\n  When the user enters \"admin' OR '1'='1\" in the email field\n  Then the input is sanitized and login fails with \"Invalid credentials\"\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Portable Best Practices\n\n- **Isolate instructions in the system message.** Place role and style rules in the system message to reduce instruction dilution.\n- **Place criteria adjacent to the generation instruction.** Use delimiters (`---CRITERIA---`) and position them immediately before the task to leverage recency bias.\n- **Enforce schema with explicit fields.** List required Gherkin components (Feature, Scenario, Given/When/Then, tags) and demand output-only blocks.\n- **Set temperature â‰¤ 0.2.** Low temperature reduces variance and improves schema compliance.\n- **Validate with a Gherkin parser.** Run outputs through a parser (e.g., `behave --dry-run`) and fail on syntax errors.\n\n### Secure API Key Loading\n\nThis code block securely loads API keys from Colab userdata, ensuring keys are never hardcoded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom google.colab import userdata\nfrom google.colab.userdata import SecretNotFoundError\n\ndef load_api_keys(required_keys):\n    \"\"\"\n    Securely loads API keys from Google Colab userdata and sets them as environment variables.\n\n    Args:\n        required_keys (list of str): List of environment variable names to load from Colab secrets.\n\n    Returns:\n        None\n\n    Raises:\n        EnvironmentError: If any required key is missing from Colab secrets.\n    \"\"\"\n    missing = []\n    for k in required_keys:\n        value = None\n        try:\n            value = userdata.get(k)\n        except SecretNotFoundError:\n            pass\n\n        os.environ[k] = value if value is not None else \"\"\n\n        if not os.environ[k]:\n            missing.append(k)\n\n    if missing:\n        raise EnvironmentError(\n            f\"Missing keys: {', '.join(missing)}. \"\n            \"Add them in Colab â†’ Settings â†’ Secrets.\"\n        )\n\n    print(\"All keys loaded.\")\n\nREQUIRED_KEYS = [\"OPENAI_API_KEY\", \"ANTHROPIC_API_KEY\"]\nload_api_keys(REQUIRED_KEYS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\n- **Use a five-part scaffold.** Role, context with delimiters, task with counts, format schema, and constraints.\n- **Delimiters prevent format contamination.** Wrap acceptance criteria and output blocks with clear markers.\n- **Explicit counts and tags drive coverage.** Specify how many scenarios per type and require tags like `@security` or `@owasp-a01`.\n- **Low temperature improves determinism.** Set temperature â‰¤ 0.2 for schema compliance.\n- **Validate outputs with a parser.** Fail fast on syntax errors to ensure runner compatibility.\n\n### When to Use This Pattern\n\n- Outputs drift to happy paths only.\n- Models ignore acceptance criteria.\n- You need schema-valid, parseable Gherkin for CI runners.\n- Security or boundary cases are missing."
      ]
    }
  ],
  "metadata": {
    "title": "How to Use GPT Prompts for Test Case Generation in QA [2025]",
    "description": "Generate comprehensive, reliable tests in minutes using proven GPT prompts, BDD templates, and tool integrationsâ€”cut flakiness, expand coverage, accelerate releases.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}