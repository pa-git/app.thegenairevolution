{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Choose an AI Model for Your App: Speed, Cost, Reliability\n\n**Description:** Quickly choose the right LLM with a practical framework: compare accuracy, context limits, latency, token cost, and risk tolerance today.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choosing the right model isn't about hype, it's about fit. This guide gives you a practical framework to pick a model that matches your app's speed, cost, and reliability constraints. For a deeper dive into when a smaller model might actually outperform a large one, and how to weigh cost versus latency tradeoffs, see our analysis on [small vs large language models](/article/small-language-models-vs-large-language-models-when-to-use-each-2).\n\n## Define Your Requirements First\n\nBefore you compare models, nail down what success looks like for your application.\n\n**Latency budget:** Is this a chatbot where users expect sub-second responses, or a batch summarization job that can tolerate minutes? Set hard ceilings for time-to-first-token (TTFT) and total latency.\n\n**Context length:** How much text do you need to fit in a single prompt? Set a safe ceiling lower than \"max supported.\" Long-context models often degrade well before their advertised limit; treat 60â€“80% as usable. If you want to understand why this happens and how to mitigate it, check out our guide on [context rot and how LLMs 'forget' as their memory grows](/article/context-rot-why-llms-forget-as-their-memory-grows-3).\n\n**Output structure:** Do you need JSON, citations, or free-form text? Models with strong instruction-following (e.g., GPT-4o, Claude 3.5 Sonnet) handle structured output better than older or smaller models.\n\n**Accuracy floor:** Define the minimum acceptable quality. If you're extracting invoice line items, 95% precision might be table stakes. If you're drafting marketing copy, you can tolerate more variance.\n\n**Cost ceiling:** Estimate tokens per request (input + output) and multiply by your expected volume. This informs cost projections and whether you must compress, retrieve, or re-architect. For strategies to further reduce spend and boost responsiveness, consider implementing [semantic caching with Redis Vector](/article/semantic-cache-llm-how-to-implement-with-redis-vector-to-cut-costs-6) to cache near-duplicate prompts and optimize LLM usage.\n\n## Benchmark on Your Data, Not Demos\n\nPublic leaderboards (MMLU, HumanEval) measure general capability, not your task. A model that excels at coding benchmarks may struggle with your domain-specific extraction or summarization.\n\n**Use your data:** Pull a representative sample of real inputs and expected outputs. If you don't have labeled data yet, create a small synthetic set that mirrors production edge cases (long documents, ambiguous queries, multilingual text).\n\n**Measure what matters:** Track task-specific metricsâ€”F1 for extraction, ROUGE for summarization, exact match for structured output. Don't rely solely on vibes or cherry-picked examples.\n\n**Automate evaluation:** Tools like Promptfoo, Ragas, or custom scripts let you run hundreds of test cases in minutes. Version your prompts and datasets so you can reproduce results as models update.\n\n## Measure Effective Cost per Correct Output\n\nPer-token pricing is a trap. A cheaper model that produces twice as many tokens or requires two retries costs more than a pricier model that nails it in one shot.\n\n**Calculate cost per correct:** Multiply tokens per request by price per token, then divide by your measured accuracy. If Model A costs $0.01 per call at 90% accuracy, its effective cost is $0.011 per correct output. If Model B costs $0.005 at 70% accuracy, it's $0.007 per correctâ€”cheaper on paper, but you'll need fallback logic or human review.\n\n**Account for retries and fallbacks:** If your pipeline retries failed requests or escalates to a larger model, factor those costs into your total. A 95% success rate with no retries beats 80% with expensive escalation.\n\n**Price long context separately:** Some providers charge more per token beyond a threshold (e.g., 128k tokens). If your use case pushes context limits, test whether chunking or retrieval is cheaper than paying the long-context premium.\n\nHere's a Python script to estimate tokens and cost per call for a batch of requests:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline Token Calculator: Estimate tokens and cost per call for LLM usage\n\nimport os\nimport logging\nfrom typing import List, Dict, Any\n\nlogging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n\ntry:\n    import tiktoken\nexcept ImportError:\n    raise ImportError(\n        \"tiktoken is required. Install with: pip install tiktoken\"\n    )\n\ntry:\n    from google.colab import userdata\nexcept ImportError:\n    pass\n\ndef estimate_tokens(text: str, model_name: str = \"gpt-3.5-turbo\") -> int:\n    try:\n        encoding = tiktoken.encoding_for_model(model_name)\n    except KeyError:\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n    tokens = encoding.encode(text)\n    return len(tokens)\n\ndef predict_cost_per_call(\n    tokens_in: int,\n    tokens_out: int,\n    model_pricing: Dict[str, Any]\n) -> float:\n    price_in = model_pricing[\"price_in\"] / 1000\n    price_out = model_pricing[\"price_out\"] / 1000\n    cost = tokens_in * price_in + tokens_out * price_out\n    return cost\n\ndef batch_estimate(\n    inputs: List[str],\n    outputs: List[str],\n    model_name: str,\n    model_pricing: Dict[str, Any]\n) -> Dict[str, Any]:\n    assert len(inputs) == len(outputs), \"Inputs and outputs must be same length\"\n    token_ins = [estimate_tokens(inp, model_name) for inp in inputs]\n    token_outs = [estimate_tokens(out, model_name) for out in outputs]\n    costs = [\n        predict_cost_per_call(ti, to, model_pricing)\n        for ti, to in zip(token_ins, token_outs)\n    ]\n    summary = {\n        \"avg_tokens_in\": sum(token_ins) / len(token_ins),\n        \"avg_tokens_out\": sum(token_outs) / len(token_outs),\n        \"avg_cost_per_call\": sum(costs) / len(costs),\n        \"max_cost_per_call\": max(costs),\n        \"min_cost_per_call\": min(costs),\n        \"total_cost\": sum(costs),\n        \"calls\": len(inputs),\n    }\n    logging.info(f\"Batch estimate summary: {summary}\")\n    return summary\n\nif __name__ == \"__main__\":\n    gpt4o_mini_pricing = {\n        \"price_in\": 0.0005,\n        \"price_out\": 0.0015,\n    }\n\n    example_inputs = [\n        \"Summarize the following Slack thread about Q2 sales targets.\",\n        \"Extract all table data from the attached PDF and cite sources.\",\n    ]\n    example_outputs = [\n        '{\"summary\": \"Q2 sales targets increased by 10%...\", \"details\": \"...\", \"citations\": [\"doc1.pdf\"]}',\n        '{\"tables\": [{\"name\": \"Revenue\", \"rows\": [...] }], \"citations\": [\"page 3\", \"page 7\"]}',\n    ]\n\n    summary = batch_estimate(\n        example_inputs,\n        example_outputs,\n        model_name=\"gpt-4o-mini\",\n        model_pricing=gpt4o_mini_pricing,\n    )\n\n    print(\"Token and Cost Estimate Summary:\")\n    for k, v in summary.items():\n        print(f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Latency Under Load\n\nAdvertised latency numbers assume ideal conditions. Real-world performance depends on server load, request batching, and network variability.\n\n**Measure TTFT and tail latency:** Time-to-first-token (TTFT) dominates perceived responsiveness in streaming apps. Track p50, p95, and p99 latencies under realistic load. A model with 200ms p50 but 2s p99 will frustrate users.\n\n**Simulate production traffic:** Use load testing tools (Locust, k6) to send concurrent requests at your expected QPS. Watch for throughput degradation and queue buildup.\n\n**Compare hosted vs self-hosted:** Hosted APIs (OpenAI, Anthropic) abstract infrastructure but add network latency. Self-hosted (vLLM, TGI) gives you control but requires tuning batch size, KV cache, and quantization.\n\n## Check for Determinism and Reproducibility\n\nIf you need auditability (compliance, debugging, A/B tests), you need reproducible outputs.\n\n**Set temperature to 0:** This disables sampling randomness. Most models will return identical outputs for identical prompts, though some providers still inject minor variance.\n\n**Log prompts and responses:** Store the exact prompt, model version, and response for every request. If a model update changes behavior, you can diff outputs and catch regressions.\n\n**Version your prompts:** Treat prompts like code. Use Git or a prompt management tool to track changes and roll back if quality drops.\n\n## Plan for Routing and Escalation\n\nNo single model is optimal for every request. Route simple queries to fast, cheap models and escalate complex ones to larger models.\n\n**Use confidence signals:** If your model returns a confidence score or you can estimate it from output structure (e.g., presence of citations, JSON validity), route low-confidence requests to a stronger model.\n\n**Implement tiered routing:** Start with a small model (GPT-4o mini, Llama 3.1 8B). If it fails validation or returns low confidence, retry with a larger model (GPT-4o, Claude 3.5 Sonnet).\n\n**Monitor escalation rate:** If more than 20% of requests escalate, your small model is underperforming. Retrain, adjust prompts, or switch to a larger base model.\n\n## Evaluate Open-Source vs Hosted Tradeoffs\n\nOpen-source models (Llama, Mistral, Qwen) give you control and eliminate per-token costs, but require infrastructure and tuning.\n\n**When to self-host:** High volume (millions of requests/month), strict data residency, or need for custom fine-tuning. You'll pay for GPUs, engineering time, and monitoring.\n\n**When to use hosted APIs:** Low to moderate volume, rapid prototyping, or lack of ML infrastructure. You trade control for simplicity and predictable pricing.\n\n**Quantization and throughput:** Self-hosted models can be quantized (8-bit, 4-bit) to fit smaller GPUs and increase throughput. Test whether quantization degrades accuracy on your task before deploying.\n\n## Monitor in Production\n\nModel behavior drifts as input distributions shift and providers update models.\n\n**Track accuracy over time:** Run a subset of production requests through your eval pipeline daily. If accuracy drops, investigate prompt drift, model updates, or data distribution changes.\n\n**Alert on latency and cost spikes:** Set thresholds for p95 latency and daily spend. If either spikes, check for traffic surges, model degradation, or upstream API issues.\n\n**Version model endpoints:** When a provider updates a model (e.g., `gpt-4o-2024-08-06` â†’ `gpt-4o-2024-11-20`), test the new version in staging before switching production traffic.\n\n## Negotiate Volume and Caching\n\nIf you're spending thousands per month, you have leverage.\n\n**Ask for volume discounts:** OpenAI, Anthropic, and others offer tiered pricing for high-volume customers. Negotiate before you scale.\n\n**Enable prompt caching:** Some providers (Anthropic, OpenAI) cache repeated prompt prefixes and charge less for cached tokens. If your prompts share a long system message or context, this can cut costs by 50%.\n\n**Batch requests:** If latency permits, batch multiple requests into a single API call. This reduces overhead and can unlock cheaper pricing tiers.\n\n## Procurement and Compliance Notes\n\nIf you're in a regulated industry or enterprise, model selection intersects with legal and compliance requirements.\n\n**Data residency:** Some providers offer regional endpoints (EU, US) to comply with GDPR or other regulations. Verify where your data is processed and stored.\n\n**SLAs and uptime:** Hosted APIs typically guarantee 99.9% uptime. If downtime costs you revenue, negotiate SLAs or architect fallbacks (multiple providers, self-hosted backup).\n\n**Audit trails:** Log every request and response with timestamps, user IDs, and model versions. This is non-negotiable for compliance in finance, healthcare, and legal.\n\n## Beware Hidden Serving Costs\n\nSelf-hosting isn't just GPU rental. Factor in:\n\n- **Engineering time:** Setting up vLLM, TGI, or Ray Serve, tuning batch sizes, and debugging OOM errors.\n- **Monitoring and observability:** Prometheus, Grafana, and custom dashboards to track latency, throughput, and GPU utilization.\n- **Model updates:** Downloading new weights, re-quantizing, and re-benchmarking every few months.\n\nIf your team is small, hosted APIs often cost less than the fully loaded cost of self-hosting.\n\n## Final Checklist\n\nBefore you commit to a model:\n\n- Define latency, context, accuracy, and cost requirements\n- Benchmark on your data with task-specific metrics\n- Calculate effective cost per correct output, including retries\n- Test latency under realistic load (p95, p99)\n- Verify determinism if you need reproducibility\n- Plan tiered routing for cost and quality optimization\n- Decide hosted vs self-hosted based on volume and control needs\n- Set up monitoring for accuracy, latency, and cost drift\n- Negotiate volume pricing and enable caching if applicable\n- Document compliance and audit requirements\n\nModel selection is not a one-time decision. As your application scales, your data shifts, and new models launch, revisit this framework to ensure you're still optimizing for the right tradeoffs."
      ]
    }
  ],
  "metadata": {
    "title": "How to Choose an AI Model for Your App: Speed, Cost, Reliability",
    "description": "Quickly choose the right LLM with a practical framework: compare accuracy, context limits, latency, token cost, and risk tolerance today.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}