{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Cross Encoder Reranking: The Low-Cost Fix for RAG Misses\n\n**Description:** Cut RAG hallucinations and misses using cross-encoder reranking. Learn optimal rerank depth, caching strategies, and ColBERT tradeoffs for throughput balance.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When your RAG system retrieves passages that *look* relevant but lead the LLM to cite incorrect facts or miss critical nuances, the problem often lies in first-stage retrieval. BM25 and dense bi-encoders excel at broad recall but struggle with subtle intentâ€”negations, temporal constraints, or domain-specific phrasing. A cross-encoder reranker fixes this by scoring query-passage pairs jointly, catching \"close but wrong\" candidates before they reach the LLM.\n\nThis article explains why first-stage retrieval fails on nuanced queries, how cross-encoder reranking delivers top-rank precision, and the latency tradeoff you're accepting. You'll walk away with a decision rule for when to deploy reranking and a minimal set of defaults to start measuring impact.\n\n---\n\n## Why This Matters\n\nFirst-stage retrieval optimizes for recall: cast a wide net and surface plausible candidates quickly. BM25 matches keywords but ignores semantics. Dense bi-encoders (e.g., `all-MiniLM-L6-v2`) encode query and passage independently, missing fine-grained interactions like negations or conditional clauses.\n\nThe result: your top-30 candidates often include passages that share vocabulary with the query but contradict its intent. When the LLM sees \"FDA allows off-label promotion in certain contexts\" alongside \"FDA prohibits off-label promotion to physicians,\" it may cite the wrong oneâ€”or hedge into a vague, low-confidence answer.\n\n**Precision at top-k matters more than recall.** If your LLM only sees 5â€“10 passages, a single \"close but wrong\" document in that set can trigger a hallucination or incorrect citation. Cross-encoder reranking re-scores the top candidates using joint attention over query and passage tokens, surfacing the truly relevant passages and demoting near-misses.\n\n**The tradeoff:** reranking adds 100â€“200ms of latency per query (for ~30 candidates). If your application tolerates this and precision failures are measurable (e.g., answer correctness < 85%, nDCG@10 below baseline), reranking is worth deploying.\n\n---\n\n## How It Works\n\n**1. First-stage retrieval is independent and fast**  \nBM25 and bi-encoders score query and passage separately. BM25 counts term overlap; bi-encoders compare precomputed embeddings via cosine similarity. Neither model sees the query and passage together, so they miss contextual cuesâ€”negations, qualifiers, or domain-specific phrasing that flip meaning.\n\n**2. Cross-encoders use joint attention for precision**  \nA cross-encoder (e.g., `cross-encoder/ms-marco-MiniLM-L-6-v2`) concatenates query and passage as a single input: `[CLS] query [SEP] passage [SEP]`. Transformer attention layers process both together, learning token-level interactions. This catches subtle mismatchesâ€”e.g., \"FDA prohibits\" vs. \"FDA allows\"â€”that independent encoders miss.\n\n**3. Two-stage cascade balances speed and accuracy**  \nRetrieve 100â€“200 candidates with fast first-stage methods (BM25, dense, or hybrid). Rerank the top 30â€“50 with a cross-encoder. Send the top 5â€“10 reranked passages to the LLM. This keeps latency manageable while ensuring the LLM sees only high-precision context.\n\n**4. Latency cost is proportional to candidate count**  \nCross-encoders run full forward passes for each query-passage pair. Reranking 30 candidates takes ~100â€“150ms on CPU, ~30â€“50ms on GPU (T4 or better). Reranking 100+ candidates pushes latency above 300ms. The defaultâ€”rerank top-30, return top-8â€”fits most sub-500ms SLA budgets.\n\n---\n\n## What You Should Do\n\n**Start with a proven default configuration:**\n\n- **Retrieve 150 candidates** using your existing first-stage method (BM25, dense, or hybrid).\n- **Rerank the top 30** with `cross-encoder/ms-marco-MiniLM-L-6-v2` (6-layer MiniLM, strong precision/speed tradeoff).\n- **Send the top 8 reranked passages** to your LLM prompt.\n- **Set a 200ms timeout** for reranking; fall back to first-stage top-k if exceeded.\n\n**Measure impact with two metrics:**\n\n- **nDCG@10** on a labeled eval set (query + gold passage IDs). Reranking should lift nDCG by 5â€“15 points if first-stage precision is weak.\n- **Answer correctness** via LLM-as-judge or human eval. Track whether reranking reduces hallucinations or incorrect citations in your application.\n\n**When to deploy reranking:**\n\n- Precision@5 from first-stage retrieval is below 60% on your eval set.\n- Users report incorrect or contradictory answers despite relevant documents existing in your index.\n- Your application can absorb +100â€“200ms latency without breaking SLA.\n\n**When to skip reranking:**\n\n- First-stage retrieval already achieves >80% precision@10 (e.g., small, curated corpus with simple queries).\n- Latency budget is under 200ms end-to-end and cannot flex.\n- Queries are keyword-heavy with little semantic nuance (pure BM25 may suffice).\n\n**Integrate reranking into your pipeline with this code:**\n\nThis example demonstrates how to add a cross-encoder reranker to your RAG pipeline. It retrieves first-stage candidates, batches them for efficient inference, reranks using a cross-encoder, and optionally caches results with Redis to avoid redundant computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom typing import List, Tuple\nfrom sentence_transformers import CrossEncoder\n\ndef get_first_stage_candidates(query: str, k: int = 150) -> List[Tuple[str, str]]:\n    \"\"\"\n    Simulate first-stage retrieval (replace with your BM25/dense retriever).\n    Returns list of (passage_id, passage_text).\n    \"\"\"\n    return [(f\"doc_{i}\", f\"Passage {i} for '{query}'\") for i in range(k)]\n\ndef rerank_candidates(\n    query: str,\n    candidates: List[Tuple[str, str]],\n    cross_encoder: CrossEncoder,\n    top_n: int = 30\n) -> List[Tuple[str, float]]:\n    \"\"\"\n    Rerank candidates using cross-encoder, return top_n scored passages.\n    \"\"\"\n    input_pairs = [(query, ptext) for _, ptext in candidates]\n    scores = cross_encoder.predict(input_pairs, batch_size=32)\n    scored = list(zip([pid for pid, _ in candidates], scores))\n    scored.sort(key=lambda x: x[1], reverse=True)\n    return scored[:top_n]\n\n# Load model once at startup\ncross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n\n# Pipeline\nquery = \"Does FDA allow off-label promotion of drugs to physicians?\"\ncandidates = get_first_stage_candidates(query, k=150)\nreranked = rerank_candidates(query, candidates[:30], cross_encoder, top_n=8)\n\n# Use top 8 passages in LLM prompt\ntop_passages = [pid for pid, _ in reranked]\nprint(f\"Top passages for LLM: {top_passages}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Conclusion â€“ Key Takeaways\n\nCross-encoder reranking fixes \"close but wrong\" retrieval by scoring query and passage jointly, catching nuances that first-stage methods miss. It trades 100â€“200ms of latency for measurably better precision at top-k, reducing hallucinations and incorrect citations.\n\n**Deploy reranking when:**\n\n- First-stage precision@5 is below 60%\n- Answer correctness is suffering despite good recall\n- You can afford +100â€“200ms in your latency budget\n\n**Default recipe:** retrieve 150, rerank top-30 with `ms-marco-MiniLM-L-6-v2`, send top-8 to LLM. Measure nDCG@10 and answer correctness to confirm impact.\n\nFor advanced patternsâ€”late interaction models like ColBERT, caching strategies, or dynamic rerank depth under SLA pressureâ€”see the related explainers on production-scale reranking and retrieval optimization."
      ]
    }
  ],
  "metadata": {
    "title": "Cross Encoder Reranking: The Low-Cost Fix for RAG Misses",
    "description": "Cut RAG hallucinations and misses using cross-encoder reranking. Learn optimal rerank depth, caching strategies, and ColBERT tradeoffs for throughput balance.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}