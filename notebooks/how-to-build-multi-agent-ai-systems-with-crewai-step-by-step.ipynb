{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📓 The GenAI Revolution Cookbook\n\n**Title:** How to Build Multi-Agent AI Systems with CrewAI, Step by Step\n\n**Description:** Design scalable multi-agent AI systems with CrewAI using YAML configs, task delegation, tools, and training—then build a Customer Feedback Analyst.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here’s a revised version of your draft content, updated to use the “first\\-crew” approach from CrewAI’s guide. I’ve refactored your pipeline to match their project layout, crew base class, agent/task decorators, CLI usage. I also adjusted writing style per your humanization rules.\n\n---\n\nGenerative AI gets useful when it ships. In this guide, you’ll build a Customer Feedback Analyst using CrewAI following the “first\\-crew” pattern. You’ll define a crew via the CLI, set up agents and tasks in YAML, write tools, and run the system end\\-to\\-end. You’ll produce a real report with sentiment tables and a pie chart. Plus, you’ll learn the architecture and code you can reuse on your own data. For best practices on extracting structured data with LLMs, check out our [structured data extraction pipeline guide](/article/structured-data-extraction-with-llms-how-to-build-a-pipeline-3).\n\n## Why This Approach Works\n\nMulti\\-agent systems shine when you need specialized roles working together. A single LLM call can classify sentiment, but it struggles with reading files, computing aggregates, generating charts, assembling reports all at once. When you split responsibilities into focused agents, you gain modularity and clarity. You can also swap models or tools for specific tasks.\n\nCrewAI gives you a clean model for agents, tasks, tools, and crews. You write config in YAML. To coordinate complex workflows you use agent/task decorators, context passing, and built\\-in memory. If you want agents that interoperate and support audits, look into the [Model Context Protocol (MCP)](/article/model-context-protocol-mcp-explained-2025-guide-for-builders).\n\nIn this tutorial, you build a sequential pipeline. Each task receives context from earlier steps. That keeps logic simple and traceable. You’ll wire a file reader, a sentiment classifier, an aggregator, a chart generator, and a report writer into a single crew defined with CrewAI’s first\\-crew pattern.\n\n## What You’ll Build\n\nBy the end, you’ll have:\n\n* An **output/report.md** file with sentiment breakdown and insights.\n* A **reports/sentiment\\_pie.png** chart visualizing the distribution.\n* A reusable pipeline you can adapt to other CSV datasets or feedback sources.\n\nYou’ll run everything via CLI and script, validate outputs, and understand why each component exists.\n\n## Setup \\& Installation\n\nStart by creating a new CrewAI project using the CLI. This scaffolds the structure for you. ([docs.crewai.com](https://docs.crewai.com/en/guides/crews/first-crew))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "crewai create crew feedback_analyst\ncd feedback_analyst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That gives you directories like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feedback_analyst/\n├── .gitignore\n├── pyproject.toml\n├── README.md\n├── .env\n└── src/\n    └── feedback_analyst/\n        ├── __init__.py\n        ├── main.py\n        ├── crew.py\n        ├── tools/\n        │   ├── __init__.py\n        │   └── custom_tools.py\n        └── config/\n            ├── agents.yaml\n            └── tasks.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then install dependencies. Pin versions for stability:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -U crewai crewai-tools python-dotenv pyyaml pandas matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your API keys in `.env`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_API_KEY=your-openai-key-here\nANTHROPIC_API_KEY=your-anthropic-key-here # optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify them in your code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nrequired = [\"OPENAI_API_KEY\"]\nmissing = [k for k in required if not os.getenv(k)]\nif missing:\n    raise EnvironmentError(f\"Missing required environment variables: {', '.join(missing)}\")\nprint(\"All required API keys found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create directories if needed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nROOT = Path.cwd()\nDATA_DIR = ROOT / \"data\"\nREPORTS_DIR = ROOT / \"reports\"\n\nfor d in [DATA_DIR, REPORTS_DIR]:\n    d.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a sample CSV if you don’t have one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "csv_path = DATA_DIR / \"customer_feedback.csv\"\nif not csv_path.exists():\n    csv_path.write_text(\n        \"id,text\\n\"\n        \"1,I love the new dashboard, clean and fast.\\n\"\n        \"2,App crashed twice during checkout. Very frustrating.\\n\"\n        \"3,Support was helpful but resolution took too long.\\n\"\n        \"4,Works as expected. No issues so far.\\n\",\n        encoding=\"utf-8\",\n    )\n    print(f\"Created example CSV at {csv_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step\\-by\\-Step Implementation\n\n### Define Tools\n\nIn `src/feedback_analyst/tools/custom_tools.py`, write tools for reading, aggregating, charting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from crewai_tools import FileReadTool, tool\nimport json\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef make_file_reader(csv_path: str):\n    return FileReadTool(file_path=csv_path, description=\"Read customer_feedback.csv\")\n\n@tool(\"aggregate_sentiment\")\ndef aggregate_sentiment(jsonl_str: str) -> str:\n    lines = [ln.strip() for ln in jsonl_str.strip().split(\"\\n\") if ln.strip()]\n    records = [json.loads(ln) for ln in lines]\n    df = pd.DataFrame(records)\n    counts = df[\"sentiment\"].value_counts().to_dict()\n    result = {\n        \"positive\": counts.get(\"positive\", 0),\n        \"neutral\": counts.get(\"neutral\", 0),\n        \"negative\": counts.get(\"negative\", 0),\n    }\n    return json.dumps(result)\n\n@tool(\"make_sentiment_pie\")\ndef make_sentiment_pie(summary_json: str) -> str:\n    data = json.loads(summary_json)\n    counts = [\n        data.get(\"positive\", 0),\n        data.get(\"neutral\", 0),\n        data.get(\"negative\", 0),\n    ]\n    labels = [\"Positive\", \"Neutral\", \"Negative\"]\n    colors = [\"#2ecc71\", \"#95a5a6\", \"#e74c3c\"]\n\n    out_path = Path(\"reports/sentiment_pie.png\")\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n\n    fig, ax = plt.subplots(figsize=(5,5), dpi=160)\n    ax.pie(counts, labels=labels, autopct=\"%1.1f%%\", startangle=140, colors=colors)\n    ax.axis(\"equal\")\n    plt.title(\"Sentiment Distribution\")\n    plt.tight_layout()\n    fig.savefig(out_path)\n    plt.close(fig)\n    return str(out_path.resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Agents\n\nEdit `src/feedback_analyst/config/agents.yaml`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```yaml\nsentiment_classifier:\n  role: Sentiment Classifier\n  goal: Label each feedback entry as positive, neutral, or negative\n  backstory: You are an expert at understanding customer tone. You read feedback text and assign accurate sentiment.\n  llm: openai/gpt-4o\n  tools:\n    - file_reader\n  verbose: true\n  allow_delegation: false\n\naggregator:\n  role: Data Aggregator\n  goal: Compute sentiment counts from classified feedback\n  backstory: You parse structured data and produce accurate numeric summaries\n  llm: openai/gpt-4o\n  tools:\n    - aggregate_sentiment\n  verbose: true\n  allow_delegation: false\n\nchart_maker:\n  role: Chart Generator\n  goal: Create pie chart visualizing sentiment distribution\n  backstory: You turn data into clear visualizations\n  llm: openai/gpt-4o\n  tools:\n    - make_sentiment_pie\n  verbose: true\n  allow_delegation: false\n\nreport_writer:\n  role: Report Writer\n  goal: Assemble final Markdown report with insights and visuals\n  backstory: You synthesize analysis into clear actionable reports\n  llm: openai/gpt-4o\n  tools: []\n  verbose: true\n  allow_delegation: false\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Tasks\n\nEdit `src/feedback_analyst/config/tasks.yaml`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```yaml\nclassify_feedback:\n  description: |\n    Read customer_feedback.csv using the file_reader tool.\n    For each row, classify the sentiment of the \"text\" field as positive, neutral, or negative.\n    Output one JSON object per line (JSONL) with fields: id, text, sentiment.\n  agent: sentiment_classifier\n  expected_output: JSONL with id, text, sentiment for each entry\n\naggregate_counts:\n  description: |\n    Take JSONL output from classify_feedback.\n    Use aggregate_sentiment tool to compute counts of positive, neutral, negative sentiments.\n    Return a JSON object with keys: positive, neutral, negative.\n  agent: aggregator\n  expected_output: JSON object with sentiment counts\n  context:\n    - classify_feedback\n\ngenerate_chart:\n  description: |\n    Take the JSON summary from aggregate_counts.\n    Use make_sentiment_pie tool to generate a pie chart PNG.\n    Return absolute file path to the chart.\n  agent: chart_maker\n  expected_output: Absolute path to sentiment_pie.png\n  context:\n    - aggregate_counts\n\nwrite_report:\n  description: |\n    Use sentiment counts and chart path from previous tasks.\n    Write a Markdown report that includes:\n      - Summary of total feedback entries and sentiment breakdown\n      - Table with counts of positive, neutral, negative\n      - Embedded image: ![Sentiment Distribution](reports/sentiment_pie.png)\n      - Insights or recommendations based on sentiment distribution\n    Return Markdown content.\n  agent: report_writer\n  expected_output: Complete Markdown report with table, chart, and insights\n  context:\n    - aggregate_counts\n    - generate_chart\n  output_file: output/report.md\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure the Crew\n\nIn `src/feedback_analyst/crew.py` use the first\\-crew pattern: agent and task decorators. Inspired by CrewAI’s guide. ([docs.crewai.com](https://docs.crewai.com/en/guides/crews/first-crew))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from crewai import Agent, Task, Crew, Process\nfrom crewai.project import CrewBase, agent, task, crew\nfrom typing import List\nfrom feedback_analyst.tools.custom_tools import make_file_reader, aggregate_sentiment, make_sentiment_pie\nfrom pathlib import Path\n\n@CrewBase\nclass FeedbackAnalystCrew():\n    agents: List[Agent]\n    tasks: List[Task]\n\n    @agent\n    def sentiment_classifier(self) -> Agent:\n        cfg = self.agents_config['sentiment_classifier']\n        tools = [ make_file_reader(str(Path('data/customer_feedback.csv'))) ]\n        return Agent(\n            config=cfg,\n            verbose=True,\n            tools=tools,\n        )\n\n    @agent\n    def aggregator(self) -> Agent:\n        cfg = self.agents_config['aggregator']\n        tools = [ aggregate_sentiment ]\n        return Agent(\n            config=cfg,\n            verbose=True,\n            tools=tools,\n        )\n\n    @agent\n    def chart_maker(self) -> Agent:\n        cfg = self.agents_config['chart_maker']\n        tools = [ make_sentiment_pie ]\n        return Agent(\n            config=cfg,\n            verbose=True,\n            tools=tools,\n        )\n\n    @agent\n    def report_writer(self) -> Agent:\n        cfg = self.agents_config['report_writer']\n        return Agent(\n            config=cfg,\n            verbose=True,\n            tools=[],\n        )\n\n    @task\n    def classify_feedback(self) -> Task:\n        return Task(\n            config=self.tasks_config['classify_feedback'],\n        )\n\n    @task\n    def aggregate_counts(self) -> Task:\n        return Task(\n            config=self.tasks_config['aggregate_counts'],\n        )\n\n    @task\n    def generate_chart(self) -> Task:\n        return Task(\n            config=self.tasks_config['generate_chart'],\n        )\n\n    @task\n    def write_report(self) -> Task:\n        return Task(\n            config=self.tasks_config['write_report'],\n            output_file='output/report.md',\n        )\n\n    @crew\n    def crew(self) -> Crew:\n        return Crew(\n            agents=self.agents,\n            tasks=self.tasks,\n            process=Process.sequential,\n            verbose=True,\n        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main Script\n\nIn `src/feedback_analyst/main.py`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom feedback_analyst.crew import FeedbackAnalystCrew\n\ndef run():\n    os.makedirs('output', exist_ok=True)\n    result = FeedbackAnalystCrew().crew().kickoff(inputs={})\n    print(\"\\n\\n=== FINAL REPORT ===\\n\\n\")\n    print(result.raw)\n    print(\"\\nReport saved to output/report.md\")\n\nif __name__ == \"__main__\":\n    run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running, Saving \\& Validating Outputs\n\nFrom your project root, use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "crewai run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This runs the crew using the YAML configs and crew.py. You’ll see logs for each agent and task. When it finishes:\n\n* Check `output/report.md` for your report content.\n* Confirm `reports/sentiment_pie.png` exists.\n* If using a notebook or previewer, render the Markdown and display the chart inline.\n\n## Key Design Decisions\n\n* Deterministic aggregation via Python tool prevents hallucinations or wrong counts.\n* Sequential execution ensures logical flow: classification before aggregation; chart after counts.\n* YAML for agents/tasks keeps config separate from code. You can swap prompts, models, or tools without modifying core logic.\n* Tools each do one job. That keeps components testable and reusable.\n\n## Conclusion \\& Next Steps\n\nYou’ve built a complete Customer Feedback Analyst following CrewAI’s first\\-crew structure. You defined agents and tasks in YAML. You implemented tools for reading data, classification, aggregation, and visualization. You wrote a crew class, ran the system, and produced a Markdown report and PNG chart.\n\nHere’s where you can go next:\n\n* Add schema validation for the JSONL outputs using Pydantic to catch malformed data early.\n* Implement chunking to handle large CSVs without hitting context limits.\n* Integrate observability: log token usage, task durations, and tool calls for cost/performance insights.\n* Experiment with parallel or dynamic task execution when steps don’t strictly depend on each other.\n\nUse this template on other datasets. Adapt the agents, tweak the goals, change tools. You now have a strong foundation for multi\\-agent pipelines that deliver structured, validated outputs."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build Multi-Agent AI Systems with CrewAI, Step by Step",
    "description": "Design scalable multi-agent AI systems with CrewAI using YAML configs, task delegation, tools, and training—then build a Customer Feedback Analyst.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}