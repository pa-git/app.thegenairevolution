{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build Multi-Agent AI Systems with CrewAI, Step by Step\n\n**Description:** Design scalable multi-agent AI systems with CrewAI using YAML configs, task delegation, tools, and trainingâ€”then build a Customer Feedback Analyst.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generative AI gets useful when it ships. In this guide, you'll build a Customer Feedback Analyst using CrewAI. You'll define agents in YAML, wire tasks and tools, run the system end-to-end, and produce a real report with sentiment tables and a pie chart. You'll learn the exact architecture and code to reuse on your own data. For those interested in building reliable pipelines for extracting structured data with LLMs, our [structured data extraction pipeline guide](/article/structured-data-extraction-with-llms-how-to-build-a-pipeline-3) provides a detailed walkthrough of best practices.\n\n## Why This Approach Works\n\nMulti-agent systems shine when you need specialized roles working together. A single LLM call can classify sentiment, but it struggles to coordinate reading files, computing aggregates, generating charts, and assembling reports in one pass. By splitting these responsibilities across focused agents, you gain modularity, clarity, and the ability to swap models or tools per task.\n\nCrewAI provides a clean model for agents, tasks, tools, and crews. You define roles and workflows in YAML, reduce boilerplate, and run sequential, hierarchical, or parallel processes with built-in context passing, delegation, and memory. To further standardize tool and data access across agents and environments, consider learning about the [Model Context Protocol (MCP)](/article/model-context-protocol-mcp-explained-2025-guide-for-builders), which can help your agents interoperate and audit more effectively.\n\nThis tutorial uses a sequential pipeline: each task receives context from previous steps, keeping logic simple and traceable. You'll wire a file reader, a sentiment classifier, an aggregation tool, a chart generator, and a report writer into a single crew that runs end-to-end.\n\n## How It Works\n\nThe pipeline has five stages:\n\n1. **Read feedback**: A file reader tool loads the CSV.\n2. **Classify sentiment**: An agent labels each row as positive, neutral, or negative, outputting JSONL.\n3. **Aggregate counts**: A Python tool parses the JSONL and computes sentiment totals deterministically.\n4. **Generate chart**: A tool creates a pie chart PNG from the aggregated counts.\n5. **Assemble report**: A writer agent combines the table and chart into a Markdown document.\n\nEach agent has a role, goal, backstory, and assigned tools. Tasks declare dependencies via context, ensuring outputs flow in order. The crew orchestrates execution, and you get a final report with real artifacts.\n\n## What You'll Build\n\nBy the end, you'll have:\n\n- A `customer_feedback_report.md` with sentiment breakdown and insights.\n- A `sentiment_pie.png` chart visualizing the distribution.\n- A reusable pipeline you can adapt to other CSV datasets or feedback sources.\n\nYou'll run everything in a notebook or script, validate outputs, and understand the design decisions behind each component.\n\n## Setup & Installation\n\nStart by installing dependencies. Pin versions to ensure stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U crewai==0.28.0 crewai-tools==0.12.0 python-dotenv==1.0.0 pyyaml==6.0.1 pandas==2.2.0 matplotlib==3.8.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your API keys. If you're in a notebook without a `.env` file, set them in-session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key-here\"\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key-here\"  # optional, if using Claude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify the keys are present:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "required_keys = [\"OPENAI_API_KEY\"]\nmissing = [k for k in required_keys if not os.getenv(k)]\nif missing:\n    raise EnvironmentError(\n        f\"Missing required environment variables: {', '.join(missing)}\\n\"\n        \"Please set them before running the notebook.\"\n    )\nprint(\"All required API keys found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the project directories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nROOT = Path.cwd()\nDATA_DIR = ROOT / \"data\"\nCONFIGS_DIR = ROOT / \"configs\"\nREPORTS_DIR = ROOT / \"reports\"\nTOOLS_DIR = ROOT / \"tools\"\n\nfor d in [DATA_DIR, CONFIGS_DIR, REPORTS_DIR, TOOLS_DIR]:\n    d.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a sample CSV if you don't have one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "csv_path = DATA_DIR / \"customer_feedback.csv\"\nif not csv_path.exists():\n    csv_path.write_text(\n        \"id,text\\n\"\n        \"1,\\\"I love the new dashboardâ€”clean and fast.\\\"\\n\"\n        \"2,\\\"App crashed twice during checkout. Very frustrating.\\\"\\n\"\n        \"3,\\\"Support was helpful, but resolution took too long.\\\"\\n\"\n        \"4,\\\"Works as expected. No issues so far.\\\"\\n\",\n        encoding=\"utf-8\",\n    )\n    print(f\"Created example CSV at {csv_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step-by-Step Implementation\n\n### Define Tools\n\nTools are functions agents call to perform actions. We'll create a file reader, a sentiment aggregator, and a chart generator.\n\nWrite the tools module to disk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tools_code = '''# tools/__init__.py\nfrom crewai_tools import FileReadTool, tool\nimport json\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef make_file_reader(csv_path: str):\n    \"\"\"Create a FileReadTool for reading the CSV.\"\"\"\n    return FileReadTool(file_path=csv_path, description=\"Read customer_feedback.csv\")\n\n@tool(\"aggregate_sentiment\")\ndef aggregate_sentiment(jsonl_str: str) -> str:\n    \"\"\"\n    Parse JSONL sentiment classifications and compute counts.\n    \n    Args:\n        jsonl_str: JSONL string with 'sentiment' field per line.\n    \n    Returns:\n        JSON string with keys 'positive', 'neutral', 'negative'.\n    \"\"\"\n    lines = [ln.strip() for ln in jsonl_str.strip().split(\"\\\\n\") if ln.strip()]\n    records = [json.loads(ln) for ln in lines]\n    df = pd.DataFrame(records)\n    counts = df[\"sentiment\"].value_counts().to_dict()\n    result = {\n        \"positive\": counts.get(\"positive\", 0),\n        \"neutral\": counts.get(\"neutral\", 0),\n        \"negative\": counts.get(\"negative\", 0),\n    }\n    return json.dumps(result)\n\n@tool(\"make_sentiment_pie\")\ndef make_sentiment_pie(summary_json: str) -> str:\n    \"\"\"\n    Generate a pie chart PNG from sentiment summary JSON.\n    \n    Args:\n        summary_json: JSON with keys 'positive', 'neutral', 'negative'.\n    \n    Returns:\n        Absolute path to the saved PNG.\n    \"\"\"\n    data = json.loads(summary_json)\n    counts = [data.get(\"positive\", 0), data.get(\"neutral\", 0), data.get(\"negative\", 0)]\n    labels = [\"Positive\", \"Neutral\", \"Negative\"]\n    colors = [\"#2ecc71\", \"#95a5a6\", \"#e74c3c\"]\n    \n    reports_dir = Path(\"reports\")\n    reports_dir.mkdir(parents=True, exist_ok=True)\n    out_path = reports_dir / \"sentiment_pie.png\"\n    \n    fig, ax = plt.subplots(figsize=(5, 5), dpi=160)\n    ax.pie(counts, labels=labels, autopct=\"%1.1f%%\", startangle=140, colors=colors)\n    ax.axis(\"equal\")\n    plt.title(\"Sentiment Distribution\")\n    plt.tight_layout()\n    fig.savefig(out_path)\n    plt.close(fig)\n    return str(out_path.resolve())\n'''\n\ntools_init = TOOLS_DIR / \"__init__.py\"\ntools_init.write_text(tools_code, encoding=\"utf-8\")\nprint(f\"Tools module written to {tools_init}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `aggregate_sentiment` tool parses JSONL output from the classifier and computes counts using pandas, ensuring deterministic results. The `make_sentiment_pie` tool generates a PNG chart from the aggregated JSON.\n\n### Configure Agents\n\nAgents are defined in YAML. Each has a role, goal, backstory, and assigned tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agents_yaml = \"\"\"\nsentiment_classifier:\n  role: Sentiment Classifier\n  goal: Label each feedback entry as positive, neutral, or negative\n  backstory: You are an expert at understanding customer tone and intent. You read feedback text and assign accurate sentiment labels.\n  llm: gpt-4o-mini\n  tools:\n    - file_reader\n  verbose: true\n  allow_delegation: false\n\naggregator:\n  role: Data Aggregator\n  goal: Compute sentiment counts from classified feedback\n  backstory: You parse structured data and produce accurate numeric summaries.\n  llm: gpt-4o-mini\n  tools:\n    - aggregate_sentiment\n  verbose: true\n  allow_delegation: false\n\nchart_maker:\n  role: Chart Generator\n  goal: Create a pie chart visualizing sentiment distribution\n  backstory: You turn data into clear, professional visualizations.\n  llm: gpt-4o-mini\n  tools:\n    - make_sentiment_pie\n  verbose: true\n  allow_delegation: false\n\nreport_writer:\n  role: Report Writer\n  goal: Assemble a final Markdown report with insights and visuals\n  backstory: You synthesize analysis into clear, actionable reports for stakeholders.\n  llm: gpt-4o-mini\n  tools: []\n  verbose: true\n  allow_delegation: false\n\"\"\"\n\nagents_path = CONFIGS_DIR / \"agents.yaml\"\nagents_path.write_text(agents_yaml, encoding=\"utf-8\")\nprint(f\"Agents config written to {agents_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each agent is specialized: the classifier reads and labels, the aggregator computes counts, the chart maker visualizes, and the writer composes the final document.\n\n### Configure Tasks\n\nTasks define what each agent does and how outputs flow between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tasks_yaml = \"\"\"\nclassify_feedback:\n  description: |\n    Read the customer_feedback.csv file using the file_reader tool.\n    For each row, classify the sentiment of the 'text' field as positive, neutral, or negative.\n    Output one JSON object per line (JSONL format) with fields: id, text, sentiment.\n    Example:\n    {\"id\": 1, \"text\": \"I love it\", \"sentiment\": \"positive\"}\n    {\"id\": 2, \"text\": \"It's okay\", \"sentiment\": \"neutral\"}\n  agent: sentiment_classifier\n  expected_output: JSONL with id, text, and sentiment for each feedback entry.\n\naggregate_counts:\n  description: |\n    Take the JSONL output from the previous task.\n    Use the aggregate_sentiment tool to compute the total counts of positive, neutral, and negative sentiments.\n    Return a JSON object with keys: positive, neutral, negative.\n  agent: aggregator\n  expected_output: JSON object with sentiment counts.\n  context:\n    - classify_feedback\n\ngenerate_chart:\n  description: |\n    Take the JSON summary from the previous task.\n    Use the make_sentiment_pie tool to generate a pie chart PNG.\n    Return the absolute file path to the saved chart.\n  agent: chart_maker\n  expected_output: Absolute path to sentiment_pie.png.\n  context:\n    - aggregate_counts\n\nwrite_report:\n  description: |\n    Using the sentiment counts and chart path from previous tasks, write a Markdown report.\n    Include:\n    - A summary of total feedback entries and sentiment breakdown.\n    - A table showing positive, neutral, and negative counts.\n    - An embedded image of the pie chart using Markdown syntax: ![Sentiment Distribution](reports/sentiment_pie.png)\n    - Brief insights or recommendations based on the sentiment distribution.\n    Save nothing yourself; just return the Markdown content.\n  agent: report_writer\n  expected_output: Complete Markdown report with table, chart, and insights.\n  context:\n    - aggregate_counts\n    - generate_chart\n\"\"\"\n\ntasks_path = CONFIGS_DIR / \"tasks.yaml\"\ntasks_path.write_text(tasks_yaml, encoding=\"utf-8\")\nprint(f\"Tasks config written to {tasks_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `context` field wires dependencies: each task receives outputs from prior tasks, ensuring the pipeline runs in order. As your pipeline grows, be mindful of issues like [context rot](/article/context-rot-why-llms-forget-as-their-memory-grows-3), where LLMs may lose track of earlier information as more data accumulates.\n\n### Instantiate Agents and Tasks\n\nLoad the YAML configs and build the agent and task objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\nfrom crewai import Agent, Task, LLM\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndef load_yaml_config(path: Path) -> dict:\n    \"\"\"Load and parse a YAML file.\"\"\"\n    if not path.exists():\n        raise FileNotFoundError(f\"Missing config: {path}\")\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        return yaml.safe_load(f) or {}\n\ndef get_llm(model_name: str | None) -> LLM:\n    \"\"\"Instantiate an LLM for the specified model.\"\"\"\n    model = model_name or os.getenv(\"DEFAULT_LLM_MODEL\", \"gpt-4o-mini\")\n    return LLM(model=model)\n\ndef build_tools_registry(csv_path: Path):\n    \"\"\"Build a registry of available tools.\"\"\"\n    from tools import make_file_reader, aggregate_sentiment, make_sentiment_pie\n    return {\n        \"file_reader\": make_file_reader(str(csv_path)),\n        \"aggregate_sentiment\": aggregate_sentiment,\n        \"make_sentiment_pie\": make_sentiment_pie,\n    }\n\ndef instantiate_agents(agents_cfg: dict, tools_registry: dict) -> dict:\n    \"\"\"Instantiate Agent objects from YAML config.\"\"\"\n    agents = {}\n    for key, spec in agents_cfg.items():\n        role = spec.get(\"role\")\n        goal = spec.get(\"goal\")\n        backstory = spec.get(\"backstory\", \"\")\n        verbose = bool(spec.get(\"verbose\", True))\n        allow_delegation = bool(spec.get(\"allow_delegation\", False))\n        llm = get_llm(spec.get(\"llm\"))\n        tool_names = spec.get(\"tools\", []) or []\n        tools = [tools_registry[name] for name in tool_names if name in tools_registry]\n        \n        if not role or not goal:\n            raise ValueError(f\"Agent {key} missing role or goal\")\n        \n        agents[key] = Agent(\n            role=role,\n            goal=goal,\n            backstory=backstory,\n            verbose=verbose,\n            allow_delegation=allow_delegation,\n            llm=llm,\n            tools=tools,\n        )\n    return agents\n\ndef instantiate_tasks(tasks_cfg: dict, agents: dict) -> dict:\n    \"\"\"Instantiate Task objects from YAML config.\"\"\"\n    tasks = {}\n    for key, spec in tasks_cfg.items():\n        description = spec.get(\"description\", \"\").strip()\n        expected_output = spec.get(\"expected_output\", \"\").strip()\n        agent_key = spec.get(\"agent\")\n        if agent_key not in agents:\n            raise KeyError(f\"Task {key} references unknown agent '{agent_key}'\")\n        tasks[key] = Task(\n            description=description,\n            agent=agents[agent_key],\n            expected_output=expected_output,\n        )\n    for key, spec in tasks_cfg.items():\n        ctx = spec.get(\"context\", []) or []\n        context_tasks = [tasks[name] for name in ctx if name in tasks]\n        if context_tasks:\n            tasks[key].context = context_tasks\n    return tasks\n\nagents_cfg = load_yaml_config(agents_path)\ntasks_cfg = load_yaml_config(tasks_path)\n\ntools_registry = build_tools_registry(csv_path)\nagents = instantiate_agents(agents_cfg, tools_registry)\ntasks = instantiate_tasks(tasks_cfg, agents)\n\nprint(f\"Instantiated {len(agents)} agents and {len(tasks)} tasks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code loads the YAML, attaches tools to agents, and wires task dependencies. Each agent gets only the tools it needs, and tasks reference prior outputs via context.\n\n### Run the Crew\n\nAssemble the crew and execute the pipeline sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from crewai import Crew, Process\n\ncrew = Crew(\n    agents=list(agents.values()),\n    tasks=list(tasks.values()),\n    process=Process.sequential,\n    verbose=True,\n)\n\nprint(\"Kicking off crew...\")\nfinal_output = crew.kickoff()\nprint(\"\\n=== Final Output (Report) ===\\n\")\nprint(final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The crew runs each task in order, passing context forward. You'll see logs for each agent's actions and tool calls.\n\n### Save and Validate Outputs\n\nWrite the final report to disk and verify the chart was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "report_path = REPORTS_DIR / \"customer_feedback_report.md\"\nreport_path.write_text(str(final_output), encoding=\"utf-8\")\nprint(f\"\\nReport saved to: {report_path.resolve()}\")\n\nchart_path = REPORTS_DIR / \"sentiment_pie.png\"\nif chart_path.exists():\n    print(f\"Chart saved to: {chart_path.resolve()}\")\nelse:\n    print(\"Warning: Chart file not found. Check tool execution logs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Open the Markdown file to see the full report, including the sentiment table and embedded chart. If running in a notebook, display the chart inline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n\nif chart_path.exists():\n    display(Image(filename=str(chart_path)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Design Decisions\n\n**Why deterministic aggregation?** Asking an LLM to compute counts risks hallucination or inconsistency. A Python tool using pandas guarantees correct totals every time.\n\n**Why sequential execution?** Dependencies are clear: you can't aggregate before classifying, or chart before aggregating. Sequential processing keeps the pipeline simple and debuggable. You can parallelize independent tasks later if needed.\n\n**Why YAML for config?** Externalizing agent and task definitions makes the system maintainable. You can swap models, adjust prompts, or add agents without touching orchestration code.\n\n**Why separate tools for each step?** Each tool has a single responsibility. This modularity lets you test, reuse, and replace components independently.\n\n## Conclusion\n\nYou've built a complete Customer Feedback Analyst with CrewAI. You defined agents in YAML, wired tasks with context dependencies, attached tools for file reading, aggregation, and charting, and ran the system end-to-end to produce a real Markdown report and PNG visualization.\n\nThis architecture is reusable: swap the CSV for another dataset, adjust the classifier prompt, or add agents for deeper analysis. You now have a template for multi-agent pipelines that coordinate specialized roles to deliver structured, validated outputs.\n\n**Next steps:**\n\n- Add schema validation for JSONL outputs using Pydantic to catch malformed data early.\n- Implement chunking for large CSVs to avoid context limits.\n- Integrate observability: log token usage, task timings, and tool calls for cost and performance tracking.\n- Explore parallel task execution for independent steps to reduce latency.\n- Deploy the pipeline as an API or scheduled job for continuous feedback analysis."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build Multi-Agent AI Systems with CrewAI, Step by Step",
    "description": "Design scalable multi-agent AI systems with CrewAI using YAML configs, task delegation, tools, and trainingâ€”then build a Customer Feedback Analyst.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}