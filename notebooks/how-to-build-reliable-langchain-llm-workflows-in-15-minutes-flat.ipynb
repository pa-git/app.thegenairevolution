{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build Reliable LangChain LLM Workflows in 15 Minutes Flat\n\n**Description:** Get hands-on with LangChain: install, configure models, build prompt-driven chains, and parse structured outputsâ€”launch reliable Python LLM workflows fast today.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You want a fast path to a reliable LLM workflow. In 15 minutes, you will build a production\\-ready LLM workflow in Python using LangChain that outputs strict JSON for downstream systems. Setup friction and unclear abstractions stall delivery. Brittle parsing breaks downstream systems. This guide shows you how to build a chain that turns customer emails into strict JSON dictionaries ready for routing. You will route tickets, initiate refunds, and update analytics without manual intervention. You need Python 3\\.9\\+, an OpenAI account with API access, and basic familiarity with pip and environment variables.\n\n## Why Use LangChain for This Problem\n\nLangChain gives you composable building blocks for prompts, models, and parsers. You avoid boilerplate for message formatting, retry logic, and output parsing. The LCEL syntax makes chains readable and testable. You can swap models, adjust prompts, and add validation without rewriting integration code.\n\nYou could call the OpenAI SDK directly. You would get full control, but you would also write your own message formatting, error handling, and parsing logic. LlamaIndex is great for retrieval\\-augmented generation. It adds complexity if you only need prompt\\-to\\-structured\\-output workflows. LangChain strikes a balance. It handles the plumbing while keeping your code explicit and maintainable.\n\n## Core Concepts for This Use Case\n\n* Runnables and chains. A runnable is any component you can invoke with input and get output. Chains are runnables composed with the pipe operator. You will chain a prompt template, an LLM, and a parser into a single callable unit.\n* Prompt templates. Templates let you parameterize system and user messages. You inject variables like persona and user input at runtime. This keeps prompts version\\-controlled and testable.\n* Structured output parsers. Parsers enforce a schema on the model output. You define fields and types. The parser validates the JSON. If the model returns invalid JSON, you can catch the exception and retry.\n* Messages. LangChain uses message objects to represent conversation turns. This makes multi\\-turn context explicit and portable across models.\n* LCEL. LangChain Expression Language connects components with the pipe operator. You read chains top to bottom. You test them in small steps.\n\n## Setup\n\nInstall the required packages. Run this command in your terminal or notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">!pip install -U langchain langchain-community langchain-openai openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python \\-m pip install \\-U \\\\\n langchain langchain\\-core langchain\\-openai \\\\\n pydantic python\\-dotenv tiktokenLoad your API keys securely. Never hardcode keys in your code. Use a .env file locally, or Colab secrets if you are in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">import os\nfrom google.colab import userdata\nfrom google.colab.userdata import SecretNotFoundError\n\nkeys = [\"OPENAI_API_KEY\", \"ANTHROPIC_API_KEY\"]\nmissing = []\nfor k in keys:\n    value = None\n    try:\n        value = userdata.get(k)\n    except SecretNotFoundError:\n        pass\n\n    os.environ[k] = value if value is not None else \"\"\n\n    if not os.environ[k]:\n        missing.append(k)\n\nif missing:\n    raise EnvironmentError(f\"Missing keys: {', '.join(missing)}. Add them in Colab â†’ Settings â†’ Secrets.\")\n\nprint(\"All keys loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import os\nfrom dotenv import load\\_dotenv\n\n\\# Load .env if present\nload\\_dotenv()\n\n\\# Option A. Standard environment variables\nrequired \\= \\[\"OPENAI\\_API\\_KEY\"]\nmissing \\= \\[k for k in required if not os.getenv(k)]\nif missing:\n raise ValueError(f\"Missing environment variables: {missing}\")\n\n\\# Optional. If you use Colab, uncomment and set your secrets there\n\\# from google.colab import userdata\n\\# os.environ\\[\"OPENAI\\_API\\_KEY\"] \\= userdata.get(\"OPENAI\\_API\\_KEY\") or os.getenv(\"OPENAI\\_API\\_KEY\", \"\")\n\nprint(\"API key loaded.\")Verify imports and environment. This block ensures your OpenAI key is present and imports the core LangChain components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">import os\n\nassert os.getenv(\"OPENAI_API_KEY\"), \"Set OPENAI_API_KEY in your Colab secrets\"\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage, AIMessage\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.output_parsers import ResponseSchema, StructuredOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import os\nassert os.getenv(\"OPENAI\\_API\\_KEY\"), \"OPENAI\\_API\\_KEY is not set.\"\n\nfrom langchain\\_openai import ChatOpenAI\nfrom langchain\\_core.messages import SystemMessage, HumanMessage, AIMessage\nfrom langchain\\_core.prompts import ChatPromptTemplate\nfrom langchain\\_core.output\\_parsers import StrOutputParser\nfrom langchain.output\\_parsers import ResponseSchema, StructuredOutputParser\n\nprint(\"Imports successful.\")Instantiate the model. Use temperature 0 for deterministic output. Set max\\_tokens to control cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">llm = ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    temperature=0,\n    max_tokens=300,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODEL\\_NAME \\= os.getenv(\"MODEL\\_NAME\", \"gpt\\-4o\\-mini\")\nTEMPERATURE \\= float(os.getenv(\"TEMPERATURE\", \"0\"))\nMAX\\_TOKENS \\= int(os.getenv(\"MAX\\_TOKENS\", \"256\"))\n\nllm \\= ChatOpenAI(\n model\\=MODEL\\_NAME,\n temperature\\=TEMPERATURE,\n max\\_tokens\\=MAX\\_TOKENS,\n)\nprint(f\"Model ready. model\\={MODEL\\_NAME}, temperature\\={TEMPERATURE}, max\\_tokens\\={MAX\\_TOKENS}\")Test the model with a single prompt. This verifies connectivity and credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">msg = llm.invoke(\"Summarize why consistent JSON outputs help downstream systems.\")\nprint(type(msg))\nprint(msg.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "res \\= llm.invoke(\"Reply with the word: pong\")\nprint(res.content) \\# expected: \"pong\"Inspect the response metadata. This shows token usage and finish reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">print(\"Response metadata:\", getattr(msg, \"response_metadata\", {}))\nprint(\"Usage metadata:\", getattr(msg, \"usage_metadata\", {}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "print(res.response\\_metadata)\n\\# Example keys: token\\_usage, finish\\_reason, model\\_name, system\\_fingerprint\n\n## Using the Tool in Practice\n\nBuild a multi\\-turn conversation. Use explicit message roles to control context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">messages = [\n    SystemMessage(content=\"You are a concise assistant that extracts key facts.\"),\n    HumanMessage(content=\"I purchased earbuds last week. The left bud is dead.\"),\n    AIMessage(content=\"Noted. A device failure on the left earbud.\"),\n    HumanMessage(content=\"What information would you need to process a warranty claim?\")\n]\n\nreply = llm.invoke(messages)\nprint(reply.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "history \\= \\[\n SystemMessage(content\\=\"You are a helpful assistant.\"),\n HumanMessage(content\\=\"Who won the 2018 FIFA World Cup?\"),\n AIMessage(content\\=\"France won the 2018 FIFA World Cup.\"),\n HumanMessage(content\\=\"Who was the runner\\-up?\")\n]\nfollowup \\= llm.invoke(history)\nprint(followup.content)Create a prompt template with variables. Inject persona and user input at runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"{persona}\"),\n        (\"human\", \"{user_input}\")\n    ]\n)\n\nrendered = prompt.invoke({\n    \"persona\": \"You are a helpful customer support assistant.\",\n    \"user_input\": \"Customer reports a faulty left earbud after 7 days. Next step?\"\n})\nprint(rendered.to_messages())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "prompt \\= ChatPromptTemplate.from\\_messages(\n \\[\n (\"system\", \"{persona}\\\\nFollow the format instructions.\\\\n{format\\_instructions}\"),\n (\"human\", \"{input}\")\n ]\n)\n\n\\# Quick smoke test with a simple string parser\nsimple\\_chain \\= prompt \\| llm \\| StrOutputParser()\nprint(simple\\_chain.invoke({\n \"persona\": \"You are concise.\",\n \"format\\_instructions\": \"Answer in one short sentence.\",\n \"input\": \"What is LCEL?\"\n}))Parameterizing prompts and enforcing structured outputs are crucial for reliability. For more strategies on building robust LLM features and reducing errors in production, see the guide on prompt engineering with LLM APIs for reliable outputs at /article/prompt\\-engineering\\-with\\-llm\\-apis\\-how\\-to\\-get\\-reliable\\-outputs\\-4\\.\n\nCompose a chain with LCEL. Connect the prompt and model into a reusable workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">chain = prompt | llm\n\nresp = chain.invoke({\n    \"persona\": \"You are a helpful customer support assistant.\",\n    \"user_input\": \"The customer wants a refund for defective earbuds. What should we do?\"\n})\nprint(resp.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "base\\_chain \\= prompt \\| llm\nresp \\= base\\_chain.invoke({\n \"persona\": \"You give calm, factual answers.\",\n \"format\\_instructions\": \"No preamble.\",\n \"input\": \"Explain what a LangChain runnable is.\"\n})\nprint(resp.content)Define response schemas for structured fields. Guide the model to output specific fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">schemas = [\n    ResponseSchema(\n        name=\"type\",\n        description=\"One of complaint, inquiry, feedback.\"\n    ),\n    ResponseSchema(\n        name=\"product\",\n        description=\"Product or service mentioned, string.\"\n    ),\n    ResponseSchema(\n        name=\"action\",\n        description=\"Recommended action like refund, replace, clarify, route_to_support.\"\n    ),\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "schemas \\= \\[\n ResponseSchema(\n name\\=\"intent\",\n description\\=\"One of: complaint, inquiry, refund, feedback\"\n ),\n ResponseSchema(\n name\\=\"customer\\_name\",\n description\\=\"Full name of the sender if present. Else null\"\n ),\n ResponseSchema(\n name\\=\"email\",\n description\\=\"Sender email if present. Else null\"\n ),\n ResponseSchema(\n name\\=\"order\\_id\",\n description\\=\"Order or ticket identifier if present. Else null\"\n ),\n ResponseSchema(\n name\\=\"priority\",\n description\\=\"One of: low, medium, high\"\n ),\n ResponseSchema(\n name\\=\"action\",\n description\\=\"One of: route, refund, escalate, ignore\"\n ),\n ResponseSchema(\n name\\=\"summary\",\n description\\=\"One sentence summary of the email\"\n ),\n]\nparser \\= StructuredOutputParser.from\\_response\\_schemas(schemas)Add a StructuredOutputParser and generate format instructions. This enforces strict JSON output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">parser = StructuredOutputParser.from_response_schemas(schemas)\nformat_instructions = parser.get_format_instructions()\nprint(format_instructions[:200], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "format\\_instructions \\= parser.get\\_format\\_instructions()\nprint(\"Format instructions ready.\\\\n\")\nprint(format\\_instructions\\[:300], \"...\\\\n\") \\# previewBuild the full prompt\\-to\\-model\\-to\\-parser chain. Extract structured fields from customer emails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">extraction_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You extract structured fields from customer emails. \"\n            \"Return JSON that strictly follows these rules. {format_instructions}\"\n        ),\n        (\"human\", \"Email:\\n{email}\")\n    ]\n)\n\nextraction_chain = extraction_prompt | llm | parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "email\\_prompt \\= ChatPromptTemplate.from\\_messages(\n \\[\n (\"system\", \"You are a data extraction assistant. Produce only valid JSON that matches the schema.\"),\n (\"human\", \"Follow these format instructions:\\\\n{format\\_instructions}\\\\n\\\\nEmail:\\\\n{email\\_text}\")\n ]\n)\nextract\\_chain \\= email\\_prompt \\| llm \\| parserRun customer emails through the extraction chain. Print parsed Python dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">emails = [\n    \"Hi, my left earbud stopped working after a week. I want a refund please.\",\n    \"Hello, can you tell me if the Model X earbuds support wireless charging?\",\n    \"Just wanted to say the new firmware fixed my microphone issue. Thanks.\"\n]\n\nfor e in emails:\n    result = extraction_chain.invoke({\n        \"email\": e,\n        \"format_instructions\": format_instructions\n    })\n    print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "email\\_text \\= \"\"\"Subject: Refund request\nHi team,\nI was charged twice for order \\#A123\\-44 on May 2\\. Please refund the duplicate charge.\nThanks, Sam Lee, sam.lee@example.com\n\"\"\"\nresult \\= extract\\_chain.invoke({\n \"format\\_instructions\": format\\_instructions,\n \"email\\_text\": email\\_text\n})\nprint(result) \\# dict with your fieldsValidate schema coverage and values. Make sure the model output matches expectations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">def validate_result(d):\n    assert isinstance(d, dict)\n    assert d[\"type\"] in {\"complaint\", \"inquiry\", \"feedback\"}\n    assert isinstance(d[\"product\"], str)\n    assert isinstance(d[\"action\"], str)\n\nfor e in emails:\n    d = extraction_chain.invoke({\"email\": e, \"format_instructions\": format_instructions})\n    validate_result(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "allowed\\_intents \\= {\"complaint\", \"inquiry\", \"refund\", \"feedback\"}\nallowed\\_priority \\= {\"low\", \"medium\", \"high\"}\nallowed\\_action \\= {\"route\", \"refund\", \"escalate\", \"ignore\"}\n\nexpected\\_keys \\= {\"intent\",\"customer\\_name\",\"email\",\"order\\_id\",\"priority\",\"action\",\"summary\"}\nassert set(result.keys()) \\=\\= expected\\_keys, f\"Missing keys. got\\={set(result.keys())}\"\n\nassert result\\[\"intent\"] in allowed\\_intents, f\"Bad intent. {result\\['intent']}\"\nassert result\\[\"priority\"] in allowed\\_priority, f\"Bad priority. {result\\['priority']}\"\nassert result\\[\"action\"] in allowed\\_action, f\"Bad action. {result\\['action']}\"\n\nprint(\"Validation passed.\")\n\n## Run and Evaluate\n\nInspect usage and latency without parsing. This helps you evaluate cost and performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">from time import perf_counter\n\nraw_chain = extraction_prompt | llm\n\nstart = perf_counter()\nraw_msg = raw_chain.invoke({\"email\": emails[0], \"format_instructions\": format_instructions})\nelapsed = perf_counter() - start\n\nprint(\"Latency seconds:\", round(elapsed, 3))\nprint(\"Usage:\", getattr(raw_msg, \"usage_metadata\", {}))\n\nparsed = parser.invoke(raw_msg)\nprint(parsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import time\n\nt0 \\= time.time()\nmsg \\= (email\\_prompt \\| llm).invoke({\n \"format\\_instructions\": format\\_instructions,\n \"email\\_text\": email\\_text\n})\nlatency\\_ms \\= int((time.time() \\- t0\\) \\* 1000\\)\nusage \\= msg.response\\_metadata.get(\"token\\_usage\", {})\nprint(f\"Latency: {latency\\_ms} ms. Usage: {usage}\")Build a reusable function for production calls. Include validation and usage tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">def extract_email_fields(email: str) -> dict:\n    raw = raw_chain.invoke({\"email\": email, \"format_instructions\": format_instructions})\n    usage = getattr(raw, \"usage_metadata\", {})\n    parsed = parser.invoke(raw)\n    validate_result(parsed)\n    return {\"data\": parsed, \"usage\": usage}\n\nprint(extract_email_fields(\"The Model X case will not charge. Need a replacement.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "from typing import Dict, Any, Tuple\n\ndef process\\_email(\n email\\_text: str,\n persona: str \\= \"You are a data extraction assistant.\",\n llm\\_model: ChatOpenAI \\= llm,\n response\\_parser: StructuredOutputParser \\= parser,\n prompt\\_template: ChatPromptTemplate \\= email\\_prompt,\n retries: int \\= 1\n) \\-\\> Tuple\\[Dict\\[str, Any], Dict\\[str, Any]]:\n \"\"\"Returns (parsed\\_data, telemetry).\"\"\"\n fmt \\= response\\_parser.get\\_format\\_instructions()\n chain \\= prompt\\_template \\| llm\\_model \\| response\\_parser\n\n last\\_error \\= None\n for attempt in range(retries \\+ 1\\):\n t0 \\= time.time()\n try:\n \\# You can also include persona as a system tweak if needed\n parsed \\= chain.invoke({\n \"format\\_instructions\": fmt,\n \"email\\_text\": email\\_text\n })\n latency\\_ms \\= int((time.time() \\- t0\\) \\* 1000\\)\n \\# Capture usage from the last LLM call by running model separately\n msg \\= (prompt\\_template \\| llm\\_model).invoke({\n \"format\\_instructions\": fmt,\n \"email\\_text\": email\\_text\n })\n usage \\= msg.response\\_metadata.get(\"token\\_usage\", {})\n telemetry \\= {\n \"latency\\_ms\": latency\\_ms,\n \"usage\": usage,\n \"attempts\": attempt \\+ 1,\n \"model\": llm\\_model.model,\n }\n return parsed, telemetry\n except Exception as e:\n last\\_error \\= e\n if attempt \\< retries:\n continue\n\n raise RuntimeError(f\"Failed to parse after {retries\\+1} attempts. Last error: {last\\_error}\")Add a persona layer for domain\\-specific language. Tailor tone and vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">persona = \"You are a support triage assistant for consumer audio devices.\"\npersona_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", persona + \" Return strict JSON. {format_instructions}\"),\n        (\"human\", \"Email:\\n{email}\")\n    ]\n)\npersona_chain = persona_prompt | llm | parser\n\nprint(persona_chain.invoke({\"email\": emails[1], \"format_instructions\": format_instructions}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "persona\\_prompt \\= ChatPromptTemplate.from\\_messages(\n \\[\n (\"system\", \"{persona} Produce only valid JSON that matches the schema.\"),\n (\"human\", \"Follow these format instructions:\\\\n{format\\_instructions}\\\\n\\\\nEmail:\\\\n{email\\_text}\")\n ]\n)\n\ndef process\\_email\\_with\\_persona(email\\_text: str, persona: str) \\-\\> Dict:\n chain \\= persona\\_prompt \\| llm \\| parser\n return chain.invoke({\n \"persona\": persona,\n \"format\\_instructions\": parser.get\\_format\\_instructions(),\n \"email\\_text\": email\\_text\n })\n\nprint(process\\_email\\_with\\_persona(email\\_text, \"You specialize in ecommerce support. Prefer precise field extraction.\"))Run the workflow with your own examples. Include edge cases to test robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">my_emails = [\n    \"Order 1234. Model X earbuds arrived scratched. I want a refund.\",\n    \"Do the Model Y earbuds pair with two phones at once?\",\n    \"Love the sound on Model Z. Battery could be better, just feedback.\"\n]\n\nfor e in my_emails:\n    print(extraction_chain.invoke({\"email\": e, \"format_instructions\": format_instructions}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "examples \\= \\[\n \"Subject: Where is my order?\\\\nI ordered on April 2\\. Order 9981\\. No delivery yet. \\- Maria\",\n \"Please cancel order \\#ZX\\-77\\. Wrong size. Email me at alex@domain.com\",\n \"Thanks for the quick support. Everything is resolved now. \\- Ray\",\n \"I was overcharged. Twice on the same card. Order A\\-55\\. Need a refund asap.\"\n]\nfor e in examples:\n data, meta \\= process\\_email(e, retries\\=1\\)\n print(data, meta)Track basic telemetry for each invocation. Return latency, usage, and parsed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">def timed_invoke(email):\n    import time\n    t0 = time.perf_counter()\n    raw = raw_chain.invoke({\"email\": email, \"format_instructions\": format_instructions})\n    dt = time.perf_counter() - t0\n    usage = getattr(raw, \"usage_metadata\", {})\n    return dt, usage, parser.invoke(raw)\n\nfor e in emails:\n    dt, usage, data = timed_invoke(e)\n    print({\"latency_s\": round(dt, 3), \"usage\": usage, \"data\": data})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sample, telemetry \\= process\\_email(email\\_text, retries\\=1\\)\nprint(\"Parsed:\", sample)\nprint(\"Telemetry:\", telemetry)Handle invalid JSON gracefully with retries. Use a corrective prompt if parsing fails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">from langchain.output_parsers import OutputParserException\n\ndef safe_extract(email, max_retries=1):\n    for attempt in range(max_retries + 1):\n        try:\n            return extraction_chain.invoke({\"email\": email, \"format_instructions\": format_instructions})\n        except OutputParserException:\n            corrective = ChatPromptTemplate.from_messages(\n                [\n                    (\"system\", \"Return valid JSON only. Do not include commentary. {format_instructions}\"),\n                    (\"human\", \"Email:\\n{email}\")\n                ]\n            )\n            retry_chain = corrective | llm | parser\n            if attempt < max_retries:\n                result = retry_chain.invoke({\"email\": email, \"format_instructions\": format_instructions})\n                return result\n            raise\n\nprint(safe_extract(\"Refund me please. Model X left earbud broke in a week.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "from langchain.schema.output\\_parser import OutputParserException\n\ndef robust\\_process\\_email(email\\_text: str, max\\_retries: int \\= 2\\) \\-\\> Dict:\n fmt \\= parser.get\\_format\\_instructions()\n\n corrective\\_prompt \\= ChatPromptTemplate.from\\_messages(\n \\[\n (\"system\", \"You are a strict JSON generator. Produce only valid JSON. No prose.\"),\n (\"human\", \"If the prior output was invalid, correct it.\\\\nSchema:\\\\n{format\\_instructions}\\\\n\\\\nEmail:\\\\n{email\\_text}\")\n ]\n )\n\n base \\= email\\_prompt \\| llm \\| parser\n fixer \\= corrective\\_prompt \\| llm \\| parser\n\n for attempt in range(max\\_retries \\+ 1\\):\n try:\n return base.invoke({\"format\\_instructions\": fmt, \"email\\_text\": email\\_text})\n except OutputParserException:\n if attempt \\< max\\_retries:\n return fixer.invoke({\"format\\_instructions\": fmt, \"email\\_text\": email\\_text})\n raise\n\nprint(robust\\_process\\_email(\"Refund my order \\#123\\. I was billed twice. joe@ex.com\"))Expand the schema when business needs change. Add an urgency field based on sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">schemas_extended = schemas + [\n    ResponseSchema(name=\"urgency\", description=\"low, medium, high based on sentiment and urgency cues.\")\n]\nparser_ext = StructuredOutputParser.from_response_schemas(schemas_extended)\nfmt_ext = parser_ext.get_format_instructions()\n\nprompt_ext = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"Extract fields and urgency. Return strict JSON. {format_instructions}\"),\n        (\"human\", \"Email:\\n{email}\")\n    ]\n)\nchain_ext = prompt_ext | llm | parser_ext\n\nprint(chain_ext.invoke({\"email\": emails[0], \"format_instructions\": fmt_ext}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "expanded\\_schemas \\= schemas \\+ \\[\n ResponseSchema(\n name\\=\"urgency\",\n description\\=\"One of: low, medium, high. Based on sentiment and explicit urgency.\"\n ),\n]\nexpanded\\_parser \\= StructuredOutputParser.from\\_response\\_schemas(expanded\\_schemas)\nexpanded\\_format\\_instructions \\= expanded\\_parser.get\\_format\\_instructions()\n\nexpanded\\_prompt \\= ChatPromptTemplate.from\\_messages(\n \\[\n (\"system\", \"You extract fields and determine urgency. Output valid JSON only.\"),\n (\"human\", \"Format instructions:\\\\n{format\\_instructions}\\\\n\\\\nEmail:\\\\n{email\\_text}\")\n ]\n)\nexpanded\\_chain \\= expanded\\_prompt \\| llm \\| expanded\\_parser\n\nprint(expanded\\_chain.invoke({\n \"format\\_instructions\": expanded\\_format\\_instructions,\n \"email\\_text\": \"This is urgent. Order \\#A33 is missing and the event is tonight.\"\n}))Add minimal unit tests for the extraction chain. Validate field values and types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">def test_extraction():\n    sample = \"Left earbud on Model X stopped working. Please replace.\"\n    d = extraction_chain.invoke({\"email\": sample, \"format_instructions\": format_instructions})\n    assert d[\"type\"] in {\"complaint\", \"inquiry\", \"feedback\"}\n    assert isinstance(d[\"product\"], str)\n    assert d[\"action\"] in {\"refund\", \"replace\", \"clarify\", \"route_to_support\"}\n\ntest_extraction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\# Save as tests/test\\_extraction.py\nimport os\nimport re\n\ndef test\\_refund\\_detection():\n email \\= \"Refund requested for order \\#A1\\. Double charged. user@mail.com\"\n data \\= extract\\_chain.invoke({\"format\\_instructions\": format\\_instructions, \"email\\_text\": email})\n assert data\\[\"intent\"] in {\"refund\", \"complaint\"}\n assert data\\[\"action\"] in {\"refund\", \"escalate\", \"route\"}\n assert re.match(r\".\\+@.\\+..\\+\", data\\[\"email\"]) is not None\n\ndef test\\_missing\\_fields\\_are\\_null():\n email \\= \"Thanks for the help. Everything works.\"\n data \\= extract\\_chain.invoke({\"format\\_instructions\": format\\_instructions, \"email\\_text\": email})\n assert data\\[\"order\\_id\"] in (None, \"\", \"null\", \"NULL\") or data\\[\"order\\_id\"] is NoneUse prompt fixtures for reproducibility. Keep test prompts stable across changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">fixtures = [\n    {\n        \"email\": \"Model X case not charging. Need a replacement.\",\n        \"expect_type\": {\"complaint\"},\n    },\n    {\n        \"email\": \"Do Model Y earbuds support USB C charging?\",\n        \"expect_type\": {\"inquiry\"},\n    },\n]\n\nfor fx in fixtures:\n    d = extraction_chain.invoke({\"email\": fx[\"email\"], \"format_instructions\": format_instructions})\n    assert d[\"type\"] in fx[\"expect_type\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\# tests/fixtures.py\nEXTRACTION\\_FORMAT \\= parser.get\\_format\\_instructions()\nSIMPLE\\_EMAIL \\= \"Order 42 arrived damaged. Need a replacement. jane@example.com\"Batch processing with simple loops. Process multiple emails in one pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">results = [extraction_chain.invoke({\"email\": e, \"format_instructions\": format_instructions}) for e in emails]\nprint(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "emails \\= \\[\n \"Please cancel order 77\\. Wrong color.\",\n \"Where is order 88? No update in 10 days.\",\n \"I was charged twice for order 99\\.\"\n]\nbatched \\= extract\\_chain.batch(\\[{\"format\\_instructions\": format\\_instructions, \"email\\_text\": e} for e in emails], batch\\_size\\=3\\)\nfor d in batched:\n print(d)Swap models with one line changes. Experiment with different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">llm_alt = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=300)\nextraction_chain_alt = extraction_prompt | llm_alt | parser\n\nprint(extraction_chain_alt.invoke({\"email\": emails[2], \"format_instructions\": format_instructions}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\# Switch to a larger model if you need better accuracy\nllm \\= ChatOpenAI(model\\=\"gpt\\-4o\", temperature\\=0, max\\_tokens\\=MAX\\_TOKENS)\n\\# Or try a cost\\-efficient mini model\n\\# llm \\= ChatOpenAI(model\\=\"gpt\\-4o\\-mini\", temperature\\=0, max\\_tokens\\=MAX\\_TOKENS)\n\n\\# Rebuild the chain with the new model\nextract\\_chain \\= email\\_prompt \\| llm \\| parserVersion prompts and parsers in code. Enable traceability and rollback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">PROMPT_VERSION = \"v1.2\"\nSCHEMA_VERSION = \"v1.1\"\n\nprint({\"prompt_version\": PROMPT_VERSION, \"schema_version\": SCHEMA_VERSION})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PROMPT\\_VERSION \\= \"email\\_extractor\\_v1\"\nSCHEMA\\_VERSION \\= \"schema\\_v1\"\n\ndef log\\_meta():\n return {\n \"prompt\\_version\": PROMPT\\_VERSION,\n \"schema\\_version\": SCHEMA\\_VERSION,\n \"model\": llm.model\n }\n\nprint(\"Meta:\", log\\_meta())Lightweight configuration with environment variables. Change model and temperature without code edits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">MODEL_NAME = os.getenv(\"MODEL_NAME\", \"gpt-4o-mini\")\nTEMP = float(os.getenv(\"TEMP\", \"0\"))\nllm_cfg = ChatOpenAI(model=MODEL_NAME, temperature=TEMP, max_tokens=300)\ncfg_chain = extraction_prompt | llm_cfg | parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\# .env\n\\# MODEL\\_NAME\\=gpt\\-4o\\-mini\n\\# TEMPERATURE\\=0\n\\# MAX\\_TOKENS\\=256\n\n\\# In code, you already read these values earlier:\n\\# MODEL\\_NAME \\= os.getenv(\"MODEL\\_NAME\", \"gpt\\-4o\\-mini\")\n\\# TEMPERATURE \\= float(os.getenv(\"TEMPERATURE\", \"0\"))Sample test cases to try now. Cover complaints, inquiries, and feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <pre><code class=\"language-python\">cases = [\n    \"I love the sound on Model X, but the right bud randomly disconnects. Can you replace it?\",\n    \"Do Model Y earbuds work with iOS 17? If yes, how to pair?\",\n    \"Great update, pairing is faster now. Just a note for your team.\"\n]\n\nfor c in cases:\n    print(cfg_chain.invoke({\"email\": c, \"format_instructions\": format_instructions}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "samples \\= \\[\n \"My package arrived damaged. Order \\#12345\\. Please replace it. \\- Kim\",\n \"What is your return window for electronics? \\- Pat\",\n \"Double charge on my card for order X\\-77\\. Need a refund now. rita@mail.com\",\n \"Just wanted to say thank you for the fast support.\"\n]\nfor s in samples:\n data \\= extract\\_chain.invoke({\"format\\_instructions\": format\\_instructions, \"email\\_text\": s})\n print(data)\n\n## Conclusion\n\nYou built a production\\-ready LLM workflow that turns customer emails into strict JSON dictionaries. You used LangChain prompt templates, structured output parsers, and LCEL chains to keep the code readable and testable. You validated outputs, tracked token usage, and handled parsing errors with retries. This approach stays maintainable because prompts, schemas, and models are decoupled. You can version prompts, swap models, and extend schemas without rewriting integration logic.\n\nIf your workflow requires grounding responses in external knowledge, consider integrating retrieval\\-augmented generation. The ultimate guide to vector store retrieval for RAG systems at /article/rag\\-101\\-build\\-an\\-index\\-run\\-semantic\\-search\\-and\\-use\\-langchain\\-to\\-automate\\-it explains how to implement semantic search and chunking to minimize hallucinations and boost reliability.\n\nIf you want to go beyond prompt\\-driven chains and customize your LLM behavior at a deeper level, consider learning how to fine\\-tune models yourself. The step\\-by\\-step guide to fine\\-tuning large language models at /article/fine\\-tuning\\-large\\-language\\-models\\-a\\-step\\-by\\-step\\-guide\\-2025\\-5 walks you through dataset preparation, training, and evaluation using Hugging Face tools."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build Reliable LangChain LLM Workflows in 15 Minutes Flat",
    "description": "Get hands-on with LangChain: install, configure models, build prompt-driven chains, and parse structured outputsâ€”launch reliable Python LLM workflows fast today.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}