{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Lost in the Middle: How to Fix It with Prompt Structuring (Guide)\n\n**Description:** Stop mid-prompt failures. Learn placement rules, retrieval, and labeling patterns that beat position bias, cut tokens, and boost long-context accuracy.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generative AI models don't read your prompt the way you do. They favor the beginning and end, often ignoring critical details buried in the middleâ€”a phenomenon known as **\"Lost in the Middle.\"** This U-shaped recall pattern means that moving a constraint from the center to the top or bottom can flip your output from wrong to correct, even when the logical content is identical.\n\nFor AI Builders designing production systems, this isn't an academic curiosityâ€”it's a reliability risk. If your RAG pipeline stuffs evidence into the middle of a long context, or your agent accumulates conversation history without re-anchoring instructions, accuracy will degrade silently. Understanding why models exhibit this behavior and how to structure prompts accordingly is essential to building systems that perform consistently at scale.\n\n## Why This Matters\n\n**Concrete scenario:** Your customer-support agent retrieves five relevant KB articles and inserts them between the system instructions and the user query. In testing, the model ignores the third articleâ€”which contains the correct troubleshooting stepâ€”and hallucinates a workaround from the first article instead. Swapping article order fixes the issue, but only until the next retrieval shuffle.\n\n**Observable symptoms in logs:**\n- Accuracy drops as context length grows, even when all required information is present\n- Reordering chunks or instructions changes outputs unpredictably\n- The model repeats constraints from the opening or closing lines but skips mid-prompt rules\n- Citation or grounding scores are lower for evidence placed in the middle third of the prompt\n\nThese failures stem from how transformer attention and next-token prediction distribute focus across your input.\n\n## How It Works\n\n**Next-token prediction favors edges.** Language models are trained to predict the next token by attending to prior context. Statistically, the most recent tokens and the initial framing (system prompt, task definition) carry the highest predictive weight. Middle sectionsâ€”especially in long contextsâ€”compete for limited attention and often lose.\n\n**Recency and setup dominate salience.** The opening establishes the task and primes the model's internal state; the closing provides the immediate trigger for generation. Information in the middle must fight both recency bias (the model prioritizes what it just read) and primacy bias (the setup anchors interpretation). When attention budgets are tight, middle content is effectively down-weighted.\n\n**Attention competition across layers.** Transformers allocate attention across all positions, but deeper layers increasingly focus on a subset of high-salience tokens. In practice, this creates a U-shaped recall curve: top and bottom chunks are retrieved reliably, while middle chunks are accessed inconsistentlyâ€”even when semantically relevant.\n\n**Architecture and training amplify the effect.** Positional encodings (e.g., RoPE) and training data distributions (which often place key information at boundaries) reinforce edge preference. Larger context windows can exacerbate the problem if the model hasn't been explicitly trained to attend uniformly across long spans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\ngraph LR\n    A[Prompt Start:<br/>System Instructions] -->|High Attention| B[Model Output]\n    C[Middle Section:<br/>Evidence, History] -->|Low Attention| B\n    D[Prompt End:<br/>Task, Constraints] -->|High Attention| B\n    style C fill:#ffcccc\n    style A fill:#ccffcc\n    style D fill:#ccffcc\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What You Should Do\n\n**Anchor critical instructions at the top.** Place system rules, output format requirements, and non-negotiable constraints in the opening lines. This ensures they are encoded into the model's initial state and remain salient throughout generation.\n\n**Position evidence immediately before the task.** In RAG workflows, insert retrieved chunks or context within a few hundred tokens of the final user query or generation trigger. This leverages recency bias to keep evidence fresh in attention when the model begins producing output.\n\n**Summarize and repeat essentials at the end.** Add a brief TL;DR (â‰¤2 lines) restating the core task and any critical constraints just before the generation prompt. This reinforces instructions without requiring the model to recall them from earlier in the context.\n\n**Preflight your prompt structure.** Before deploying, run a simple position-sensitivity check: place the same key fact at the top, middle, and bottom of your prompt and measure output correctness. If middle placement causes a >10 percentage point accuracy drop, restructure or reduce context length.\n\nInline example of a preflight config:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "positions: [top, middle, bottom]\nfact: \"User timezone is UTC+8\"\nexpected_output_contains: \"UTC+8\"\nthreshold: 90% pass rate across all positions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion â€“ Key Takeaways\n\nLost in the Middle is a structural failure mode, not a prompt-wording issue. Models reliably attend to the start and end of your input, so place instructions and evidence accordingly. Anchor rules at the top, position context near the task, and reinforce constraints at the end. Test position sensitivity in CI to catch regressions before production.\n\n**When to care:**\n- Your prompts exceed 2,000 tokens or include multiple retrieved chunks\n- You're building agents that accumulate conversation history over multiple turns\n- Accuracy varies unpredictably when you reorder logically equivalent sections\n- You need consistent grounding and citation behavior across long contexts"
      ]
    }
  ],
  "metadata": {
    "title": "Lost in the Middle: How to Fix It with Prompt Structuring (Guide)",
    "description": "Stop mid-prompt failures. Learn placement rules, retrieval, and labeling patterns that beat position bias, cut tokens, and boost long-context accuracy.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}