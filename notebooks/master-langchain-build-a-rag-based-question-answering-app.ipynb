{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vQmc9EAEKp6"
      },
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n",
        "\n",
        "**Title:** Master LangChain: Build a RAG-Based Question Answering App\n",
        "\n",
        "**Description:** Unlock the power of LangChain for precise question answering. Learn to integrate retrieval-augmented generation with real-world data sources in this step-by-step guide.\n",
        "\n",
        "---\n",
        "\n",
        "*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysjl-BsdEKp7"
      },
      "source": [
        "# Building a Retrieval-Augmented Generation (RAG) System with LangChain and ChromaDB\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In the rapidly evolving field of artificial intelligence, Retrieval-Augmented Generation (RAG) stands out as a powerful technique for enhancing language models with external knowledge. This tutorial will guide you through building a RAG system using LangChain and ChromaDB, enabling you to create applications that are not only intelligent but also contextually aware. By the end of this tutorial, you'll have a solid understanding of integrating language models with vector databases to solve real-world problems like question answering and document summarization.\n",
        "\n",
        "## Installation\n",
        "\n",
        "To get started, you'll need to install the necessary libraries. Run the following commands in a code cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9VLRXWxEKp7"
      },
      "outputs": [],
      "source": [
        "!pip install langchain transformers torch chromadb langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8YiXME1EKp8"
      },
      "source": [
        "## Project Setup\n",
        "\n",
        "Before diving into the code, ensure you have the following prerequisites:\n",
        "\n",
        "- An OpenAI API key for accessing GPT-3.\n",
        "- A data source for creating embeddings.\n",
        "\n",
        "Define your environment variables and configuration files as needed.\n",
        "\n",
        "## Step-by-Step Build\n",
        "\n",
        "### Data Ingestion and Embedding Creation\n",
        "\n",
        "First, we'll ingest data and create embeddings for storage in a vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmHYCGF2EKp8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "def load_data(source):\n",
        "    \"\"\"Load data from the specified source.\n",
        "\n",
        "    Args:\n",
        "        source (str): The path or identifier for the data source.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of text data loaded from the source.\n",
        "    \"\"\"\n",
        "    # Placeholder for data loading logic\n",
        "    return [\"Sample text 1\", \"Sample text 2\"]\n",
        "\n",
        "# Load your data\n",
        "data = load_data('your_data_source')\n",
        "\n",
        "# Initialize tokenizer and model for embedding creation\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess and create embeddings\n",
        "embeddings = []\n",
        "for text in data:\n",
        "    # Tokenize the text and create embeddings\n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        embedding = model(**inputs).last_hidden_state.mean(dim=1)\n",
        "    embeddings.append(embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL3RkZkWEKp8"
      },
      "source": [
        "### Language Model Integration\n",
        "\n",
        "Integrate a language model using LangChain to handle the generation aspect of RAG. For those interested in tailoring language models to specific domains, our article on [fine-tuning large language models for domain-specific applications](/blog/44830763/mastering-fine-tuning-of-large-language-models-for-domain-applications) provides valuable insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ8If2-yEKp8"
      },
      "outputs": [],
      "source": [
        "from langchain import LLMChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "\n",
        "# Initialize LangChain with your model\n",
        "# Get the OpenAI API key securely\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize the ChatOpenAI language model\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", api_key=openai_api_key)\n",
        "\n",
        "# Define a simple prompt template\n",
        "template = \"\"\"Use the following context to answer the question:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Initialize the LLMChain with the language model and prompt\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgEPmbnXEKp8"
      },
      "source": [
        "### Vector Database Setup\n",
        "\n",
        "Set up a vector database for storing and querying embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiHPw6DcEKp8"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "import torch\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "client = chromadb.Client()\n",
        "\n",
        "# Define the embedding function using the same model as before\n",
        "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"bert-base-uncased\")\n",
        "\n",
        "# Delete the collection if it already exists to avoid the embedding function conflict\n",
        "try:\n",
        "    client.delete_collection(\"my_rag_collection\")\n",
        "    print(\"Deleted existing collection 'my_rag_collection'.\")\n",
        "except:\n",
        "    print(\"Collection 'my_rag_collection' did not exist or could not be deleted.\")\n",
        "    pass # Collection did not exist\n",
        "\n",
        "# Get or create a collection with the specified embedding function\n",
        "collection = client.get_or_create_collection(\"my_rag_collection\", embedding_function=embedding_function)\n",
        "print(\"Created or got collection 'my_rag_collection'.\")\n",
        "\n",
        "# Store embeddings in the vector database\n",
        "# ChromaDB requires documents and ids when adding embeddings\n",
        "# Assuming embeddings correspond to the sample data loaded earlier\n",
        "# You would replace \"doc_texts\" and \"doc_ids\" with your actual document texts and unique identifiers\n",
        "doc_texts = [\"Sample text 1\", \"Sample text 2\"]\n",
        "doc_ids = [\"doc_1\", \"doc_2\"]\n",
        "\n",
        "# Convert embeddings from tensors to a list of lists of floats for ChromaDB\n",
        "embeddings_list = [embedding.squeeze().tolist() for embedding in embeddings]\n",
        "\n",
        "# Add documents and embeddings to the collection\n",
        "# Note: If you are using an embedding function with the collection, you don't need to provide embeddings here.\n",
        "# ChromaDB will generate embeddings from the documents using the specified embedding function.\n",
        "# Since we already have pre-computed embeddings, I will add them directly.\n",
        "# If you want ChromaDB to compute embeddings, you would use: collection.add(documents=doc_texts, ids=doc_ids)\n",
        "collection.add(\n",
        "    embeddings=embeddings_list,\n",
        "    documents=doc_texts,\n",
        "    ids=doc_ids\n",
        ")\n",
        "\n",
        "print(f\"Added {len(doc_ids)} documents to the collection.\")\n",
        "\n",
        "# Query the database with a sample query\n",
        "# Replace 'your_query' with an actual query string\n",
        "query_text = 'your_query'\n",
        "# When querying, ChromaDB will use the collection's embedding function to embed the query text\n",
        "query_result = collection.query(query_texts=[query_text], n_results=1)\n",
        "\n",
        "print(\"Query Result:\", query_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEjOEz8_EKp8"
      },
      "source": [
        "### Full End-to-End Application\n",
        "\n",
        "Now, let's put all components together to build a complete RAG application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59Vzdd-OEKp8"
      },
      "outputs": [],
      "source": [
        "def answer_question(question):\n",
        "    \"\"\"Answer a question using retrieval-augmented generation.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to be answered.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer to the question.\n",
        "    \"\"\"\n",
        "    # Retrieve relevant information from the vector database\n",
        "    # Use the 'collection' object created in the previous cell\n",
        "    retrieved_result = collection.query(query_texts=[question], n_results=1) # Get top 1 relevant document\n",
        "    retrieved_docs = retrieved_result['documents'][0] if retrieved_result and retrieved_result['documents'] else [\"No relevant information found.\"]\n",
        "\n",
        "    # Generate a response using the language model\n",
        "    # Pass the retrieved context and question to the LLMChain\n",
        "    response = llm_chain.invoke({\"context\": \"\\n\".join(retrieved_docs), \"question\": question})\n",
        "\n",
        "    return response['text'] # Assuming the response structure from invoke\n",
        "\n",
        "# Example usage of the question-answering function\n",
        "# Replace 'your_query' with an actual question\n",
        "print(answer_question(\"What is the capital of France?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMzlD6utEKp9"
      },
      "source": [
        "### Testing & Validation\n",
        "\n",
        "Test and validate the application with various queries to ensure robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH5LGvoVEKp9"
      },
      "outputs": [],
      "source": [
        "# Test cases for the question-answering application\n",
        "test_queries = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Why is the sky blue?\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Response: {answer_question(query)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRwQJTfXEKp9"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we've built a RAG system using LangChain and ChromaDB, demonstrating how to integrate language models with vector databases for enhanced AI applications. While this guide provides a foundational understanding, consider exploring advanced topics such as integrating additional data sources or optimizing for different performance metrics. This will help you create more scalable and efficient AI solutions."
      ]
    }
  ],
  "metadata": {
    "title": "Master LangChain: Build a RAG-Based Question Answering App",
    "description": "Unlock the power of LangChain for precise question answering. Learn to integrate retrieval-augmented generation with real-world data sources in this step-by-step guide.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}