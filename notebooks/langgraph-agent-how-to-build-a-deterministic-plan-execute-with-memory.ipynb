{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** LangGraph Agent: How to Build a Deterministic Plan-Execute with Memory\n\n**Description:** Build a production-ready LangGraph agent that plans, executes, validates tools, persists state, remembers context, and serves a deterministic JSON /agent.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Separating plan from execution lets you lock an explicit sequence of steps before any tool fires. That reduces unpredictable LLM back\\-and\\-forth. You'll ask the model to output a structured, multi\\-step plan. Then the executor runs tools deterministically against that plan, with validation and guardrails, making behavior repeatable. For more on how prompt structure and information placement can impact model performance, see our analysis of [position bias in long prompts](/article/lost-in-the-middle-placing-critical-info-in-long-prompts).\n\nThis pattern gives you:\n\n* **Determinism**: The plan is fixed before execution, so you can log, audit, and replay it.\n\n* **Safety**: Every tool call is validated with Pydantic schemas before and after execution.\n\n* **Recovery**: If a step fails, the agent can replan and continue rather than crashing.\n\n* **Memory**: LangGraph checkpointers persist state across turns, enabling multi\\-turn workflows.\n\nYou'll build a FastAPI `/agent` endpoint backed by a LangGraph state graph that plans, executes, and optionally replans on error. The result is a working, production\\-ready agent you can extend with new tools, memory backends, and observability.\n\n## How It Works\n\nHere's the high\\-level flow:\n\n1. **User sends** `thread_id` and `query` to `/agent`\n\n2. **Planner node** invokes the LLM with structured output to generate a `PlanModel` (list of steps)\n\n3. **Executor node** runs each step:\n\n    * For `tool` steps: validate input, call the tool, validate output, store result\n\n    * For `respond` step: synthesize final answer from step results using the LLM\n\n4. **On error**: route to **Replan node**, which generates a revised plan and re\\-enters execution\n\n5. **Checkpointer** persists state per `thread_id` for conversation memory\n\n6. **API returns** plan, step results, final answer, error (if any), and trace\n\nThis architecture keeps planning and execution separate, making the system auditable, testable, and easy to extend.\n\n## Setup \\& Installation\n\nRun this in a Colab notebook or local Python 3\\.10\\+ environment. Install dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q langgraph langchain-openai pydantic httpx fastapi uvicorn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your OpenAI API key. In Colab, store it in Secrets as `OPENAI_API_KEY`. In a local environment, export it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\ntry:\n    from google.colab import userdata\n    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\nexcept ImportError:\n    pass  # Not in Colab; ensure OPENAI_API_KEY is set in your shell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify your environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert \"OPENAI_API_KEY\" in os.environ, \"Set OPENAI_API_KEY in environment or Colab Secrets\"\nprint(\"âœ“ Environment ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step\\-by\\-Step Implementation\n\n### Define the agent's shared state and plan models\n\nWe use `TypedDict` for the state and Pydantic for structured plan output. This ensures type safety and validation at every step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Dict, Any, Optional\nfrom pydantic import BaseModel, Field\n\nclass PlanStep(BaseModel):\n    \"\"\"\n    Represents a single step in the agent's plan.\n    \"\"\"\n    id: int = Field(..., description=\"Step index starting at 1\")\n    action: str = Field(..., description=\"Either 'tool' or 'respond'\")\n    name: Optional[str] = Field(None, description=\"Tool name if action is 'tool'\")\n    args: Optional[Dict[str, Any]] = Field(None, description=\"Arguments for the tool\")\n    description: str = Field(..., description=\"Short description of the step\")\n\nclass PlanModel(BaseModel):\n    \"\"\"\n    Represents the overall plan, including rationale and steps.\n    \"\"\"\n    rationale: str\n    steps: List[PlanStep]\n\nclass AgentState(TypedDict, total=False):\n    \"\"\"\n    Shared state for the agent, passed between nodes.\n    \"\"\"\n    user_input: str\n    plan: List[PlanStep]\n    step_results: List[Dict[str, Any]]\n    final_answer: Optional[str]\n    error: Optional[str]\n    trace: List[Dict[str, Any]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define safe tools with explicit schemas\n\nEach tool has Pydantic input/output models for validation. This prevents malformed data from propagating through the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\nimport httpx\nimport math\n\nclass SumInput(BaseModel):\n    numbers: list[float] = Field(..., min_items=1)\n\nclass SumOutput(BaseModel):\n    total: float\n\ndef sum_numbers_tool(inp: SumInput) -> SumOutput:\n    \"\"\"Sums a list of numbers.\"\"\"\n    total = float(math.fsum(inp.numbers))\n    return SumOutput(total=total)\n\nclass KBQueryInput(BaseModel):\n    topic: str = Field(..., min_length=1)\n\nclass KBQueryOutput(BaseModel):\n    topic: str\n    content: str\n\nKB = {\n    \"refund_policy\": \"Refunds available within 30 days with receipt.\",\n    \"sla\": \"Standard support SLA is 24 hours response time.\",\n}\n\ndef kb_retrieve_tool(inp: KBQueryInput) -> KBQueryOutput:\n    \"\"\"Retrieves a KB article by topic.\"\"\"\n    topic = inp.topic.strip().lower()\n    if topic not in KB:\n        raise ValueError(f\"Topic '{topic}' not found\")\n    return KBQueryOutput(topic=topic, content=KB[topic])\n\nclass HttpGetInput(BaseModel):\n    url: str = Field(..., pattern=r\"^https://httpbin.org/.*\")\n\nclass HttpGetOutput(BaseModel):\n    status_code: int\n    json: dict\n\ndef http_get_json_tool(inp: HttpGetInput) -> HttpGetOutput:\n    \"\"\"Fetches JSON from a safe endpoint.\"\"\"\n    with httpx.Client(timeout=10.0) as client:\n        resp = client.get(inp.url)\n        data = resp.json() if \"application/json\" in resp.headers.get(\"content-type\", \"\") else {}\n        return HttpGetOutput(status_code=resp.status_code, json=data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wrap tools in a registry with validation\n\nThe registry validates inputs and outputs, catching errors before they propagate. This is critical for determinism and safety."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Callable, Type, Any\nfrom pydantic import ValidationError\n\nclass ToolError(Exception):\n    \"\"\"Custom exception for tool validation or execution errors.\"\"\"\n    pass\n\nclass Tool:\n    \"\"\"\n    Registry entry for a tool, including validation and execution.\n    \"\"\"\n    def __init__(self, name: str, description: str, input_model: Type[BaseModel], output_model: Type[BaseModel], fn: Callable[[Any], Any]):\n        self.name = name\n        self.description = description\n        self.input_model = input_model\n        self.output_model = output_model\n        self.fn = fn\n\n    def run(self, args: dict) -> dict:\n        \"\"\"Validates input, runs the tool, and validates output.\"\"\"\n        try:\n            validated_in = self.input_model(**args)\n        except ValidationError as ve:\n            raise ToolError(f\"Input validation failed for {self.name}: {ve}\") from ve\n        try:\n            raw_out = self.fn(validated_in)\n        except Exception as e:\n            raise ToolError(f\"Tool {self.name} execution failed: {e}\") from e\n        try:\n            validated_out = self.output_model.model_validate(raw_out)\n        except ValidationError as ve:\n            raise ToolError(f\"Output validation failed for {self.name}: {ve}\") from ve\n        return validated_out.model_dump()\n\nTOOL_REGISTRY: dict[str, Tool] = {\n    \"sum_numbers\": Tool(\n        name=\"sum_numbers\",\n        description=\"Return the sum of an array of numbers\",\n        input_model=SumInput,\n        output_model=SumOutput,\n        fn=sum_numbers_tool,\n    ),\n    \"kb_retrieve\": Tool(\n        name=\"kb_retrieve\",\n        description=\"Retrieve a short KB article by topic\",\n        input_model=KBQueryInput,\n        output_model=KBQueryOutput,\n        fn=kb_retrieve_tool,\n    ),\n    \"http_get_json\": Tool(\n        name=\"http_get_json\",\n        description=\"GET JSON from https://httpbin.org endpoints only\",\n        input_model=HttpGetInput,\n        output_model=HttpGetOutput,\n        fn=http_get_json_tool,\n    ),\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare the LLM with structured output\n\nWe use LangChain's OpenAI wrapper with `temperature=0` for deterministic planning. The `with_structured_output` method ensures the LLM returns a valid `PlanModel`. If you're deciding which language model to use for your agent, check out our comprehensive guide on [how to pick an LLM for your application](/article/how-to-choose-an-ai-model-for-your-app-speed-cost-reliability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\nplanner_llm = llm.with_structured_output(PlanModel)\n\nPLAN_PROMPT = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a planning assistant. You must output a feasible, minimal plan.\"),\n    (\"system\", \"Available tools:\\n{tool_summaries}\\nOnly call tools listed above.\"),\n    (\"user\", \"User request: {user_input}\\nProduce a plan with one or more steps. Use 'respond' as the last step.\"),\n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implement the planner node\n\nThe planner generates a structured plan from the user's input. It validates that all tool steps refer to known tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tool_summaries() -> str:\n    \"\"\"Returns a summary of available tools and their input schemas.\"\"\"\n    lines = []\n    for t in TOOL_REGISTRY.values():\n        lines.append(f\"- {t.name}: {t.description}; input={t.input_model.model_json_schema()['properties']}\")\n    return \"\\n\".join(lines)\n\ndef plan_node(state: AgentState) -> AgentState:\n    \"\"\"Planner node: generates a plan from user input using the LLM.\"\"\"\n    ui = state[\"user_input\"]\n    result = planner_llm.invoke(PLAN_PROMPT.format_messages(\n        tool_summaries=tool_summaries(),\n        user_input=ui,\n    ))\n    plan: PlanModel = result\n    steps = []\n    for s in plan.steps:\n        if s.action == \"tool\" and (not s.name or s.name not in TOOL_REGISTRY):\n            raise ValueError(f\"Planner proposed unknown tool: {s.name}\")\n        steps.append(s)\n    return {\n        \"plan\": steps,\n        \"trace\": (state.get(\"trace\") or []) + [{\"event\": \"plan\", \"plan\": [s.model_dump() for s in steps]}],\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implement the executor node\n\nThe executor runs each step in the plan. Tool steps are validated and executed via the registry. The final `respond` step synthesizes an answer from the step results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n\nANSWER_PROMPT = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a precise assistant. Use the provided step results to answer.\"),\n    (\"user\", \"Question: {user_input}\\nStep results:\\n{step_results}\\nCompose a concise answer.\"),\n])\nanswer_llm = llm\n\ndef execute_node(state: AgentState) -> AgentState:\n    \"\"\"Executor node: runs the plan step by step, validates tool calls, and builds the final answer.\"\"\"\n    plan = state.get(\"plan\") or []\n    step_results = state.get(\"step_results\") or []\n    trace = state.get(\"trace\") or []\n    error = None\n    for step in plan:\n        if step.action == \"tool\":\n            if step.name not in TOOL_REGISTRY:\n                error = f\"Unknown tool {step.name}\"\n                trace.append({\"event\": \"tool_error\", \"step_id\": step.id, \"error\": error})\n                break\n            tool = TOOL_REGISTRY[step.name]\n            try:\n                result = tool.run(step.args or {})\n                step_results.append({\"step_id\": step.id, \"tool\": step.name, \"args\": step.args, \"output\": result})\n                trace.append({\"event\": \"tool_ok\", \"step_id\": step.id, \"tool\": step.name, \"output\": result})\n            except ToolError as te:\n                error = str(te)\n                trace.append({\"event\": \"tool_error\", \"step_id\": step.id, \"tool\": step.name, \"error\": error})\n                break\n        elif step.action == \"respond\":\n            sr_str = \"\\n\".join([f\"- Step {r['step_id']} ({r['tool']}): {r['output']}\" for r in step_results])\n            msg = ANSWER_PROMPT.format_messages(user_input=state[\"user_input\"], step_results=sr_str)\n            final = answer_llm.invoke(msg).content\n            trace.append({\"event\": \"respond\", \"text\": final})\n            return {\"step_results\": step_results, \"final_answer\": final, \"trace\": trace}\n        else:\n            error = f\"Unknown action {step.action}\"\n            trace.append({\"event\": \"plan_error\", \"error\": error})\n            break\n    if error:\n        return {\"step_results\": step_results, \"error\": error, \"trace\": trace}\n    return {\"step_results\": step_results, \"error\": \"Plan missing 'respond' step\", \"trace\": trace}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add a replan node for recovery\n\nIf execution fails, the replan node generates a revised plan based on completed steps and the error. This enables graceful recovery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REPLAN_PROMPT = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a repair planner. Create a minimal revised plan to complete the task.\"),\n    (\"system\", \"Available tools:\\n{tool_summaries}\"),\n    (\"user\", \"Original request: {user_input}\\nCompleted steps:\\n{done}\\nError: {error}\\nPropose a revised plan (include 'respond' as last step).\"),\n])\nreplanner_llm = llm.with_structured_output(PlanModel)\n\ndef replan_node(state: AgentState) -> AgentState:\n    \"\"\"Replanner node: generates a revised plan after an error.\"\"\"\n    done_lines = []\n    for r in state.get(\"step_results\") or []:\n        done_lines.append(f\"Step {r['step_id']} {r['tool']} -> OK\")\n    msgs = REPLAN_PROMPT.format_messages(\n        tool_summaries=tool_summaries(),\n        user_input=state[\"user_input\"],\n        done=\"\\n\".join(done_lines) or \"None\",\n        error=state.get(\"error\") or \"Unknown error\",\n    )\n    revised: PlanModel = replanner_llm.invoke(msgs)\n    steps = []\n    for s in revised.steps:\n        if s.action == \"tool\" and (not s.name or s.name not in TOOL_REGISTRY):\n            raise ValueError(f\"Replanner proposed unknown tool: {s.name}\")\n        steps.append(s)\n    trace = (state.get(\"trace\") or []) + [{\"event\": \"replan\", \"plan\": [s.model_dump() for s in steps]}]\n    return {\"plan\": steps, \"error\": None, \"trace\": trace}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wire the graph with conditional edges\n\nLangGraph's `StateGraph` connects the nodes. After execution, we route to replan on error or end on success."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n\ngraph = StateGraph(AgentState)\ngraph.add_node(\"plan\", plan_node)\ngraph.add_node(\"execute\", execute_node)\ngraph.add_node(\"replan\", replan_node)\n\ngraph.add_edge(\"plan\", \"execute\")\n\ndef route_after_execute(state: AgentState) -> str:\n    \"\"\"Determines the next node after execution.\"\"\"\n    return \"replan\" if state.get(\"error\") else END\n\ngraph.add_conditional_edges(\"execute\", route_after_execute, {\"replan\": \"replan\", END: END})\ngraph.add_edge(\"replan\", \"execute\")\n\ngraph.set_entry_point(\"plan\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Persist memory with checkpointers\n\nLangGraph checkpointers persist state across turns per thread. This gives you conversation memory or multi\\-call workflows. For quick starts, use the in\\-memory saver. For production, use Postgres. If you're interested in why LLM memory isn't infinite and how to manage accumulated context, our guide on [context rot and LLM memory limitations](/article/context-rot-why-llms-forget-as-their-memory-grows-3) offers practical strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n\nmemory = MemorySaver()\napp_graph = graph.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Visualize your graph**\n\nLangGraph provides built\\-in utilities for visualizing and inspecting your graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n\ndisplay(Image(app_graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Serve a production\\-friendly JSON API\n\nFastAPI provides a lightweight, typed API for the agent. The `/agent` endpoint accepts a `thread_id` and `query`, invokes the graph, and returns the full state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom typing import Optional\n\napi = FastAPI(title=\"Plan-Execute Agent\")\n\nclass AgentRequest(BaseModel):\n    thread_id: str\n    query: str\n\nclass AgentResponse(BaseModel):\n    thread_id: str\n    plan: list[dict]\n    step_results: list[dict]\n    final_answer: Optional[str]\n    error: Optional[str]\n    trace: list[dict]\n\n@api.post(\"/agent\", response_model=AgentResponse)\ndef agent_endpoint(req: AgentRequest):\n    \"\"\"FastAPI endpoint for agent queries.\"\"\"\n    state = app_graph.invoke({\"user_input\": req.query}, config={\"configurable\": {\"thread_id\": req.thread_id}})\n    plan = [s.model_dump() if hasattr(s, \"model_dump\") else s for s in state.get(\"plan\", [])]\n    return AgentResponse(\n        thread_id=req.thread_id,\n        plan=plan,\n        step_results=state.get(\"step_results\", []),\n        final_answer=state.get(\"final_answer\"),\n        error=state.get(\"error\"),\n        trace=state.get(\"trace\", []),\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Validate\n\n### Test the graph directly\n\nInvoke the graph with a sample query to verify planning and execution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = app_graph.invoke(\n    {\"user_input\": \"What is 10 + 20 + 30?\"},\n    config={\"configurable\": {\"thread_id\": \"test-thread-1\"}}\n)\nprint(\"Plan:\", result.get(\"plan\"))\nprint(\"Final Answer:\", result.get(\"final_answer\"))\nprint(\"Trace:\", result.get(\"trace\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected output: The planner generates a plan with a `sum_numbers` tool step and a `respond` step. The executor runs the tool and synthesizes the answer.\n\n### Test error handling and replanning\n\nTrigger an error by requesting a non\\-existent KB topic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = app_graph.invoke(\n    {\"user_input\": \"What is the warranty policy?\"},\n    config={\"configurable\": {\"thread_id\": \"test-thread-2\"}}\n)\nprint(\"Error:\", result.get(\"error\"))\nprint(\"Trace:\", result.get(\"trace\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected output: The executor fails on the `kb_retrieve` step, routes to replan, and generates a revised plan (or returns a partial answer if replanning also fails).\n\n### Run the FastAPI server\n\nStart the server in a notebook or local environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uvicorn\nimport nest_asyncio\n\nnest_asyncio.apply()  # Required for running uvicorn in Jupyter/Colab\nuvicorn.run(api, host=\"0.0.0.0\", port=8000, log_level=\"info\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a separate terminal or notebook cell, test the endpoint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n\nresponse = requests.post(\"http://localhost:8000/agent\", json={\n    \"thread_id\": \"user-123\",\n    \"query\": \"What is 5 + 10?\"\n})\nprint(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected output: A JSON response with `plan`, `step_results`, `final_answer`, and `trace`.\n\n## Conclusion\n\nYou've built a deterministic, plan\\-execute agent with LangGraph, Pydantic\\-validated tools, and a FastAPI endpoint. The system plans before acting, validates every tool call, and recovers from errors via replanning. Memory persists across turns using checkpointers, enabling multi\\-turn workflows.\n\n**Key decisions:**\n\n* **LangGraph** for deterministic routing and state management\n\n* **Pydantic** for strict input/output validation\n\n* **FastAPI** for a lightweight, typed API\n\n* **Temperature\\=0** for reproducible planning\n\n**Next steps:**\n\n* Swap `MemorySaver` for `PostgresSaver` for production persistence\n\n* Add retries with exponential backoff for transient tool failures\n\n* Extend the tool registry with new tools (e.g., database queries, external APIs)\n\n* Add observability with structured logging or tracing (e.g., LangSmith)\n\n* Harden prompts with explicit constraints and few\\-shot examples\n\nThis architecture scales from prototypes to production. Start with the core build, validate it end\\-to\\-end, then layer in production features as needed."
      ]
    }
  ],
  "metadata": {
    "title": "LangGraph Agent: How to Build a Deterministic Plan-Execute with Memory",
    "description": "Build a production-ready LangGraph agent that plans, executes, validates tools, persists state, remembers context, and serves a deterministic JSON /agent.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}