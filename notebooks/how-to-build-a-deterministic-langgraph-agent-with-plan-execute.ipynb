{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build a Deterministic LangGraph Agent with Plan-Execute\n\n**Description:** Ship a production-grade LangGraph agent: deterministic plan-execute, strict JSON schemas, thread memory, SQLite checkpoints, and a single FastAPI /agent endpoint.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Approach Works\n\nBuilding reliable AI agents for production requires more than chaining LLM callsâ€”you need **deterministic planning**, **strict validation**, and **resumable state**. This guide shows you how to combine LangGraph's state management with structured schemas and guardrails to create an agent that:\n\n- **Plans before acting** using a fixed schema, so you can audit and control every step\n- **Validates tool inputs** with Pydantic, catching errors before execution\n- **Persists checkpoints** in SQLite, enabling pause/resume and debugging\n- **Exposes a clean API** via FastAPI for integration into real systems\n\nYou'll build a single `/agent` endpoint that accepts a user query, plans a sequence of tool calls, executes them with retries and guardrails, and returns a final answerâ€”all with predictable costs and debuggable runs.\n\n## How It Works (High-Level Overview)\n\nThe agent follows a three-node graph:\n\n1. **Planner** â€“ The LLM receives the user query and outputs a structured `Plan` (list of steps with tool names and arguments). Temperature is set to 0 for consistency.\n2. **Executor** â€“ Each step is validated against the tool's Pydantic schema, then executed with retries and timeout. Results (or errors) are appended to state.\n3. **Finalizer** â€“ The LLM synthesizes all evidence into a final answer, citing step results.\n\nState flows through a SQLite checkpointer, so you can interrupt, inspect, and resume at any step. Conditional edges route errors to END immediately, short-circuiting the graph when guardrails fail.\n\n## Setup & Installation\n\nInstall dependencies (pinned for reproducibility):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install langgraph~=0.2.0 langgraph-checkpoint-sqlite~=0.1.0 langchain-core~=0.3.0 langchain-openai~=0.2.0 openai~=1.0 pydantic~=2.0 fastapi~=0.115.0 uvicorn~=0.30.0 python-dotenv~=1.0 httpx~=0.27.0 nest-asyncio~=1.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your OpenAI API key (for Colab, set programmatically; for local, use `.env`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Replace with your key\n# Optional: os.environ[\"LANGCHAIN_API_KEY\"] = \"...\" for LangSmith tracing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print versions to confirm setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import langgraph, langchain_core, langchain_openai, pydantic, fastapi\nprint(f\"LangGraph: {langgraph.__version__}\")\nprint(f\"LangChain Core: {langchain_core.__version__}\")\nprint(f\"LangChain OpenAI: {langchain_openai.__version__}\")\nprint(f\"Pydantic: {pydantic.__version__}\")\nprint(f\"FastAPI: {fastapi.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step-by-Step Implementation\n\n### 1. Define Pydantic Schemas\n\nStrict schemas enforce structure at both the planning and tool layers. The planner outputs a `Plan` with a list of `Step` objects; each tool defines its own input schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\nfrom typing import Literal\n\nclass Step(BaseModel):\n    \"\"\"A single planned action.\"\"\"\n    step_id: int = Field(..., description=\"Unique step number\")\n    tool: Literal[\"calculator\", \"search_docs\"] = Field(..., description=\"Tool name\")\n    args: dict = Field(..., description=\"Tool arguments as key-value pairs\")\n    reason: str = Field(..., description=\"Why this step is needed\")\n\nclass Plan(BaseModel):\n    \"\"\"Structured plan output by the planner LLM.\"\"\"\n    steps: list[Step] = Field(..., description=\"Ordered list of steps\")\n\nclass CalculatorInput(BaseModel):\n    \"\"\"Schema for calculator tool.\"\"\"\n    expression: str = Field(..., description=\"Math expression using +, -, *, /, (, )\")\n\nclass SearchDocsInput(BaseModel):\n    \"\"\"Schema for document search tool.\"\"\"\n    query: str = Field(..., description=\"Search query string\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why strict schemas?** They prevent the LLM from hallucinating invalid tool names or malformed arguments, and they make every plan auditable before execution.\n\n### 2. Implement Tools with Validation\n\nEach tool validates its input schema and includes guardrails (e.g., disallowing unsafe operations). We use `ast.parse` to whitelist safe math nodes and reject anything else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\nimport operator\n\n# Calculator tool with AST-based guardrails\ndef calculator(expression: str) -> str:\n    \"\"\"\n    Evaluate a math expression safely.\n    Only allows +, -, *, /, parentheses, and numbers.\n    \"\"\"\n    allowed_nodes = (ast.Expression, ast.BinOp, ast.UnaryOp, ast.Num, ast.Constant)\n    allowed_ops = (ast.Add, ast.Sub, ast.Mult, ast.Div, ast.USub, ast.UAdd)\n    \n    try:\n        tree = ast.parse(expression, mode='eval')\n        for node in ast.walk(tree):\n            if not isinstance(node, allowed_nodes):\n                return f\"Error: Disallowed operation in expression: {expression}\"\n            if isinstance(node, (ast.BinOp, ast.UnaryOp)) and not isinstance(node.op, allowed_ops):\n                return f\"Error: Disallowed operator in expression: {expression}\"\n        \n        result = eval(compile(tree, filename=\"\", mode=\"eval\"))\n        return str(result)\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Mock document search tool\ndef search_docs(query: str) -> str:\n    \"\"\"\n    Mock document search. Replace with real retrieval logic.\n    \"\"\"\n    mock_db = {\n        \"policy\": \"Our return policy allows 30-day returns.\",\n        \"hours\": \"We are open Mon-Fri 9am-5pm.\"\n    }\n    for key, doc in mock_db.items():\n        if key in query.lower():\n            return doc\n    return \"No relevant documents found.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why AST parsing?** Simple `eval` is unsafe. AST whitelisting ensures only arithmetic operations run, blocking code injection or disallowed functions.\n\nTest the calculator guardrail:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(calculator(\"2 + 2 * 3\"))       # Should return \"8\"\nprint(calculator(\"2 + sqrt(4)\"))     # Should return error (sqrt not allowed)\nprint(calculator(\"2 + 2 ^ 3\"))       # Should return error (^ not allowed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Register Tools in a Guarded Registry\n\nThe registry maps tool names to (function, schema) pairs and wraps execution with retries, timeout, and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tenacity import retry, stop_after_attempt, wait_exponential\nimport asyncio\n\nTOOL_REGISTRY = {\n    \"calculator\": (calculator, CalculatorInput),\n    \"search_docs\": (search_docs, SearchDocsInput),\n}\n\n@retry(stop=stop_after_attempt(2), wait=wait_exponential(min=1, max=4))\nasync def call_tool_safe(tool_name: str, args: dict) -> str:\n    \"\"\"\n    Validate args against schema, then call tool with timeout and retry.\n    \"\"\"\n    if tool_name not in TOOL_REGISTRY:\n        return f\"Error: Unknown tool '{tool_name}'\"\n    \n    func, schema = TOOL_REGISTRY[tool_name]\n    \n    # Validate args\n    try:\n        validated = schema(**args)\n    except Exception as e:\n        return f\"Validation error: {str(e)}\"\n    \n    # Execute with timeout\n    try:\n        result = await asyncio.wait_for(\n            asyncio.to_thread(func, **validated.dict()),\n            timeout=5.0\n        )\n        return result\n    except asyncio.TimeoutError:\n        return \"Error: Tool execution timeout\"\n    except Exception as e:\n        return f\"Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why retries and timeout?** External tools (APIs, databases) can fail transiently. Retries with exponential backoff improve reliability; timeout prevents hanging.\n\nTest the guarded caller:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\nresult = asyncio.run(call_tool_safe(\"calculator\", {\"expression\": \"10 / 2\"}))\nprint(result)  # Should print \"5.0\"\n\nresult = asyncio.run(call_tool_safe(\"calculator\", {\"expression\": \"10 / 0\"}))\nprint(result)  # Should print error message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Configure LLM Clients\n\nWe use two clients: one for structured planning (with `with_structured_output`), one for natural language finalization. Temperature is 0 for determinism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n\n# Planner: outputs structured Plan\nplanner_llm = ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    temperature=0  # Deterministic planning\n).with_structured_output(Plan)\n\n# Finalizer: outputs natural language\nfinalizer_llm = ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    temperature=0\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why temperature=0?** It minimizes variance in plan structure and final answers, making costs and behavior predictable across runs.\n\nTest the planner with a simple prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_plan = planner_llm.invoke(\"Calculate 5 + 3 and search for return policy\")\nprint(test_plan)  # Should print a Plan object with two steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Define State Schema\n\nState carries the query, plan, execution results, memory summary, and step index through the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\nfrom langgraph.graph import add_messages\n\nclass AgentState(TypedDict):\n    query: str\n    plan: Plan | None\n    results: Annotated[list[dict], add_messages]  # Append-only list of step results\n    memory_summary: str  # Thread-scoped context from prior runs\n    step_index: int\n    final_answer: str | None\n    error: str | None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why `add_messages` for results?** It ensures results accumulate across graph invocations without overwriting, critical for resumability.\n\n### 6. Implement Graph Nodes\n\nEach node is an async function that reads and writes state.\n\n#### Planner Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def planner_node(state: AgentState) -> dict:\n    \"\"\"\n    Generate a structured plan from the user query and memory.\n    \"\"\"\n    prompt = f\"User query: {state['query']}\\n\"\n    if state.get(\"memory_summary\"):\n        prompt += f\"Context from prior conversation: {state['memory_summary']}\\n\"\n    prompt += \"Create a step-by-step plan using available tools: calculator, search_docs.\"\n    \n    plan = await planner_llm.ainvoke(prompt)\n    return {\"plan\": plan, \"step_index\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why inject memory_summary?** It allows the planner to adapt based on prior interactions in the same thread, enabling multi-turn workflows.\n\n#### Executor Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def executor_node(state: AgentState) -> dict:\n    \"\"\"\n    Execute the current step, validate, and append result to state.\n    \"\"\"\n    plan = state[\"plan\"]\n    idx = state[\"step_index\"]\n    \n    if idx >= len(plan.steps):\n        return {\"step_index\": idx}  # All steps done\n    \n    step = plan.steps[idx]\n    result_text = await call_tool_safe(step.tool, step.args)\n    \n    result_entry = {\n        \"step_id\": step.step_id,\n        \"tool\": step.tool,\n        \"args\": step.args,\n        \"result\": result_text\n    }\n    \n    # Check for errors\n    if result_text.startswith(\"Error:\"):\n        return {\n            \"results\": [result_entry],\n            \"error\": result_text,\n            \"step_index\": idx + 1\n        }\n    \n    return {\n        \"results\": [result_entry],\n        \"step_index\": idx + 1\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why check for \"Error:\" prefix?** It's a simple convention to route failures to END via conditional edges, short-circuiting the graph.\n\n#### Finalizer Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def finalizer_node(state: AgentState) -> dict:\n    \"\"\"\n    Synthesize all step results into a final answer.\n    \"\"\"\n    evidence = \"\\n\".join([\n        f\"Step {r['step_id']}: {r['tool']}({r['args']}) -> {r['result']}\"\n        for r in state[\"results\"]\n    ])\n    \n    prompt = f\"User query: {state['query']}\\n\\nEvidence:\\n{evidence}\\n\\nProvide a concise final answer.\"\n    \n    response = await finalizer_llm.ainvoke(prompt)\n    return {\"final_answer\": response.content}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why cite evidence?** It makes the final answer auditable and helps debug incorrect results by tracing back to specific tool outputs.\n\n### 7. Build the Graph with Conditional Edges\n\nWire the nodes with conditional routing: continue executing steps until done or error, then finalize or end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n\ndef should_continue(state: AgentState) -> str:\n    \"\"\"Route to executor, finalizer, or end based on state.\"\"\"\n    if state.get(\"error\"):\n        return END\n    if state[\"step_index\"] >= len(state[\"plan\"].steps):\n        return \"finalizer\"\n    return \"executor\"\n\n# Build graph\ngraph_builder = StateGraph(AgentState)\ngraph_builder.add_node(\"planner\", planner_node)\ngraph_builder.add_node(\"executor\", executor_node)\ngraph_builder.add_node(\"finalizer\", finalizer_node)\n\ngraph_builder.set_entry_point(\"planner\")\ngraph_builder.add_edge(\"planner\", \"executor\")\ngraph_builder.add_conditional_edges(\"executor\", should_continue, {\n    \"executor\": \"executor\",\n    \"finalizer\": \"finalizer\",\n    END: END\n})\ngraph_builder.add_edge(\"finalizer\", END)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why conditional edges?** They enable dynamic routing (loop over steps, short-circuit on error) without hardcoding the number of steps.\n\n### 8. Add SQLite Checkpointer and Compile\n\nThe checkpointer persists state after each node, enabling pause/resume and debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.sqlite import SqliteSaver\n\ncheckpointer = SqliteSaver.from_conn_string(\"checkpoints.db\")\ngraph = graph_builder.compile(checkpointer=checkpointer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why SQLite?** It's simple for development and supports thread-scoped state. For production, consider Postgres or Redis for concurrency and scale.\n\nTest the graph without the API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"test-thread-1\"}}\nresult = await graph.ainvoke(\n    {\"query\": \"Calculate 10 + 5 and search for return policy\", \"results\": [], \"step_index\": 0, \"memory_summary\": \"\"},\n    config\n)\nprint(result[\"final_answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Expose via FastAPI\n\nWrap the graph in a POST endpoint that accepts a query and thread_id, invokes the graph, and returns the final answer or error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel as PydanticBase\n\napp = FastAPI()\n\nclass AgentRequest(PydanticBase):\n    query: str\n    thread_id: str = \"default\"\n    memory_summary: str = \"\"\n\nclass AgentResponse(PydanticBase):\n    final_answer: str | None = None\n    error: str | None = None\n    thread_id: str\n\n@app.post(\"/agent\", response_model=AgentResponse)\nasync def agent_endpoint(req: AgentRequest):\n    \"\"\"\n    Run the agent graph for a given query and thread.\n    \"\"\"\n    config = {\"configurable\": {\"thread_id\": req.thread_id}}\n    initial_state = {\n        \"query\": req.query,\n        \"results\": [],\n        \"step_index\": 0,\n        \"memory_summary\": req.memory_summary,\n        \"plan\": None,\n        \"final_answer\": None,\n        \"error\": None\n    }\n    \n    try:\n        final_state = await graph.ainvoke(initial_state, config)\n        return AgentResponse(\n            final_answer=final_state.get(\"final_answer\"),\n            error=final_state.get(\"error\"),\n            thread_id=req.thread_id\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why thread_id?** It scopes checkpoints and memory to a conversation, enabling multi-turn interactions and resumability.\n\n**Concurrency note:** If multiple requests for the same `thread_id` arrive concurrently, checkpoint writes may race. For production, add a simple lock per thread or use a queue.\n\nRun the server (for local execution):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uvicorn\n# For notebooks, use nest_asyncio to allow uvicorn in the same event loop\nimport nest_asyncio\nnest_asyncio.apply()\n\n# Run in background or separate cell\nuvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For Colab, run the server in a background thread and test with `httpx` in the same notebook.\n\n## Run and Validate\n\n### Happy Path\n\nTest a successful query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import httpx\nimport asyncio\n\nasync def test_happy_path():\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            \"http://localhost:8000/agent\",\n            json={\"query\": \"Calculate 15 * 3 and search for business hours\", \"thread_id\": \"thread-1\"}\n        )\n        print(response.json())\n\nasyncio.run(test_happy_path())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```json\n{\n  \"final_answer\": \"15 * 3 equals 45. Our business hours are Mon-Fri 9am-5pm.\",\n  \"error\": null,\n  \"thread_id\": \"thread-1\"\n}\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guardrail Failure\n\nTest an invalid expression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def test_guardrail():\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            \"http://localhost:8000/agent\",\n            json={\"query\": \"Calculate 2 + sqrt(4)\", \"thread_id\": \"thread-2\"}\n        )\n        print(response.json())\n\nasyncio.run(test_guardrail())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```json\n{\n  \"final_answer\": null,\n  \"error\": \"Error: Disallowed operation in expression: 2 + sqrt(4)\",\n  \"thread_id\": \"thread-2\"\n}\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resumability\n\nTo demonstrate resumability, you would interrupt the graph mid-execution (e.g., by adding a human-in-the-loop node) and re-invoke with the same `thread_id`. The checkpointer restores state and continues from the last completed node.\n\nFor a simple test, run two sequential requests with the same `thread_id` and pass a `memory_summary` in the second request to verify the planner adapts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def test_memory():\n    async with httpx.AsyncClient() as client:\n        # First request\n        r1 = await client.post(\n            \"http://localhost:8000/agent\",\n            json={\"query\": \"Calculate 10 + 5\", \"thread_id\": \"thread-3\"}\n        )\n        print(\"First response:\", r1.json())\n        \n        # Second request with memory\n        r2 = await client.post(\n            \"http://localhost:8000/agent\",\n            json={\n                \"query\": \"Now multiply that result by 2\",\n                \"thread_id\": \"thread-3\",\n                \"memory_summary\": \"User previously calculated 10 + 5 = 15\"\n            }\n        )\n        print(\"Second response:\", r2.json())\n\nasyncio.run(test_memory())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The second response should reference the prior result (15) and calculate 15 * 2 = 30.\n\n## Conclusion\n\nYou've built a deterministic, resumable AI agent with:\n\n- **Structured planning** via Pydantic schemas and `with_structured_output`\n- **Guardrails** using AST parsing and strict validation\n- **Retries and timeout** for reliable tool execution\n- **SQLite checkpoints** for pause/resume and debugging\n- **FastAPI endpoint** for integration into real systems\n\n**Key design decisions:**\n\n- Temperature=0 ensures consistent plans and answers across runs\n- Strict schemas prevent hallucinated tool calls and malformed arguments\n- Conditional edges enable dynamic routing (loop, short-circuit on error)\n- Thread-scoped state supports multi-turn conversations and resumability\n\n**Next steps for production:**\n\n- Add authentication (FastAPI dependency for API key validation)\n- Implement rate limiting (e.g., `slowapi` or Redis-based limiter)\n- Add observability (Prometheus metrics, LangSmith tracing, structured logging)\n- Replace SQLite with Postgres or Redis for concurrent access\n- Deploy with Docker + Kubernetes or serverless (AWS Lambda, GCP Cloud Run)\n- Add a simple lock or queue per `thread_id` to prevent checkpoint races\n\nYou now have a ship-ready foundation for building reliable, auditable AI agents that scale."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build a Deterministic LangGraph Agent with Plan-Execute",
    "description": "Ship a production-grade LangGraph agent: deterministic plan-execute, strict JSON schemas, thread memory, SQLite checkpoints, and a single FastAPI /agent endpoint.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}