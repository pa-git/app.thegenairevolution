{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build a Deterministic LangGraph Agent with Plan-Execute\n\n**Description:** Ship a production-grade LangGraph agent: deterministic plan-execute, strict JSON schemas, thread memory, SQLite checkpoints, and a single FastAPI /agent endpoint.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nBuilding reliable GenAI agents means controlling their behavior. Free-form loops can spiral into unpredictable costs, latencies, and debugging nightmares. This guide shows you how to build a **deterministic plan-execute agent** with LangGraph that fixes the plan upfront, validates every step with strict schemas, and checkpoints progress so you can resume mid-execution. You'll walk away with a working FastAPI service that's debuggable, cost-predictable, and production-ready.\n\n**Prerequisites:** Python 3.10+, an OpenAI API key with billing enabled, and basic familiarity with FastAPI. This tutorial is designed to run end-to-end in a local environment or Colab notebook.\n\n**What you'll build:** A fixed plan â†’ execute â†’ finalize graph with Pydantic schemas, SQLite checkpointing, and a `/agent` endpoint that handles multi-turn conversations with thread-scoped memory.\n\n---\n\n## Why This Approach Works\n\n**Determinism over improvisation.** Traditional ReAct loops let the LLM decide tools and parameters on every turn, leading to drift, redundant calls, and unpredictable token usage. A plan-execute pattern locks in the strategy upfront: the planner emits a fixed sequence of steps, the executor runs them one by one with validated inputs, and the finalizer synthesizes the answer. You get predictable costs, clear traces, and no surprise loops.\n\n**Strict schemas enforce correctness.** Pydantic models validate every tool input and output at runtime. If the planner hallucinates a parameter or the tool returns malformed data, the system fails fast with a typed errorâ€”no silent corruption, no downstream crashes.\n\n**Checkpointing enables resumability.** LangGraph's SQLite checkpointer persists state after every node. If a tool times out or the server restarts mid-execution, you resume from the last completed step using the same `thread_id`. This makes the agent resilient to transient failures and safe for long-running workflows.\n\n**Thread-scoped memory keeps context clean.** Each conversation thread maintains its own state and memory summary. You can run multiple concurrent requests without cross-contamination, and the agent can reference prior turns when planning follow-up steps.\n\n---\n\n## How It Works (High-Level Overview)\n\n1. **User sends a request** with a `thread_id` and `user_input` to the `/agent` endpoint.\n2. **Planner node** calls OpenAI with function calling to emit a structured `Plan` (title, rationale, list of steps). Each step specifies a tool name, parameters, and description. The plan is capped at 5 steps and uses only allowlisted tools.\n3. **Executor node** runs one step per graph visit. It validates the step's parameters against the tool's input schema, calls the tool with retries and timeout enforcement, validates the output, and appends the result to `steps_results`. The graph loops back to executor until all steps are complete.\n4. **Finalizer node** sends the accumulated evidence to OpenAI with `response_format=json_object` to generate a concise answer. It packages the answer, evidence, plan summary, and metadata into a `FinalResult`.\n5. **Checkpointer** persists state after every node transition. If execution is interrupted, the next request with the same `thread_id` resumes from the last checkpoint.\n6. **Memory update** runs after the graph completes, summarizing the final result and storing it in thread-scoped memory for future turns.\n\n**Graph flow:**  \n`user_input â†’ plan â†’ execute (loop) â†’ finalize â†’ END`\n\n---\n\n## Setup & Installation\n\nInstall dependencies in a single cell (Colab-ready):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install langgraph langchain-core openai pydantic fastapi uvicorn python-dotenv nest-asyncio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your OpenAI API key securely:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise ValueError(\"OPENAI_API_KEY not set. Add it to .env or set it in the environment.\")\n\nOPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")  # Ensure this model supports tool calling\nTEMPERATURE = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step-by-Step Implementation\n\n### Step 1: Define Schemas and State\n\nUse Pydantic to define tool inputs, outputs, plan steps, and agent state. These schemas enforce type safety and enable automatic validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\nimport sys\nfrom typing import List, Dict, Any, TypedDict\nfrom pydantic import BaseModel, Field\n\n# Set up structured logging\nlogger = logging.getLogger(\"agent\")\nlogger.setLevel(logging.INFO)\nhandler = logging.StreamHandler(sys.stdout)\nformatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n\n# Plan step schema: each step specifies a tool, parameters, and rationale\nclass PlanStep(BaseModel):\n    tool: str = Field(..., description=\"Tool name from the allowlist\")\n    params: Dict[str, Any] = Field(default_factory=dict, description=\"JSON inputs for the tool\")\n    description: str = Field(..., description=\"Why this step is needed\")\n\n# Plan schema: title, rationale, and a list of steps (max 5)\nclass Plan(BaseModel):\n    title: str\n    rationale: str\n    steps: List[PlanStep] = Field(..., min_items=1, max_items=5)\n\n# Tool input/output schemas\nclass SearchInput(BaseModel):\n    query: str\n\nclass SearchOutput(BaseModel):\n    results: List[str]\n\nclass RetrieveInput(BaseModel):\n    doc_id: str\n\nclass RetrieveOutput(BaseModel):\n    content: str\n\nclass CalcInput(BaseModel):\n    expression: str\n\nclass CalcOutput(BaseModel):\n    value: float\n\n# Final result schema: answer, evidence, plan summary, and metadata\nclass FinalResult(BaseModel):\n    answer: str\n    evidence: Dict[str, Any]\n    plan_summary: Dict[str, Any]\n    metadata: Dict[str, Any]\n\n# Agent state: stores thread_id, user input, plan, execution progress, results, and memory\nclass AgentState(TypedDict, total=False):\n    thread_id: str\n    user_input: str\n    plan_dict: Dict[str, Any]  # Store plan as dict for checkpoint portability\n    step_index: int\n    steps_results: List[Dict[str, Any]]\n    final_result_dict: Dict[str, Any]  # Store final result as dict\n    memory: Dict[str, Any]\n    error: Dict[str, Any]  # Structured error payload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why dicts in state?** SQLite checkpointer serializes state to JSON. Storing Pydantic models directly can break portability. We store `plan_dict` and `final_result_dict` as plain dicts and convert to/from Pydantic at node boundaries.\n\n### Step 2: Implement Tools with Validation\n\nDefine three simple tools: search, retrieve, and calculator. Each tool takes a validated input model and returns a validated output model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\nimport operator\nfrom typing import Callable, Tuple\n\n# In-memory corpus for demonstration\nCORPUS = {\n    \"doc:langgraph\": \"LangGraph is a library for building stateful, structured LLM workflows.\",\n    \"doc:pydantic\": \"Pydantic validates data using Python type hints.\",\n    \"doc:fastapi\": \"FastAPI is a modern, fast web framework for building APIs with Python.\",\n}\n\n# Safe arithmetic operations for calculator\n_ALLOWED_OPS = {\n    ast.Add: operator.add, ast.Sub: operator.sub, ast.Mult: operator.mul,\n    ast.Div: operator.truediv, ast.Pow: operator.pow, ast.USub: operator.neg\n}\n\ndef _eval_expr(node):\n    \"\"\"Evaluate a safe arithmetic expression (no variables, no function calls).\"\"\"\n    if isinstance(node, ast.Num):\n        return node.n\n    if isinstance(node, ast.UnaryOp) and type(node.op) in _ALLOWED_OPS:\n        return _ALLOWED_OPS[type(node.op)](_eval_expr(node.operand))\n    if isinstance(node, ast.BinOp) and type(node.op) in _ALLOWED_OPS:\n        return _ALLOWED_OPS[type(node.op)](_eval_expr(node.left), _eval_expr(node.right))\n    if isinstance(node, ast.Expression):\n        return _eval_expr(node.body)\n    raise ValueError(\"Disallowed expression\")\n\ndef search_tool(inp: SearchInput) -> SearchOutput:\n    \"\"\"Search tool: finds documents matching the query.\"\"\"\n    q = inp.query.lower().strip()\n    hits = [k for k, v in CORPUS.items() if q in v.lower() or q in k.lower()]\n    return SearchOutput(results=hits[:5])\n\ndef calc_tool(inp: CalcInput) -> CalcOutput:\n    \"\"\"Calculator tool: evaluates arithmetic expressions safely.\"\"\"\n    expr = inp.expression\n    node = ast.parse(expr, mode=\"eval\")\n    val = float(_eval_expr(node))\n    return CalcOutput(value=val)\n\ndef retrieve_tool(inp: RetrieveInput) -> RetrieveOutput:\n    \"\"\"Retrieve tool: fetches document content by ID.\"\"\"\n    content = CORPUS.get(inp.doc_id)\n    if content is None:\n        raise ValueError(f\"Document not found: {inp.doc_id}\")\n    return RetrieveOutput(content=content)\n\n# Tool allowlist: maps tool names to (InputModel, OutputModel, callable)\nToolCallable = Callable[[BaseModel], BaseModel]\nALLOWLIST: Dict[str, Tuple[type[BaseModel], type[BaseModel], ToolCallable]] = {\n    \"search\": (SearchInput, SearchOutput, search_tool),\n    \"retrieve_doc\": (RetrieveInput, RetrieveOutput, retrieve_tool),\n    \"calculator\": (CalcInput, CalcOutput, calc_tool),\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why an allowlist?** It prevents the planner from hallucinating tool names or invoking unsafe operations. Only pre-approved tools can execute.\n\n### Step 3: Build the Planner Node\n\nThe planner calls OpenAI with function calling to emit a structured plan. We use `tool_choice` to force the model to call `emit_plan` and return a JSON schema matching `Plan`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\nfrom pydantic import ValidationError\n\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\n# Planner system prompt: instructs the model to produce a deterministic, finite plan\nPLANNER_SYSTEM = \"\"\"You are a deterministic planning assistant.\n- Produce a concise plan to answer the user.\n- Use only these tools: search, retrieve_doc, calculator.\n- Steps must be finite and <= 5.\n- No loops, no self-modifying plans.\n- Prefer minimal steps that still deliver a correct answer.\"\"\"\n\ndef make_plan(user_input: str) -> Plan:\n    \"\"\"Create a plan using OpenAI's function calling.\"\"\"\n    tools = [{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"emit_plan\",\n            \"description\": \"Emit a structured execution plan\",\n            \"parameters\": Plan.model_json_schema()\n        }\n    }]\n    msg = client.chat.completions.create(\n        model=OPENAI_MODEL,\n        temperature=TEMPERATURE,\n        messages=[\n            {\"role\": \"system\", \"content\": PLANNER_SYSTEM},\n            {\"role\": \"user\", \"content\": user_input},\n        ],\n        tools=tools,\n        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"emit_plan\"}},\n    )\n    tool_calls = msg.choices[0].message.tool_calls or []\n    if not tool_calls:\n        raise ValueError(\"Planner did not return a plan\")\n    args = tool_calls[0].function.arguments\n    try:\n        return Plan.model_validate_json(args)\n    except ValidationError as e:\n        raise ValueError(f\"Invalid plan schema: {e}\")\n\ndef planner_node(state: AgentState) -> AgentState:\n    \"\"\"Planner node: creates a plan if not already present, stores it as dict.\"\"\"\n    if \"plan_dict\" in state:\n        return state\n    try:\n        plan = make_plan(state[\"user_input\"])\n        logger.info(f\"Plan created: {plan.title}\")\n        return {\n            **state,\n            \"plan_dict\": plan.model_dump(),\n            \"step_index\": 0,\n            \"steps_results\": [],\n            \"memory\": state.get(\"memory\", {}),\n        }\n    except Exception as e:\n        logger.error(f\"Planner error: {e}\")\n        return {**state, \"error\": {\"type\": \"planner_error\", \"message\": str(e)}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why temperature=0?** Deterministic output. The same input should produce the same plan every time, making the system predictable and testable.\n\n### Step 4: Build the Executor Node\n\nThe executor runs one step per visit. It validates the step's parameters, calls the tool with retries and timeout enforcement, validates the output, and appends the result to `steps_results`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\nfrom concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError\n\nclass ValidationFailure(Exception):\n    pass\n\nclass ToolExecutionError(Exception):\n    pass\n\ndef call_tool_with_timeout(fn: Callable, inp: BaseModel, timeout_s: float) -> BaseModel:\n    \"\"\"Call a tool with a real timeout using ThreadPoolExecutor.\"\"\"\n    with ThreadPoolExecutor(max_workers=1) as executor:\n        future = executor.submit(fn, inp)\n        try:\n            return future.result(timeout=timeout_s)\n        except FuturesTimeoutError:\n            raise ToolExecutionError(f\"Tool timed out after {timeout_s}s\")\n\ndef call_tool(step_dict: Dict[str, Any], timeout_s: float = 5.0, retries: int = 2) -> Dict[str, Any]:\n    \"\"\"Call a tool with validation and retries.\"\"\"\n    name = step_dict[\"tool\"]\n    if name not in ALLOWLIST:\n        raise ValidationFailure(f\"Unknown tool: {name}\")\n    InpModel, OutModel, fn = ALLOWLIST[name]\n    try:\n        inp = InpModel.model_validate(step_dict[\"params\"])\n    except ValidationError as e:\n        raise ValidationFailure(f\"Invalid params for {name}: {e}\")\n\n    last_err = None\n    for attempt in range(retries + 1):\n        try:\n            logger.info(f\"Tool call start: {name} with {inp.model_dump()}\")\n            raw = call_tool_with_timeout(fn, inp, timeout_s)\n            out = OutModel.model_validate(raw.model_dump())\n            logger.info(f\"Tool call end: {name} returned {out.model_dump()}\")\n            return {\"tool\": name, \"input\": inp.model_dump(), \"output\": out.model_dump(), \"attempt\": attempt}\n        except Exception as e:\n            last_err = e\n            logger.warning(f\"Tool call error: {name} attempt {attempt} failed with {e}\")\n            time.sleep(0.2 * (2 ** attempt))\n    raise ToolExecutionError(f\"Tool {name} failed after retries: {last_err}\")\n\ndef executor_node(state: AgentState) -> AgentState:\n    \"\"\"Executor node: runs one step, validates IO, appends result.\"\"\"\n    plan_dict = state.get(\"plan_dict\")\n    if not plan_dict:\n        return state\n    steps = plan_dict[\"steps\"]\n    idx = state[\"step_index\"]\n    if idx >= len(steps):\n        return state\n    step = steps[idx]\n    try:\n        result = call_tool(step)\n        new_results = [*state[\"steps_results\"], result]\n        logger.info(f\"Step {idx} completed: {step['tool']}\")\n        return {**state, \"steps_results\": new_results, \"step_index\": idx + 1}\n    except (ValidationFailure, ToolExecutionError) as e:\n        logger.error(f\"Executor error at step {idx}: {e}\")\n        return {**state, \"error\": {\"type\": \"executor_error\", \"step\": idx, \"message\": str(e)}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why retries and timeouts?** Real-world tools can fail transiently (network glitches, rate limits). Retries with exponential backoff improve reliability. Timeouts prevent runaway calls from blocking the agent indefinitely.\n\n### Step 5: Build the Finalizer Node\n\nThe finalizer sends the accumulated evidence to OpenAI with `response_format=json_object` to generate a concise answer. It packages the answer, evidence, plan summary, and metadata into a `FinalResult`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n\ndef finalize_node(state: AgentState) -> AgentState:\n    \"\"\"Finalizer node: synthesizes the final answer from tool results.\"\"\"\n    plan_dict = state.get(\"plan_dict\")\n    if not plan_dict:\n        return state\n    evidence = {\"steps\": state[\"steps_results\"]}\n    prompt = f\"\"\"Using the tool results, answer the user succinctly.\nUser: {state['user_input']}\nEvidence: {json.dumps(evidence, ensure_ascii=False)}\"\"\"\n    try:\n        msg = client.chat.completions.create(\n            model=OPENAI_MODEL,\n            temperature=TEMPERATURE,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You return JSON only with an 'answer' field.\"},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            response_format={\"type\": \"json_object\"},\n        )\n        raw = msg.choices[0].message.content\n        parsed = json.loads(raw or \"{}\")\n        final = FinalResult(\n            answer=parsed.get(\"answer\", \"\"),\n            evidence=evidence,\n            plan_summary=plan_dict,\n            metadata={\"model\": OPENAI_MODEL, \"steps\": len(plan_dict[\"steps\"])}\n        )\n        logger.info(f\"Finalized answer: {final.answer}\")\n        return {**state, \"final_result_dict\": final.model_dump()}\n    except Exception as e:\n        logger.error(f\"Finalizer error: {e}\")\n        return {**state, \"error\": {\"type\": \"finalizer_error\", \"message\": str(e)}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why JSON mode?** It forces the model to return valid JSON, reducing parsing errors and ensuring structured output.\n\n### Step 6: Add Memory Update Node\n\nMemory runs after finalization to summarize the result and store it in thread-scoped memory. This node updates state so memory is checkpointed and available on resume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def memory_node(state: AgentState) -> AgentState:\n    \"\"\"Memory node: updates memory summary after finalization.\"\"\"\n    mem = state.get(\"memory\", {})\n    final_dict = state.get(\"final_result_dict\")\n    if final_dict:\n        mem[\"summary\"] = f\"Last answer: {final_dict['answer']}\"\n    return {**state, \"memory\": mem}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why a separate node?** Placing memory updates inside the graph ensures they're checkpointed. If you update memory outside the graph (e.g., in the API handler), it won't persist across restarts.\n\n### Step 7: Wire the Graph with Conditional Routing\n\nDefine the state graph, add nodes, and set up conditional edges to route between plan, execute, finalize, memory, and error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n\ngraph = StateGraph(AgentState)\ngraph.add_node(\"plan\", planner_node)\ngraph.add_node(\"execute\", executor_node)\ngraph.add_node(\"finalize\", finalize_node)\ngraph.add_node(\"memory\", memory_node)\n\ngraph.set_entry_point(\"plan\")\n\ndef route(state: AgentState):\n    \"\"\"Determine the next node based on the current state.\"\"\"\n    if \"error\" in state:\n        return \"done\"  # Route to END on error\n    if \"final_result_dict\" in state and \"memory\" in state:\n        return \"done\"  # Memory already updated, finish\n    if \"final_result_dict\" in state:\n        return \"memory\"  # Finalized, update memory\n    if \"plan_dict\" not in state:\n        return \"plan\"  # No plan yet, go to planner\n    plan_dict = state[\"plan_dict\"]\n    if state[\"step_index\"] < len(plan_dict[\"steps\"]):\n        return \"execute\"  # More steps to run\n    return \"finalize\"  # All steps done, finalize\n\ngraph.add_conditional_edges(\"plan\", route, {\"plan\": \"plan\", \"execute\": \"execute\", \"finalize\": \"finalize\", \"done\": END})\ngraph.add_conditional_edges(\"execute\", route, {\"execute\": \"execute\", \"finalize\": \"finalize\", \"done\": END})\ngraph.add_conditional_edges(\"finalize\", route, {\"memory\": \"memory\", \"done\": END})\ngraph.add_edge(\"memory\", END)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why conditional edges?** They let the graph adapt to state changes. If an error occurs, we route to END immediately. If all steps are done, we finalize. If memory is updated, we finish.\n\n### Step 8: Compile the Graph with SQLite Checkpointer\n\nCompile the graph with a SQLite checkpointer to enable resumability. The checkpointer persists state after every node transition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.sqlite import SqliteSaver\n\ncheckpointer = SqliteSaver.from_conn_string(\"checkpoints.db\")\napp_graph = graph.compile(checkpointer=checkpointer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why SQLite?** It's portable, requires no external services, and works great for single-instance local development. For multi-instance production, swap to Postgres or Redis.\n\n### Step 9: Wrap the Graph in a FastAPI Endpoint\n\nExpose the agent via a `/agent` POST endpoint. The endpoint accepts a `thread_id` and `user_input`, invokes the graph with the checkpointer config, and returns the final result or error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\nfrom fastapi import FastAPI, HTTPException\n\napi = FastAPI(title=\"Deterministic Plan-Execute Agent\")\n\nclass AgentRequest(BaseModel):\n    thread_id: str\n    user_input: str\n\nclass AgentResponse(BaseModel):\n    request_id: str\n    final_result: FinalResult | None = None\n    error: Dict[str, Any] | None = None\n\n@api.post(\"/agent\", response_model=AgentResponse)\ndef agent_endpoint(req: AgentRequest):\n    \"\"\"FastAPI endpoint to handle agent requests.\"\"\"\n    request_id = str(uuid.uuid4())\n    try:\n        state_in: AgentState = {\n            \"thread_id\": req.thread_id,\n            \"user_input\": req.user_input,\n        }\n        config = {\"configurable\": {\"thread_id\": req.thread_id}}\n        out: AgentState = app_graph.invoke(state_in, config=config)\n        \n        if \"error\" in out:\n            return AgentResponse(request_id=request_id, error=out[\"error\"])\n        if \"final_result_dict\" not in out:\n            raise HTTPException(status_code=500, detail=\"Final result missing\")\n        \n        final_result = FinalResult.model_validate(out[\"final_result_dict\"])\n        return AgentResponse(request_id=request_id, final_result=final_result)\n    except Exception as e:\n        logger.exception(\"Unhandled error in agent endpoint\")\n        raise HTTPException(status_code=500, detail=str(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why FastAPI?** It's fast, has built-in Pydantic validation, and generates OpenAPI docs automatically. Perfect for exposing agents as APIs.\n\n---\n\n## Run and Validate\n\n### Direct Graph Invocation (Notebook-Friendly)\n\nTest the graph directly without starting a server. This validates the plan â†’ execute â†’ finalize flow with a deterministic input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: Basic flow\nstate_in = {\n    \"thread_id\": \"test-thread-1\",\n    \"user_input\": \"Search for LangGraph and tell me what it is.\"\n}\nconfig = {\"configurable\": {\"thread_id\": \"test-thread-1\"}}\nresult = app_graph.invoke(state_in, config=config)\n\nassert \"final_result_dict\" in result, \"Final result missing\"\nfinal = FinalResult.model_validate(result[\"final_result_dict\"])\nprint(f\"Answer: {final.answer}\")\nprint(f\"Steps executed: {len(final.plan_summary['steps'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation Error Test\n\nTrigger a validation error by requesting an unknown tool. The graph should route to END with a structured error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: Validation error (manually inject a bad plan)\nstate_bad = {\n    \"thread_id\": \"test-thread-2\",\n    \"user_input\": \"Use the 'unknown_tool' to do something.\",\n    \"plan_dict\": {\n        \"title\": \"Bad Plan\",\n        \"rationale\": \"Testing error handling\",\n        \"steps\": [{\"tool\": \"unknown_tool\", \"params\": {}, \"description\": \"This will fail\"}]\n    },\n    \"step_index\": 0,\n    \"steps_results\": []\n}\nconfig_bad = {\"configurable\": {\"thread_id\": \"test-thread-2\"}}\nresult_bad = app_graph.invoke(state_bad, config=config_bad)\n\nassert \"error\" in result_bad, \"Expected error in state\"\nprint(f\"Error: {result_bad['error']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resumability Test\n\nSimulate a mid-run failure by manually advancing the state to step 1, then resuming with the same `thread_id`. The graph should pick up where it left off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3: Resumability (simulate partial execution)\nstate_partial = {\n    \"thread_id\": \"test-thread-3\",\n    \"user_input\": \"Search for Pydantic and FastAPI, then calculate 10 + 20.\",\n    \"plan_dict\": {\n        \"title\": \"Multi-step Plan\",\n        \"rationale\": \"Test resumability\",\n        \"steps\": [\n            {\"tool\": \"search\", \"params\": {\"query\": \"Pydantic\"}, \"description\": \"Find Pydantic\"},\n            {\"tool\": \"search\", \"params\": {\"query\": \"FastAPI\"}, \"description\": \"Find FastAPI\"},\n            {\"tool\": \"calculator\", \"params\": {\"expression\": \"10 + 20\"}, \"description\": \"Calculate sum\"}\n        ]\n    },\n    \"step_index\": 1,  # Already completed step 0\n    \"steps_results\": [{\"tool\": \"search\", \"input\": {\"query\": \"Pydantic\"}, \"output\": {\"results\": [\"doc:pydantic\"]}, \"attempt\": 0}]\n}\nconfig_partial = {\"configurable\": {\"thread_id\": \"test-thread-3\"}}\nresult_partial = app_graph.invoke(state_partial, config=config_partial)\n\nassert result_partial[\"step_index\"] == 3, \"Should complete all steps\"\nprint(f\"Resumed and completed: {len(result_partial['steps_results'])} steps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### API Test with Python Requests\n\nStart the FastAPI server (in a separate terminal or notebook cell with `nest_asyncio`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\nimport uvicorn\n\nnest_asyncio.apply()\n\n# Run server in background (for notebook environments)\nimport threading\ndef run_server():\n    uvicorn.run(api, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n\nserver_thread = threading.Thread(target=run_server, daemon=True)\nserver_thread.start()\n\nimport time\ntime.sleep(2)  # Wait for server to start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then test the endpoint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n\nresponse = requests.post(\n    \"http://localhost:8000/agent\",\n    json={\"thread_id\": \"api-test-1\", \"user_input\": \"Search for LangGraph and summarize it.\"}\n)\ndata = response.json()\nprint(f\"Request ID: {data['request_id']}\")\nprint(f\"Answer: {data['final_result']['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Conclusion\n\nYou've built a deterministic plan-execute agent with LangGraph that:\n\n- **Fixes the plan upfront** using OpenAI function calling and strict schemas, eliminating drift and unpredictable loops.\n- **Validates every step** with Pydantic models, ensuring type safety and failing fast on bad inputs or outputs.\n- **Checkpoints progress** with SQLite, enabling mid-execution resumability and resilience to transient failures.\n- **Exposes a clean API** via FastAPI, making the agent easy to integrate into larger systems.\n- **Maintains thread-scoped memory** so each conversation retains context without cross-contamination.\n\n**Key design decisions:**\n\n- **Temperature=0** for deterministic planning.\n- **Tool allowlist** to prevent hallucinated or unsafe tool calls.\n- **Retries and timeouts** to handle transient failures gracefully.\n- **Dicts in state** for checkpoint portability (SQLite serializes to JSON).\n- **Conditional routing** to handle errors and state transitions cleanly.\n\n**Next steps:**\n\n- **Integrate real tools:** Replace the in-memory corpus with a vector store (Pinecone, Weaviate) or live APIs (Google Search, Slack, Notion).\n- **Add observability:** Instrument with LangSmith tracing or structured JSON logging to track token usage, latencies, and tool call patterns.\n- **Harden for production:** Add rate limiting, circuit breakers, and health checks. Deploy with Docker and scale horizontally with a Postgres checkpointer.\n- **Expand memory:** Store full conversation history and use retrieval-augmented memory for long-running threads.\n\nYou now have a production-ready foundation for building reliable, cost-predictable GenAI agents. Start with this core and iterate toward your specific use case."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build a Deterministic LangGraph Agent with Plan-Execute",
    "description": "Ship a production-grade LangGraph agent: deterministic plan-execute, strict JSON schemas, thread memory, SQLite checkpoints, and a single FastAPI /agent endpoint.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}