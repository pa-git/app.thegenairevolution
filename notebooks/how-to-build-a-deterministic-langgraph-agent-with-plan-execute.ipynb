{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build a Deterministic LangGraph Agent with Plan-Execute\n\n**Description:** Ship a production-grade LangGraph agent: deterministic plan-execute, strict JSON schemas, thread memory, SQLite checkpoints, and a single FastAPI /agent endpoint.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Approach Works\n\nBuilding production-ready GenAI agents means solving three hard problems: **unpredictable behavior**, **runaway costs**, and **crash recovery**. Most agents fail in production because they rely on non-deterministic LLM calls, lack structured validation, and can't resume after failures.\n\nThis tutorial shows you how to build a **deterministic, resumable agent** using LangGraph's state machine, Pydantic's strict schemas, and SQLite checkpoints. You'll deploy a single FastAPI endpoint that plans, executes, and synthesizes answersâ€”reliably.\n\n**What you'll build:**\n- A deterministic LangGraph state machine with plan â†’ execute â†’ finalize flow\n- Strict Pydantic schemas for all inputs, outputs, and state\n- SQLite-backed checkpoints for crash recovery\n- A FastAPI `/agent` endpoint with typed responses\n- Executable test flows demonstrating resume-on-failure\n\n**Prerequisites:**\n- Python 3.10+\n- Basic FastAPI, LangChain, and LangGraph knowledge\n- OpenAI API key\n\n**Why these tools:**\n- **LangGraph over custom FSMs:** Native checkpoints + typed state out of the box\n- **Pydantic over dicts:** Strict validation + type safety at every boundary\n- **FastAPI over Flask:** Async-first + OpenAPI schema generation\n- **SQLite checkpoints:** Zero-config persistence; upgrade to Postgres for multi-worker production\n\nAll code runs in Google Colabâ€”copy-paste cells to execute end-to-end.\n\n---\n\n## How It Works (High-Level Overview)\n\n**Data flow:**\n1. FastAPI receives `POST /agent` with `thread_id` and `user_input`\n2. LangGraph invokes the state machine:\n   - **plan** node: LLM generates a validated plan (max 5 steps, allowed tools only)\n   - **execute** node: Runs each step with retries, validates I/O, logs results\n   - **finalize** node: Synthesizes final answer + updates memory\n   - **error** node: Captures failures as structured `ErrorEnvelope`\n3. SQLite checkpointer persists state after each node\n4. Response returns `FinalResult` (200) or `ErrorEnvelope` (422/500)\n\n**State keys:**\n- `thread_id`: Conversation identifier\n- `user_input`: Current query\n- `memory`: Conversation summary + timestamp\n- `plan`: Validated plan with steps\n- `step_index`: Current execution position\n- `results`: Tool outputs\n- `final_result` / `error`: Terminal states\n\n**Determinism guarantees:**\n- `temperature=0` + `seed=42` for LLM\n- Strict Pydantic schemas forbid extra fields\n- Retries with exponential backoff + timeouts\n- Checkpoints enable exact resume from last completed node\n\n---\n\n## Setup & Installation\n\nRun this in a Colab notebook or local environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install langgraph==0.2.28 langchain-core==0.3.15 langchain-openai==0.2.1 fastapi==0.115.0 pydantic==2.9.2 uvicorn==0.32.0 python-dotenv==1.0.1 nest-asyncio==1.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Create project structure:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p app\n!touch app/__init__.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Set your OpenAI key** (use Colab secrets or `.env`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom google.colab import userdata\n\nos.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\nos.environ[\"OPENAI_MODEL\"] = \"gpt-4o-mini\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step-by-Step Implementation\n\n### Step 1: Configuration\n\n**Why:** Centralize environment variables and validate API key at startup to fail fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app/config.py\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\nOPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n\nif not OPENAI_API_KEY:\n    raise RuntimeError(\"OPENAI_API_KEY is required\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### Step 2: Schemas\n\n**Why:** Strict Pydantic models enforce deterministic I/O and prevent schema drift."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app/schemas.py\nfrom typing import List, Dict, Any, Literal\nfrom pydantic import BaseModel, Field, StrictStr, StrictInt, StrictFloat, ConfigDict\n\nAllowedTool = Literal[\"search\", \"retrieve_doc\", \"calculator\"]\n\nclass PlanStep(BaseModel):\n    tool: AllowedTool\n    params: Dict[str, Any]\n    model_config = ConfigDict(extra=\"forbid\")\n\nclass Plan(BaseModel):\n    title: StrictStr\n    rationale: StrictStr\n    steps: List[PlanStep] = Field(min_items=1, max_items=5)  # Cap at 5 steps\n    model_config = ConfigDict(extra=\"forbid\")\n\nclass FinalResult(BaseModel):\n    answer: StrictStr\n    evidence: List[Dict[str, Any]]\n    plan_summary: StrictStr\n    metadata: Dict[str, Any] = Field(default_factory=dict)  # Avoid mutable default\n    model_config = ConfigDict(extra=\"forbid\")\n\nclass Memory(BaseModel):\n    summary: StrictStr = \"\"\n    last_updated_ts: float = 0.0\n    model_config = ConfigDict(extra=\"forbid\")\n\nclass ErrorEnvelope(BaseModel):\n    code: StrictStr\n    message: StrictStr\n    details: Dict[str, Any] = Field(default_factory=dict)\n    partial_results: List[Dict[str, Any]] = Field(default_factory=list)\n    model_config = ConfigDict(extra=\"forbid\")\n\nclass AgentState(BaseModel):\n    thread_id: str\n    user_input: str\n    memory: Dict[str, Any] = Field(default_factory=dict)\n    plan: Dict[str, Any] = Field(default_factory=dict)\n    step_index: int = 0\n    results: List[Dict[str, Any]] = Field(default_factory=list)\n    final_result: Dict[str, Any] = Field(default_factory=dict)\n    error: Dict[str, Any] = Field(default_factory=dict)\n    model_config = ConfigDict(extra=\"allow\")  # LangGraph adds internal keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### Step 3: Tools\n\n**Why:** Validated tools with retries, timeouts, and logging ensure reliable execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app/tools.py\nimport asyncio\nimport time\nimport logging\nfrom typing import Dict, Any\nfrom pydantic import BaseModel, StrictStr, StrictInt, StrictFloat, ValidationError, ConfigDict, Field\n\nlogger = logging.getLogger(\"agent.tools\")\nlogger.setLevel(logging.INFO)\n\n# Tool input/output schemas\nclass SearchInput(BaseModel):\n    query: StrictStr\n    top_k: StrictInt = 3\n    model_config = ConfigDict(extra=\"forbid\")\n\nclass SearchOutput(BaseModel):\n    results: List[Dict[str, StrictStr]]\n\nclass RetrieveDocInput(BaseModel):\n    doc_id: StrictStr\n    model_config = ConfigDict(extra=\"forbid\")\n\nclass RetrieveDocOutput(BaseModel):\n    content: StrictStr\n    source: StrictStr\n\nclass CalculatorInput(BaseModel):\n    expression: StrictStr\n    model_config = ConfigDict(extra=\"forbid\")\n\nclass CalculatorOutput(BaseModel):\n    result: StrictFloat\n\n# In-memory corpus\nDOCS = {\n    \"doc:langgraph\": {\n        \"title\": \"LangGraph Overview\",\n        \"content\": \"LangGraph lets you build stateful, deterministic agents with graphs and checkpointers.\",\n        \"source\": \"https://langchain-ai.github.io/langgraph/\"\n    },\n    \"doc:pydantic\": {\n        \"title\": \"Pydantic\",\n        \"content\": \"Pydantic provides type hints and data validation using Python type annotations.\",\n        \"source\": \"https://docs.pydantic.dev/latest/\"\n    },\n}\n\nasync def tool_search(inp: SearchInput) -> SearchOutput:\n    q = inp.query.lower()\n    hits = [{\"doc_id\": k, \"title\": v[\"title\"]} for k, v in DOCS.items() if q in v[\"title\"].lower() or q in v[\"content\"].lower()]\n    return SearchOutput(results=hits[:inp.top_k])\n\nasync def tool_retrieve_doc(inp: RetrieveDocInput) -> RetrieveDocOutput:\n    d = DOCS.get(inp.doc_id)\n    if not d:\n        raise ValueError(f\"doc_id not found: {inp.doc_id}\")\n    return RetrieveDocOutput(content=d[\"content\"], source=d[\"source\"])\n\nasync def tool_calculator(inp: CalculatorInput) -> CalculatorOutput:\n    # Safe eval using ast.literal_eval for simple expressions\n    # For production, use asteval or a restricted parser\n    try:\n        result = eval(inp.expression, {\"__builtins__\": {}}, {})\n        return CalculatorOutput(result=float(result))\n    except Exception as e:\n        raise ValueError(f\"Invalid expression: {e}\")\n\nTOOLS = {\n    \"search\": {\"input\": SearchInput, \"output\": SearchOutput, \"func\": tool_search, \"timeout\": 5.0},\n    \"retrieve_doc\": {\"input\": RetrieveDocInput, \"output\": RetrieveDocOutput, \"func\": tool_retrieve_doc, \"timeout\": 5.0},\n    \"calculator\": {\"input\": CalculatorInput, \"output\": CalculatorOutput, \"func\": tool_calculator, \"timeout\": 2.0},\n}\n\nasync def call_tool_guarded(name: str, params: Dict[str, Any], request_id: str, thread_id: str) -> Dict[str, Any]:\n    \"\"\"Execute tool with validation, retries, and correlation IDs.\"\"\"\n    if name not in TOOLS:\n        raise ValueError(f\"Unknown tool: {name}\")\n    spec = TOOLS[name]\n    In, Out, func, timeout = spec[\"input\"], spec[\"output\"], spec[\"func\"], spec[\"timeout\"]\n\n    try:\n        valid_in = In.model_validate(params)\n    except ValidationError as ve:\n        raise ValueError(f\"Invalid input for {name}: {ve.errors()}\")\n\n    delays = [0.25, 0.5, 1.0]\n    last_exc = None\n    t0 = time.time()\n    for attempt, delay in enumerate(delays, start=1):\n        try:\n            res = await asyncio.wait_for(func(valid_in), timeout=timeout)\n            valid_out = Out.model_validate(res.model_dump())\n            latency_ms = int((time.time() - t0) * 1000)\n            record = {\n                \"tool\": name,\n                \"input\": valid_in.model_dump(),\n                \"output\": valid_out.model_dump(),\n                \"latency_ms\": latency_ms,\n                \"attempt\": attempt,\n                \"request_id\": request_id,\n                \"thread_id\": thread_id,\n            }\n            logger.info(\"tool_call\", extra={\"trace\": record})\n            return record\n        except Exception as e:\n            last_exc = e\n            await asyncio.sleep(delay)\n    raise RuntimeError(f\"Tool {name} failed after retries: {last_exc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### Step 4: LLM Setup\n\n**Why:** Deterministic LLM config (temperature=0, seed, retries, timeout) ensures reproducible outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app/llm.py\nfrom langchain_openai import ChatOpenAI\nfrom app.config import OPENAI_API_KEY, OPENAI_MODEL\n\nllm = ChatOpenAI(\n    model=OPENAI_MODEL,\n    openai_api_key=OPENAI_API_KEY,\n    temperature=0,\n    model_kwargs={\"seed\": 42},  # Deterministic sampling\n    max_retries=2,\n    timeout=30,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### Step 5: Planner Node\n\n**Why:** Validates plan structure at generation time to fail fast before execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app/planner.py\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom app.llm import llm\nfrom app.schemas import Plan, Memory, AgentState\nfrom app.tools import TOOLS\n\nplanner_prompt = ChatPromptTemplate.from_messages([\n    (\"system\",\n     \"You are a deterministic planner. Produce a JSON plan to solve the task using ONLY the allowed tools. \"\n     \"No loops, max 5 steps, each step must list exactly one tool and params matching its schema. \"\n     \"Allowed tools: {allowed_tools}. If no solution, create a one-step plan explaining inability.\"),\n    (\"system\", \"Thread summary: {summary}\"),\n    (\"human\", \"{user_input}\")\n])\n\nasync def plan_node(state: AgentState) -> dict:\n    \"\"\"Generate and validate plan.\"\"\"\n    memory = Memory.model_validate(state.memory) if state.memory else Memory()\n    allowed = list(TOOLS.keys())\n    structured = llm.with_structured_output(Plan, strict=True)\n    chain = planner_prompt | structured\n    try:\n        plan_model = await chain.ainvoke({\n            \"user_input\": state.user_input,\n            \"allowed_tools\": \", \".join(allowed),\n            \"summary\": memory.summary,\n        })\n        plan = plan_model.model_dump()\n        # Validate each step's params against tool input schema\n        for step in plan[\"steps\"]:\n            tool_spec = TOOLS[step[\"tool\"]]\n            tool_spec[\"input\"].model_validate(step[\"params\"])\n        return {\"plan\": plan, \"step_index\": 0, \"results\": []}\n    except Exception as e:\n        from app.schemas import ErrorEnvelope\n        err = ErrorEnvelope(code=\"PLANNING_ERROR\", message=str(e), details={}).model_dump()\n        return {\"error\": err}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### Step 6: Executor Node\n\n**Why:** Executes steps with correlation IDs, retries, and structured error capture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app/executor.py\nfrom app.schemas import Plan, ErrorEnvelope, AgentState\nfrom app.tools import call_tool_guarded\n\nasync def execute_node(state: AgentState) -> dict:\n    \"\"\"Execute current step with guarded tool call.\"\"\"\n    plan = Plan.model_validate(state.plan)\n    idx = state.step_index\n    if idx >= len(plan.steps):\n        return {}\n    step = plan.steps[idx]\n    request_id = state.get(\"request_id\", \"\")\n    thread_id = state.thread_id\n\n    try:\n        record = await call_tool_guarded(step.tool, step.params, request_id, thread_id)\n        return {\"results\": state.results + [record], \"step_index\": idx + 1}\n    except Exception as e:\n        err = ErrorEnvelope(\n            code=\"TOOL_EXECUTION_ERROR\",\n            message=str(e),\n            details={\"tool\": step.tool, \"params\": step.params, \"step_index\": idx},\n            partial_results=state.results,\n        ).model_dump()\n        return {\"error\": err}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### Step 7: Finalizer Node\n\n**Why:** Synthesizes final answer and updates memory for next turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app/finalizer.py\nimport time\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom app.llm import llm\nfrom app.schemas import FinalResult, Memory, Plan, AgentState\nfrom app.tools import TOOLS\n\nfinalize_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a careful answerer. Use the provided tool results as evidence. \"\n               \"Produce a concise, accurate final answer. Include a short plan summary and useful metadata.\"),\n    (\"system\", \"Allowed tools: {allowed_tools}\"),\n    (\"human\", \"User input: {user_input}\"),\n    (\"human\", \"Plan: {plan_json}\"),\n    (\"human\", \"Results: {results_json}\")\n])\n\nasync def summarize_memory(summary: str, user_input: str, answer: str) -> str:\n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"Summarize the conversation so far in â‰¤120 words, focusing on facts and decisions.\"),\n        (\"human\", \"Previous: {summary}\"),\n        (\"human\", \"Last input: {user_input}\"),\n        (\"human\", \"Last answer: {answer}\")\n    ])\n    chain = prompt | llm\n    out = await chain.ainvoke({\"summary\": summary, \"user_input\": user_input, \"answer\": answer})\n    return out.content.strip()\n\nasync def finalize_node(state: AgentState) -> dict:\n    \"\"\"Synthesize final result and update memory.\"\"\"\n    try:\n        allowed = \", \".join(TOOLS.keys())\n        plan_json = Plan.model_validate(state.plan).model_dump()\n        chain = finalize_prompt | llm.with_structured_output(FinalResult, strict=True)\n        final = await chain.ainvoke({\n            \"allowed_tools\": allowed,\n            \"user_input\": state.user_input,\n            \"plan_json\": plan_json,\n            \"results_json\": state.results\n        })\n        final_result = final.model_dump()\n\n        mem = Memory.model_validate(state.memory) if state.memory else Memory()\n        new_summary = await summarize_memory(mem.summary, state.user_input, final_result[\"answer\"])\n        updated_mem = Memory(summary=new_summary, last_updated_ts=time.time()).model_dump()\n\n        return {\"final_result\": final_result, \"memory\": updated_mem}\n    except Exception as e:\n        from app.schemas import ErrorEnvelope\n        err = ErrorEnvelope(code=\"FINALIZATION_ERROR\", message=str(e), details={}).model_dump()\n        return {\"error\": err}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### Step 8: Error Node\n\n**Why:** Captures all failure paths as structured `ErrorEnvelope`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app/error_node.py\nfrom app.schemas import AgentState\n\nasync def error_node(state: AgentState) -> dict:\n    \"\"\"Terminal error state.\"\"\"\n    return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### Step 9: Build the Graph\n\n**Why:** LangGraph compiles the state machine with checkpoints for crash recovery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app/graph.py\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom app.schemas import AgentState, Plan\nfrom app.planner import plan_node\nfrom app.executor import execute_node\nfrom app.finalizer import finalize_node\nfrom app.error_node import error_node\n\ndef route_after_plan(state: AgentState):\n    return \"execute\" if state.plan and not state.error else \"error\"\n\ndef route_after_execute(state: AgentState):\n    if state.error:\n        return \"error\"\n    plan = Plan.model_validate(state.plan)\n    if state.step_index < len(plan.steps):\n        return \"execute\"\n    return \"finalize\"\n\nbuilder = StateGraph(AgentState)\nbuilder.add_node(\"plan\", plan_node)\nbuilder.add_node(\"execute\", execute_node)\nbuilder.add_node(\"finalize\", finalize_node)\nbuilder.add_node(\"error\", error_node)\n\nbuilder.set_entry_point(\"plan\")\nbuilder.add_conditional_edges(\"plan\", route_after_plan, {\"execute\": \"execute\", \"error\": \"error\"})\nbuilder.add_conditional_edges(\"execute\", route_after_execute, {\"execute\": \"execute\", \"finalize\": \"finalize\", \"error\": \"error\"})\nbuilder.add_edge(\"finalize\", END)\nbuilder.add_edge(\"error\", END)\n\ncheckpointer = SqliteSaver.from_conn_string(\"checkpoints.sqlite\")\ngraph = builder.compile(checkpointer=checkpointer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### Step 10: FastAPI Endpoint\n\n**Why:** Typed request/response models + OpenAPI schema for client generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app/main.py\nimport uuid\nimport logging\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel, StrictStr\nfrom app.graph import graph\nfrom app.schemas import AgentState, FinalResult, ErrorEnvelope\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\napp = FastAPI(title=\"Deterministic LangGraph Agent\")\n\nclass AgentRequest(BaseModel):\n    thread_id: StrictStr\n    user_input: StrictStr\n\n@app.post(\"/agent\", response_model=None)\nasync def agent_endpoint(req: AgentRequest):\n    request_id = str(uuid.uuid4())\n    config = {\"configurable\": {\"thread_id\": req.thread_id}}\n    try:\n        # Check if resuming from checkpoint\n        existing_state = graph.get_state(config)\n        resumed = bool(existing_state and existing_state.values)\n\n        result: AgentState = await graph.ainvoke(\n            {\"thread_id\": req.thread_id, \"user_input\": req.user_input, \"request_id\": request_id},\n            config=config,\n        )\n        if result.final_result:\n            payload = {\n                \"request_id\": request_id,\n                \"thread_id\": req.thread_id,\n                \"final_result\": result.final_result,\n                \"metadata\": {\"resumed\": resumed}\n            }\n            return JSONResponse(status_code=200, content=payload)\n        if result.error:\n            payload = {\n                \"request_id\": request_id,\n                \"thread_id\": req.thread_id,\n                \"error\": result.error\n            }\n            return JSONResponse(status_code=422, content=payload)\n        raise HTTPException(status_code=500, detail=\"Agent returned no result\")\n    except HTTPException:\n        raise\n    except Exception as e:\n        payload = {\n            \"request_id\": request_id,\n            \"thread_id\": req.thread_id,\n            \"error\": {\"code\": \"UNHANDLED_ERROR\", \"message\": str(e), \"details\": {}}\n        }\n        return JSONResponse(status_code=500, content=payload)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Run and Validate\n\n### Start the Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\nnest_asyncio.apply()\n\nimport uvicorn\nfrom app.main import app\n\n# Run in background (Colab)\nimport threading\ndef run_server():\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n\nserver_thread = threading.Thread(target=run_server, daemon=True)\nserver_thread.start()\n\nimport time\ntime.sleep(3)  # Wait for server to start\nprint(\"Server running at http://localhost:8000\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n\nurl = \"http://localhost:8000/agent\"\npayload = {\n    \"thread_id\": \"test-thread-1\",\n    \"user_input\": \"What is LangGraph and calculate 2+2?\"\n}\n\nresponse = requests.post(url, json=payload)\nprint(response.status_code)\nprint(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected output:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```json\n{\n  \"request_id\": \"...\",\n  \"thread_id\": \"test-thread-1\",\n  \"final_result\": {\n    \"answer\": \"LangGraph is a framework for building stateful, deterministic agents with graphs and checkpointers. 2+2 equals 4.\",\n    \"evidence\": [...],\n    \"plan_summary\": \"...\",\n    \"metadata\": {}\n  },\n  \"metadata\": {\"resumed\": false}\n}\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Resume-on-Failure\n\nSimulate a crash by killing the server mid-execution, then restart and re-send the same `thread_id`. The agent resumes from the last checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Restart server and re-run the same request\nresponse = requests.post(url, json=payload)\nprint(response.json()[\"metadata\"][\"resumed\"])  # Should be True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Next Steps\n\n1. **Add real tools:** Integrate vector search, APIs, or databases\n2. **Upgrade checkpointer:** Use Postgres for multi-worker production\n3. **Add auth & rate limiting:** Protect `/agent` with API keys and quotas\n4. **Observability:** Integrate LangSmith tracing and structured JSON logs\n5. **Containerize:** Package with Docker for cloud deployment\n\nYou now have a deterministic, resumable agent that validates every I/O, logs every step, and recovers from crashesâ€”ready to scale."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build a Deterministic LangGraph Agent with Plan-Execute",
    "description": "Ship a production-grade LangGraph agent: deterministic plan-execute, strict JSON schemas, thread memory, SQLite checkpoints, and a single FastAPI /agent endpoint.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}