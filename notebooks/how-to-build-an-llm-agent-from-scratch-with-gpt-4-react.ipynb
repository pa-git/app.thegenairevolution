{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìì The GenAI Revolution Cookbook\n\n**Title:** How to Build an LLM Agent from Scratch with GPT-4 ReAct\n\n**Description:** Build a fully functional LLM agent in Python using ReAct, tool actions, regex parsing, and GPT-4, automated control loop included.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here‚Äôs an improved version of your draft content. I‚Äôve added an introduction to situate this as a fundamental demonstration, linked it explicitly to the *ReAct* paper (arXiv:2210\\.03629\\) with key insights, and preserved your original technical structure. I also applied humanization rules for clarity and flow.\n\n---\n\n## Demonstrating What AI Agents Really Are\n\nYou probably use agent frameworks like LangChain or BabyAGI without really seeing how they work under the hood. This tutorial is a chance to peel back the layers. You‚Äôre going to build a ReAct\\-style agent from scratch. This will help you understand what AI agents fundamentally are, how they reason, act, and loop with tools‚Äîall without any third\\-party framework. When you see how each part works, you‚Äôll understand more clearly what those frameworks are doing for you.\n\nThe core idea comes from the paper *‚ÄúReAct: Synergizing Reasoning and Acting in Language Models‚Äù* (arXiv:2210\\.03629\\) by Yao et al. ([arxiv.org](https://arxiv.org/abs/2210.03629?utm_source=openai)) ReAct shows that you can get better performance and interpretability when a model both reasons (writes *Thoughts*) and acts (calls tools), weaving those steps together with observations. ([arxiv.org](https://arxiv.org/abs/2210.03629?utm_source=openai))\n\n---\n\n## Introduction\n\nBuilding an agent that can reason, call tools, and iterate without supervision is now straightforward‚Äîif you control the format, validate the inputs, and guard against runaway loops. This tutorial shows you how to build a ReAct agent from scratch using the OpenAI Python SDK, three simple tools (distance lookup, travel time calculation, sum), a regex\\-based action parser, and a single\\-action\\-per\\-turn control loop with a max\\-turn guard. You‚Äôll end with a runnable, testable agent that can answer multi\\-step questions like ‚ÄúHow long to drive from Montreal to Boston at 60 mph?‚Äù\n\n**Prerequisites:** Python 3\\.10\\+, an OpenAI API key, and basic familiarity with LLMs. Token usage is minimal. Using `gpt-4o-mini` for tool lookups keeps costs low. This tutorial is Colab\\-ready and starts with a `!pip install` cell.\n\n---\n\n## Why This Approach Works\n\nFrameworks add convenience‚Äîbut they hide behavior. If you build things from scratch, you gain complete control. You can debug faster. You‚Äôll know exactly when and why the model does something.\n\n**Why GPT\\-4o and GPT\\-4o\\-mini?**\nGPT\\-4o provides strong reasoning for the main agent loop. GPT\\-4o\\-mini handles simple tool lookups cheaply and quickly. The combination keeps cost and latency balanced without losing reliability.\n\n**Why ReAct?**\nThe ReAct pattern (Reason \\+ Act) forces the model to articulate its reasoning before taking action. That makes behavior interpretable and easier to debug. A strict format‚ÄîThought ‚Üí Action ‚Üí PAUSE ‚Üí Observation‚Äîlets you parse and validate every step programmatically.\n\n**Why regex parsing?**\nRegex is deterministic, fast, and transparent. You know exactly what matches and what does not. For production you could swap in JSON\\-based arguments or the OpenAI function\\-call API. Regex is a simple way to start.\n\n---\n\n## How It Works (High\\-Level Overview)\n\n1. **Agent receives a question**, then generates a Thought and an Action. Example: `lookup_distance[Montreal, Boston]`.\n2. **Control loop parses the Action** using regex, validates it, and calls its Python function.\n3. **Tool returns a result** (e.g. `308 miles`), which becomes the Observation.\n4. **Agent updates reasoning** with the new info. It either calls another tool or gives a final Answer.\n5. **Loop stops** once the agent outputs `Answer:` or hits the max turn limit.\n\n---\n\n## Setup \\& Installation\n\nRun this cell in Colab or your local environment to install dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your OpenAI API key. In Colab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Replace with your key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or create a `.env` file locally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_API_KEY=sk-..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify the key is set before proceeding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise EnvironmentError(\"OPENAI_API_KEY not set. Please set it before running.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step\\-by\\-Step Implementation\n\n### Define the System Prompt\n\nThis is the contract between you and the model. It enforces the ReAct format and lists available actions with exact signatures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REACT_SYSTEM_PROMPT = \"\"\"You are a ReAct-style agent. Follow this exact format:\n\nThought: describe your reasoning briefly.\nAction: function_name[param1, param2, ...]\nPAUSE\n\nAfter you receive an Observation, continue:\n\nThought: update your reasoning briefly.\n(Optional another) Action: function_name[‚Ä¶]\nPAUSE\n\nWhen done, provide:\nAnswer: <final answer, concise and complete>\n\nRules:\n- At most ONE Action per turn.\n- If no action is needed, go directly to Answer.\n- Use only these actions with exact signatures:\n  - lookup_distance[location1, location2]\n  - calculate_travel_time[distance, speed]\n  - calculate_sum[value1, value2]\n- Use miles for distance and mph for speed unless specified.\n- Wait for Observation after PAUSE; do not fabricate results.\n\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### Build the Agent Class\n\nThe `Agent` class manages conversation history. It makes calls to the OpenAI API. It keeps track of messages so the model has full context each turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport logging\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict\nfrom openai import OpenAI\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n@dataclass\nclass Agent:\n    system_prompt: str\n    model: str = \"gpt-4o\"\n    temperature: float = 0.0\n    messages: List[Dict[str, str]] = field(default_factory=list)\n\n    def __post_init__(self):\n        self.messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n\n    def __call__(self, user_content: str) -> str:\n        self.messages.append({\"role\": \"user\", \"content\": user_content})\n        return self.execute()\n\n    def execute(self) -> str:\n        resp = client.chat.completions.create(\n            model=self.model,\n            temperature=self.temperature,\n            messages=self.messages,\n        )\n        content = resp.choices[0].message.content\n        self.messages.append({\"role\": \"assistant\", \"content\": content})\n        logger.debug(f\"Assistant response: {content}\")\n        return content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### Implement the Tools\n\nEach tool is a simple Python function. Distance lookup uses an LLM call for realism. You can swap in deterministic maps or APIs later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n\ndef generate_response(prompt: str, model: str = \"gpt-4o-mini\") -> str:\n    resp = client.chat.completions.create(\n        model=model,\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a precise assistant. Reply with the answer only.\"},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    )\n    return resp.choices[0].message.content.strip()\n\ndef lookup_distance(location1: str, location2: str) -> str:\n    prompt = (\n        f\"What is the typical driving distance in miles between {location1} and {location2}? \"\n        \"Return a single number followed by ' miles'. No extra text.\"\n    )\n    ans = generate_response(prompt)\n    m = re.search(r\"([0-9]+(?:.[0-9]+)?)\\\\s*miles\", ans, re.IGNORECASE)\n    if not m:\n        ans = generate_response(\n            f\"Return only the distance number in miles for driving between {location1} and {location2} in the format '<number> miles'.\"\n        )\n        m = re.search(r\"([0-9]+(?:.[0-9]+)?)\\\\s*miles\", ans, re.IGNORECASE)\n    return f\"{m.group(1)} miles\" if m else \"unknown miles\"\n\ndef _extract_number(s: str) -> float:\n    m = re.search(r\"(-?\\\\d+(?:\\.\\\\d+)?)\", s)\n    if not m:\n        raise ValueError(f\"Cannot parse number from: {s}\")\n    return float(m.group(1))\n\ndef calculate_travel_time(distance: str, speed: str) -> str:\n    d = _extract_number(distance)\n    v = _extract_number(speed)\n    if v == 0:\n        return \"infinite hours\"\n    hours = d / v\n    return f\"{round(hours, 2)} hours\"\n\ndef _extract_number_and_unit(s: str) -> tuple:\n    m = re.search(r\"(-?\\\\d+(?:\\.\\\\d+)?)\\\\s*([a-zA-Z/%]+)?\", s.strip())\n    if not m:\n        raise ValueError(f\"Cannot parse: {s}\")\n    value = float(m.group(1))\n    unit = m.group(2) or \"\"\n    return value, unit\n\ndef calculate_sum(value1: str, value2: str) -> str:\n    v1, u1 = _extract_number_and_unit(value1)\n    v2, u2 = _extract_number_and_unit(value2)\n    unit = u1 if u1 == u2 else \"\"\n    total = v1 + v2\n    out = f\"{round(total, 2)}{(' ' + unit) if unit else ''}\"\n    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### Register Tools and Define Parsers\n\nSet up a registry for dispatch. Use regex patterns to parse the agent‚Äôs output into actions or answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional, List, Tuple\n\nKNOWN_ACTIONS = {\n    \"lookup_distance\": lookup_distance,\n    \"calculate_travel_time\": calculate_travel_time,\n    \"calculate_sum\": calculate_sum,\n}\n\nACTION_RE = re.compile(\n    r\"Action:\\\\s*(?P<name>\\\\w+)\\\\s*\\[(?P<params>.*?)]\\\\s*PAUSE\",\n    re.IGNORECASE | re.DOTALL\n)\nANSWER_RE = re.compile(r\"Answer:\\\\s*(?P<answer>.+)\", re.IGNORECASE | re.DOTALL)\n\ndef parse_action(text: str) -> Optional[Tuple[str, List[str]]]:\n    m = ACTION_RE.search(text)\n    if not m:\n        return None\n    name = m.group(\"name\")\n    raw = m.group(\"params\").strip()\n    params = [p.strip().strip(\"\"'\").strip() for p in raw.split(\",\")] if raw else []\n    return name, params\n\ndef parse_answer(text: str) -> Optional[str]:\n    m = ANSWER_RE.search(text)\n    return m.group(\"answer\").strip() if m else None\n\ndef validate_action(name: str, params: List[str]) -> bool:\n    if name not in KNOWN_ACTIONS:\n        raise ValueError(f\"Unknown action: {name}\")\n    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### Build the Control Loop\n\nThis loops through thinking, acting, observing until you get a final answer or hit a turn limit. This prevents runaway agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_agent_loop(question: str, max_turns: int = 10, verbose: bool = True) -> str:\n    agent = Agent(system_prompt=REACT_SYSTEM_PROMPT, model=\"gpt-4o\", temperature=0)\n    last = agent(question)\n    if verbose:\n        print(\"TURN 1 - ASSISTANT\\n\", last, \"\\n\")\n\n    turn = 1\n    while turn < max_turns:\n        answer = parse_answer(last)\n        if answer:\n            if verbose:\n                print(\"FINAL ANSWER\\n\", answer)\n            return answer\n\n        parsed = parse_action(last)\n        if not parsed:\n            if verbose:\n                print(\"No action or answer detected. Stopping.\")\n            return \"Unable to complete: no action or answer detected.\"\n\n        name, params = parsed\n        try:\n            validate_action(name, params)\n            tool = KNOWN_ACTIONS[name]\n            result = tool(*params)\n        except Exception as e:\n            result = f\"ERROR: {str(e)}\"\n\n        obs_msg = f\"Observation: {result}\"\n        turn += 1\n        last = agent(obs_msg)\n        if verbose:\n            print(f\"TURN {turn} - OBSERVATION\\n\", obs_msg)\n            print(f\"TURN {turn} - ASSISTANT\\n\", last, \"\\n\")\n\n    if verbose:\n        print(\"Max turns reached without final answer.\")\n    return \"Unable to complete within turn limit.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Run and Validate\n\nTest the agent with a multi\\-step question that requires two tool calls: distance lookup, then travel time calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    print(run_agent_loop(\"How long to drive from Montreal to Boston at 60 mph?\", max_turns=8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected output:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TURN 1 - ASSISTANT\nThought: I need to find the distance from Montreal to Boston first.\nAction: lookup_distance[Montreal, Boston]\nPAUSE\n\nTURN 2 - OBSERVATION\n..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Connecting Back to ReAct (the Paper)\n\nHere is what *ReAct: Synergizing Reasoning and Acting in Language Models* teaches you, and how it shaped this tutorial:\n\n* The paper shows that when you interleave reasoning and action you get much more robust behavior. Reasoning lets the model plan. Actions let it get grounded information. Observations let it correct and update what it thought. ([arxiv.org](https://arxiv.org/abs/2210.03629?utm_source=openai))\n* ReAct outperforms reasoning\\-only (Chain\\-of\\-Thought) and acting\\-only approaches on tasks like HotpotQA and FEVER. It reduces hallucinations and error propagation by using external sources. ([arxiv.org](https://arxiv.org/abs/2210.03629?utm_source=openai))\n* Its format is similar here: *Thought ‚Üí Action ‚Üí Observation ‚Üí Thought ‚Üí ‚Ä¶ Answer*. That gives you both interpretability and control.\n* You‚Äôll see in the frameworks you already use that they adopt very similar contracts: they make you define tool signatures, control loops, stopping criteria, etc. This tutorial reproduces those pieces explicitly so you see them.\n\n---\n\nYou‚Äôre now set up to experiment, tweak formats, add tools, or change the logic. Doing this by hand teaches you what the frameworks automate. That knowledge will make you a better builder, better debugger, and better decision\\-maker about which abstractions to introduce."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build an LLM Agent from Scratch with GPT-4 ReAct",
    "description": "Build a fully functional LLM agent in Python using ReAct, tool actions, regex parsing, and GPT-4, automated control loop included.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}