{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìì The GenAI Revolution Cookbook\n\n**Title:** The Ultimate Guide to Vector Store Retrieval for RAG Systems\n\n**Description:** Build reliable RAG today: implement vector store retrieval with semantic search, chunking, and embeddings‚Äîmanually or with LangChain‚Äîto reduce LLM hallucinations.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieval Augmented Generation, RAG, is a powerful design pattern for leveraging large language models. It powers data\\-aware chatbots, customer support assistants, and many other tools. It also solves a key problem. It grounds model responses in real information, which reduces hallucinations.\n\nRAG has two halves. Retrieval and generation. This guide is RAG 101, the retrieval part. You will focus only on how to find the right context for a model. You will create an index, then search it. You will do it first by hand, then with LangChain. You can plug the retrieved results into your LLM later.\n\nThe heart of this tutorial is retrieval. Retrieval is the process of finding and selecting the most relevant documents. Depending on your use case, you can approach retrieval in several ways. Vector store retrieval is the most popular.\n\nIn this walkthrough you will learn the two essential retrieval steps. You will create the index. Then you will search the index. You will see the mechanics clearly. You will also see how LangChain streamlines the same tasks.\n\n### Semantic Similarity Search done manually\n\nSemantic Similarity Search is a widely used retrieval approach. The goal is to identify documents that best match a given input based on meaning. This method works best when you need to match semantic content across text. For example, you may compare a user query to sentences or paragraphs in a document. It is a strong choice when you want to retrieve internal knowledge to enhance customer support. It also helps you add grounded context to LLM responses.\n\nYou will implement Semantic Similarity Search manually using Sentence Transformers for embeddings and ChromaDB as your vector store.\n\n### Setup\n\nBefore you begin, make sure you install the required Python libraries. This example uses:\n\n* chromadb. A fast and efficient vector database.\n* sentence\\-transformers. A library for generating sentence embeddings.\n* PyPDF. A modern library for reading and processing PDF files.\n* langchain. A framework for building LLM\\-powered applications.\n* langchain\\-community. Extensions and integrations for LangChain.\n* langchain\\-huggingface. Tools to use Hugging Face models within LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install chromadb\n%pip install sentence-transformers\n%pip install pypdf\n%pip install langchain\n%pip install langchain-community\n%pip install langchain-huggingface\n%pip install langchain-chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1\\. Create the index\n\nWith everything installed, start by preparing an index for your documents. You will:\n\n1. Load the documents. Extract text from the source file.\n2. Split the text. Break the text into semantically meaningful chunks.\n3. Generate embeddings. Use a pre\\-trained model to represent each chunk as a vector.\n4. Store data in a vector database. Save chunks, embeddings, and metadata in a vector store for retrieval.\n\nHere is a step\\-by\\-step implementation using the Minutes of the Federal Open Market Committee as the document:[https://www.federalreserve.gov/monetarypolicy/files/fomcminutes20241107\\.pdf](https://www.federalreserve.gov/monetarypolicy/files/fomcminutes20241107.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Import required libraries\nfrom sentence_transformers import SentenceTransformer\nfrom chromadb import Client\nfrom chromadb.config import Settings\nfrom pypdf import PdfReader\n\n# Step 2: Define functions for processing\n\ndef load_pdf(filepath):\n    \"\"\"\n    Load text from a PDF document.\n    \"\"\"\n    reader = PdfReader(filepath)\n    text = \"\"\n    for page in reader.pages:\n        text += page.extract_text()\n    print(f\"Text with {len(text.split())} words extracted from pdf {filepath}.\")\n    return text\n\ndef split_text(text, chunk_size=500, overlap=100):\n    \"\"\"\n    Split the text into chunks of a specified size with overlap.\n    \"\"\"\n    chunks = []\n    start = 0\n    while start < len(text):\n        end = start + chunk_size\n        chunks.append(text[start:end])\n        start = end - overlap\n    print(f\"Text split in {len(chunks)} chunks.\")\n    return chunks\n\n# Step 3: Load and process the document\npdf_path = \"fomcminutes20241107.pdf\"  # Replace with your PDF path\ndocument_text = load_pdf(pdf_path)\ndocument_chunks = split_text(document_text)\n\n# Step 4: Generate embeddings\n# Load a pre-trained SentenceTransformer model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')  # A lightweight and fast embedding model\nembeddings = model.encode(document_chunks)\nprint(f\"Embeddings generated for {len(embeddings)} chunks.\")\nprint(f\"Each embedding is of length {len(embeddings[0])}.\")\n\n# Step 5: Set up ChromaDB and create the index\npersist_directory = \"./chroma_db\"\nchroma_client = Client(Settings(persist_directory=persist_directory))\n\ncollection_name = \"fomc_minutes_20241107\"\ncollection = chroma_client.create_collection(collection_name)\nprint(f\"ChromaDB collection {collection_name} created.\")\n\n# Prepare bulk data for adding to the collection\nids = [f\"chunk_{i}\" for i in range(len(document_chunks))]\nmetadatas = [{\"chunk_id\": i, \"source\": \"sample_product_document.pdf\"} for i in range(len(document_chunks))]\n\n# Add chunks, embeddings, and metadata in bulk\ncollection.add(\n    documents=document_chunks,\n    metadatas=metadatas,\n    ids=ids,\n    embeddings=embeddings.tolist(),  # Convert numpy array to list\n)\n\n# Step 7: Confirm the index\nprint(f\"All {collection.count()} documents added to the collection {collection_name}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Text with 7385 words extracted from pdf fomcminutes20241107.pdf.\nText split in 127 chunks.\nEmbeddings generated for 127 chunks.\nEach embedding is of length 384.\nChromaDB collection fomc_minutes_20241107 created.\nAll 127 documents added to the collection fomc_minutes_20241107."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2\\. Search the index\n\nWith the vector database populated, you can now perform searches. You will:\n\n* Load the collection. Initialize the database client and load the collection.\n* Generate a query embedding. Convert the search query into a vector.\n* Retrieve results. Use the vector database to find the most similar chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Initialize ChromaDB client and load the collection\n# We won't run this since the chroma client was already created in this session\n\n# chroma_client = Client(Settings(persist_directory=\"chroma_db\")) \n# collection_name = \"fomc_minutes_20241107\"\n# collection = chroma_client.get_collection(collection_name)\n# print(f\"Collection '{collection_name}' loaded successfully.\")\n\n# Step 2: Define the search query\nsearch_query = \"What was discussed about monetary policy?\"  # Replace with your query\nprint(f\"Search Query: {search_query}\")\n\n# Step 3: Generate embeddings for the search query\nquery_embedding = model.encode([search_query])  # Generate embedding for the query\nprint(f\"Embedding generated for search query.\")\nprint(f\"Embedding is of length {len(query_embedding[0])}.\")\n\n# Step 4: Perform the search\n# Set the number of top results to retrieve\ntop_k = 5\nresults = collection.query(\n    query_embeddings=query_embedding.tolist(),  # Convert numpy array to list\n    n_results=top_k,\n)\n\n# Access the first (and only) batch of results\ndocuments = results['documents'][0]\nmetadatas = results['metadatas'][0]\ndistances = results['distances'][0]\n\n# Print each result\nprint(f\"\\nTop {top_k} Results:\")\nfor i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances), start=1):\n    print(f\"\\nResult {i}:\")\n    print(f\"Document Chunk: {doc}\")\n    print(f\"Metadata: {metadata}\")\n    print(f\"Distance: {distance}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Search Query: What was discussed about monetary policy?\nEmbedding generated for search query.\nEmbedding is of length 384.\n\nTop 5 Results:\n\nResult 1:\nDocument Chunk: to\ncontinue the process of reducing the Federal Reserve‚Äôs securities holdings.\nIn discussing the outlook for monetary policy, participants anticipated that if the data came in about\nas expected, with inflation continuing to move down sustainably to 2 percent and the economy\nremaining near maximum employment, it would likely be appropriate to move gradually toward a more\nneutral stance of policy over time. Participants noted that monetary policy decisions were not on a\npreset course and w\nMetadata: {'chunk_id': 84, 'source': 'sample_product_document.pdf'}\nDistance: 0.6293219923973083\n\nResult 2:\nDocument Chunk: icy over time. Participants noted that monetary policy decisions were not on a\npreset course and were conditional on the evolution of the economy and the implications for the\neconomic outlook and the balance of risks; they stressed that it would be important for the Committee\nto make this clear as it adjusted its policy stance. While emphasizing that monetary policy would be\ndata dependent, many participants noted the volatility of recent economic data and highlighted the\nimportance of fo\nMetadata: {'chunk_id': 85, 'source': 'sample_product_document.pdf'}\nDistance: 0.7037546634674072\n\nResult 3:\nDocument Chunk: percent objective.\nMembers agreed that, in assessing the appropriate stance of monetary policy, they would continue to\nmonitor the implications of incoming information for the economic outlook. They would be prepared to Minutes of the Federal Open Market Committee 13\n\nadjust the stance of monetary policy as appropriate if risks emerged that could impede the attainment\nof the Committee‚Äôs goals. Members also agreed that their assessments would take into account a\nwide range of informati\nMetadata: {'chunk_id': 95, 'source': 'sample_product_document.pdf'}\nDistance: 0.736047089099884\n\nResult 4:\nDocument Chunk: tered. Many participants observed\nthat uncertainties concerning the level of the neutral rate of interest complicated the assessment of\nthe degree of restrictiveness of monetary policy and, in their view, made it appropriate to reduce policy\nrestraint gradually.\nCommittee Policy Actions\nIn their discussions of monetary policy for this meeting, members agreed that economic activity had\ncontinued to expand at a solid pace. Labor market conditions had generally eased since earlier in the\ny\nMetadata: {'chunk_id': 90, 'source': 'sample_product_document.pdf'}\nDistance: 0.736309289932251\n\nResult 5:\nDocument Chunk: t and inflation goals\nremained roughly in balance. Some participants judged that downside risks to economic activity or\nthe labor market had diminished. Participants noted that monetary policy would need to balance the\nrisks of easing policy too quickly, thereby possibly hindering further progress on inflation, with the risks\nof easing policy too slowly, thereby unduly weakening economic activity and employment. In\ndiscussing the positioning of monetary policy in response to potential ch\nMetadata: {'chunk_id': 88, 'source': 'sample_product_document.pdf'}\nDistance: 0.8098248243331909"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streamlining with LangChain. RAG 101 retrieval made simpler\n\nThe manual approach gives you complete control. It can be verbose. It also requires more setup. LangChain provides a streamlined alternative that abstracts many steps. It helps you build and manage retrieval\\-augmented applications faster. In this section, you will replicate the process using LangChain. You will see the value of its high\\-level utilities for retrieval.\n\n### Step 1\\. Create the index\n\nWith LangChain, creating an index becomes more intuitive. The library offers tools for document loading, text splitting, and embedding generation. Here is how to create an index for the same Minutes of the Federal Open Market Committee document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Import required libraries\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain.vectorstores import Chroma\n\n# Step 2: Load the document\npdf_path = \"fomcminutes20241107.pdf\"  # Replace with your PDF path\nloader = PyPDFLoader(pdf_path)  # LangChain's PDF loader\ndocuments = loader.load()  # Load text from the PDF\nprint(f\"Loaded {len(documents)} page(s) from the PDF.\")\n\n# Step 3: Split the text into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\nchunks = text_splitter.split_documents(documents)\nprint(f\"Split all text into {len(chunks)} chunks.\")\n\n# Step 4: Generate embeddings\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Step 5: Store in a Vector Database\npersist_directory = \"fomc_vector_db\"\nvector_db = Chroma.from_documents(\n    documents=chunks,\n    embedding=embedding_model,\n    persist_directory=persist_directory,  # Persistence is automatic in Chroma >= 0.4.x\n)\n\nprint(f\"Vector database created and stored at '{persist_directory}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Loaded 17 page(s) from the PDF.\nSplit all text into 131 chunks.\nVector database created and stored at 'fomc_vector_db'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2\\. Search the index\n\nLangChain also simplifies querying the index by managing the search process internally. Here is how to perform a semantic search on the vector database:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Import required libraries\nfrom langchain_chroma import Chroma\nfrom langchain_huggingface import HuggingFaceEmbeddings \n\n# Step 2: Load the vector database\npersist_directory = \"fomc_vector_db\"\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nvector_db = Chroma(\n    persist_directory=persist_directory,\n    embedding_function=embedding_model,\n)\nprint(f\"Vector database loaded from '{persist_directory}'.\")\n\n# Step 3: Define the search query\nquery = \"What were the key points discussed about monetary policy?\"\nprint(f\"Search Query: {query}\")\n\n# Step 4: Perform the search\ntop_k = 5  # Number of top results to retrieve\nresults = vector_db.similarity_search(query, k=top_k)\n\n# Step 5: Display the results\nprint(f\"\\nTop {top_k} Results:\")\nfor i, result in enumerate(results, start=1):\n    print(f\"\\nResult {i}:\")\n    print(f\"Document Chunk: {result.page_content}\")\n    print(f\"Metadata: {result.metadata}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Search Query: What were the key points discussed about monetary policy?\n\nTop 5 Results:\n\nResult 1:\nDocument Chunk: 25 basis points to 4¬Ω to 4¬æ percent. Participants observed that such a further recalibration of the\nmonetary policy stance would help maintain the strength in the economy and the labor market while\ncontinuing to enable further progress on inflation. Participants judged that it was appropriate to\ncontinue the process of reducing the Federal Reserve‚Äôs securities holdings.\nIn discussing the outlook for monetary policy, participants anticipated that if the data came in about\nMetadata: {'page': 10, 'source': 'fomcminutes20241107.pdf'}\n\nResult 2:\nDocument Chunk: as expected, with inflation continuing to move down sustainably to 2 percent and the economy\nremaining near maximum employment, it would likely be appropriate to move gradually toward a more\nneutral stance of policy over time. Participants noted that monetary policy decisions were not on a\npreset course and were conditional on the evolution of the economy and the implications for the\neconomic outlook and the balance of risks; they stressed that it would be important for the Committee\nMetadata: {'page': 10, 'source': 'fomcminutes20241107.pdf'}\n\nResult 3:\nDocument Chunk: would be prepared to adjust the stance of monetary policy as appropriate if risks emerge that\ncould impede the attainment of the Committee‚Äôs goals. The Committee‚Äôs assessments will\ntake into account a wide range of information, including readings on labor market conditions,\ninflation pressures and inflation expectations, and financial and international developments.‚Äù\nVoting for this action: Jerome H. Powell, John C. Williams, Thomas I. Barkin, Michael S. Barr,\nMetadata: {'page': 13, 'source': 'fomcminutes20241107.pdf'}\n\nResult 4:\nDocument Chunk: commencement of policy easing in September and therefore was no longer needed. Almost all\nmembers agreed that the risks to achieving the Committee‚Äôs employment and inflation goals were\nroughly in balance. Members viewed the economic outlook as uncertain and agreed that they were\nattentive to the risks to both sides of the Committee‚Äôs dual mandate.\nIn support of its goals, the Committee decided to lower the target range for the federal funds rate by\nMetadata: {'page': 11, 'source': 'fomcminutes20241107.pdf'}\n\nResult 5:\nDocument Chunk: American countries, notably Brazil, inflation increased, partly because of renewed food price\npressures.\nMany foreign central banks eased policy during the intermeeting period, including the Bank of Canada\nand the European Central Bank among the AFEs and the central banks of Colombia, Mexico, Korea,\nthe Philippines, and Thailand among the emerging market economies.\nStaff Review of the Financial Situation\nMetadata: {'page': 3, 'source': 'fomcminutes20241107.pdf'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n\nRetrieval is foundational to RAG. This is RAG 101\\. You learned how to create an index by processing and embedding documents. You also learned how to search that index to find relevant information based on a query. You saw both methods in action using the Minutes of the Federal Open Market Committee. The manual path gave you transparency and control. The LangChain path showed how abstraction saves time and simplifies workflows.\n\nYou are now ready to apply retrieval in your applications. Use it to enhance chatbot responses. Use it to build powerful search experiences. Retrieval is the heart of RAG. It enables meaningful, grounded interactions in LLM\\-powered applications."
      ]
    }
  ],
  "metadata": {
    "title": "The Ultimate Guide to Vector Store Retrieval for RAG Systems",
    "description": "Build reliable RAG today: implement vector store retrieval with semantic search, chunking, and embeddings‚Äîmanually or with LangChain‚Äîto reduce LLM hallucinations.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}