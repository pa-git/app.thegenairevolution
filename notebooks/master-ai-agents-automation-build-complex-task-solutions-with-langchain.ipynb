{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Master AI Agents Automation: Build Complex Task Solutions with LangChain\n\n**Description:** Discover how to automate complex tasks using AI agents with LangChain. Learn integration strategies, tool development, and deployment best practices for real-world applications.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a Memory-Aware Chatbot with LangChain and Hugging Face\n\n## Introduction\n\nIn this tutorial, we will guide you through building a memory-aware chatbot using LangChain and Hugging Face Transformers. This project addresses the real-world problem of creating chatbots that can remember past interactions, providing a more personalized user experience. By the end of this tutorial, you'll have a fully functional chatbot that can be deployed in a production environment.\n\n## Installation\n\nFirst, let's install the necessary libraries. Open a new Colab notebook and run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install langchain transformers chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project Setup\n\nWe'll start by setting up our environment variables and configuration files. This is crucial for managing API keys and other sensitive information securely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n\n# Set your API key\nos.environ['OPENAI_API_KEY'] = 'your-openai-api-key'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step-by-Step Build\n\n### Data Handling\n\nThe first component involves handling user input and storing conversation history. We'll use a simple list to store past interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize conversation history\nconversation_history = []\n\ndef add_to_history(user_input, bot_response):\n    conversation_history.append({'user': user_input, 'bot': bot_response})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Integration\n\nNext, we'll integrate a pre-trained language model from Hugging Face to generate responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load model and tokenizer\nmodel_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\ndef generate_response(user_input):\n    inputs = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n    outputs = model.generate(inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector DB Setup\n\nFor memory, we'll use ChromaDB to store and retrieve conversation history efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from chromadb import ChromaDB\n\n# Initialize ChromaDB\ndb = ChromaDB()\n\ndef store_interaction(user_input, bot_response):\n    db.add_document({'user_input': user_input, 'bot_response': bot_response})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### API Calls\n\nWe'll create a function to handle API calls, integrating the chatbot with external services if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n\ndef call_external_api(query):\n    response = requests.get(f\"https://api.example.com/search?q={query}\")\n    return response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full End-to-End Application\n\nNow, let's put all the components together to create a working chatbot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chatbot():\n    print(\"Chatbot initialized. Type 'exit' to end the conversation.\")\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == 'exit':\n            break\n        bot_response = generate_response(user_input)\n        add_to_history(user_input, bot_response)\n        store_interaction(user_input, bot_response)\n        print(f\"Bot: {bot_response}\")\n\nchatbot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing & Validation\n\nWe'll test the chatbot by running a few example queries and checking the responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example test\ntest_input = \"Hello, how are you?\"\ntest_response = generate_response(test_input)\nprint(f\"Test Input: {test_input}\")\nprint(f\"Test Response: {test_response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this tutorial, we built a memory-aware chatbot using LangChain and Hugging Face. This project demonstrated how to integrate a language model, manage conversation history, and store interactions in a vector database. For further development, consider deploying your chatbot using serverless architectures or containerization for scalability and reliability. Explore additional features of LangChain to enhance your chatbot's capabilities.\n\nFor more information on customizing LLMs for specific applications, check out our article on [customizing LLMs for domain-specific applications](/blog/44830763/mastering-domain-specific-llm-customization-techniques-and-tools-unveiled).\n\nHappy coding!"
      ]
    }
  ],
  "metadata": {
    "title": "Master AI Agents Automation: Build Complex Task Solutions with LangChain",
    "description": "Discover how to automate complex tasks using AI agents with LangChain. Learn integration strategies, tool development, and deployment best practices for real-world applications.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}