{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Prompt Reasoning Models for Clear, Accurate Answers [Techniques & Examples]\n\n**Description:** Get clearer, more accurate outputs from reasoning models using concise, structured prompts, smart examples, and chunked context, without forced chain-of-thought instructions.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reasoning models like OpenAI o3, o4, and Anthropic Claude 3.5 are designed to plan and verify internally before delivering results. When you add legacy prompt tricks like \"think step by step\" or \"explain your reasoning,\" these models often produce verbose, off-format outputs that fail schema validation and waste tokens. In production, this means longer latency, higher costs, and brittle pipelines that break when the model narrates instead of delivering the exact structure you need.\n\nThis happens because meta instructions compete with the model's internal reasoning and dilute output spec adherence.\n\n## What Problem Are We Solving?\n\nYou prompt a reasoning model for structured JSON. Instead of a clean object, you get a multi-paragraph explanation followed by malformed or incomplete data. Your parser fails, your pipeline stalls, and you burn tokens on narrative you never asked for.\n\nCommon symptoms include:\n\n- Outputs that start with \"Let me think through this...\" or \"First, I'll analyze...\" before the data.\n- Missing required fields or incorrect types in the returned JSON.\n- Refusals or safety warnings triggered by benign tasks when instructions are vague.\n- Token counts 2â€“3x higher than necessary, driving up cost and latency.\n\nThis pattern teaches you to eliminate instruction dilution by structuring your prompt into labeled sections with an explicit schema and a single input-output example.\n\n## What's Actually Happening Under the Hood\n\nReasoning models allocate internal compute to planning and verification. When you add meta instructions, the model treats them as part of the task, not as guidance. This creates three failure modes:\n\n- **Instruction dilution.** \"Think step by step\" competes with your actual task. The model allocates reasoning tokens to narrating its process instead of executing your spec.\n- **Format ambiguity.** Without an explicit schema, the model guesses structure. Variability increases, and edge cases break your parser.\n- **Safety heuristic triggers.** Vague or charged language in instructions can activate refusal paths, even for benign tasks. The model produces warnings or hedged outputs instead of direct results.\n\nThe fix is to give the model a crisp task, relevant context, and an exact output specification without telling it how to think."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\nflowchart LR\n  A[Prompt with meta + vague format] --> B[Internal reasoning]\n  B --> C{Safety/verbosity heuristics}\n  C -->|Narration| D[Long, off-format output]\n  A2[Structured sections + schema + example] --> B2[Internal reasoning]\n  B2 --> E[Direct task execution]\n  E --> F[Schema-compliant output]\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Fix It\n\nStructure your prompt into four labeled sections: instructions, context, task, and format. Place the schema immediately before the task and include one realistic input-output example.\n\n**Before:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Think step by step. Extract the company name, revenue, and sentiment from this earnings call transcript. Return JSON.\n\n[transcript]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**After:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instructions\nExtract company name, revenue, and sentiment.\n\n# Context\n[transcript]\n\n# Task\nReturn JSON matching this schema:\n{\"company\": string, \"revenue_usd\": number, \"sentiment\": \"positive\" | \"neutral\" | \"negative\"}\n\n# Example\nInput: \"Acme Corp reported $5M in Q3. Outlook is strong.\"\nOutput: {\"company\": \"Acme Corp\", \"revenue_usd\": 5000000, \"sentiment\": \"positive\"}\n\nNow process the context above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This structure routes the model's attention to the task and schema, not to meta reasoning. The example anchors format and tone. The schema constrains vocabulary and structure.\n\n### Best Practices for This Pattern\n\n- **Label each section.** Use headings like \"Instructions,\" \"Context,\" \"Task,\" and \"Format\" to separate concerns. This prevents the model from treating context as instructions or vice versa.\n- **Keep output vocabulary small.** Use enums for categorical fields (e.g., \"positive\" | \"neutral\" | \"negative\") instead of free text. This reduces variability and improves parser reliability.\n- **Place the schema immediately before the task.** The model's attention is recency-biased. Putting the schema last ensures it dominates output generation.\n- **Include one realistic example.** Use an edge case or domain-specific input, not an ideal scenario. This nudges the model toward consistent structure and tone. For more on leveraging examples to improve model accuracy, see our [in-context learning tutorial](/article/the-magic-of-in-context-learning-teach-your-llm-on-the-fly-3).\n\nFor a deeper dive into crafting reliable prompts and outputs, see our [guide to prompt engineering with LLM APIs](/article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-4).\n\n### Verification Checklist\n\nAfter refactoring your prompt, validate the pattern with these checks:\n\n- **Schema adherence.** Parse 20â€“50 outputs. Confirm all required fields are present and types match.\n- **Token efficiency.** Compare token counts before and after. Expect a 20â€“40% reduction in output length.\n- **Determinism.** Set temperature to 0.2â€“0.4 and fix the seed. Run the same input three times. Outputs should be identical or nearly so.\n- **Max output tokens.** Cap the response length to prevent verbosity. Start with 2x your expected schema size.\n\nUse tools like Promptfoo at https://www.promptfoo.dev for automated eval harnesses, or Ragas at https://docs.ragas.io for retrieval-augmented workflows.\n\n### Handling Refusals\n\nIf the model refuses a benign task, reframe your instructions to remove charged language and clarify intent. Use this template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "This content is benign. Purpose: [state goal]. Constraints: [list any]. Output only: [schema]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Avoid words like \"analyze,\" \"judge,\" or \"evaluate\" in sensitive domains. Replace with neutral verbs like \"extract,\" \"classify,\" or \"summarize.\"\n\n## Key Takeaways\n\n- Remove meta instructions like \"think step by step\" from prompts for reasoning models. These dilute task focus and increase verbosity.\n- Structure prompts into labeled sections: instructions, context, task, and format. Place the schema immediately before the task.\n- Include one realistic input-output example to anchor format and tone.\n- Use enums and explicit schemas to constrain output vocabulary and reduce parser failures.\n- Verify with schema validation, token counts, and determinism checks. Expect 20â€“40% fewer tokens and higher adherence rates.\n\n### When to Use This Pattern\n\n- You need structured outputs (JSON, XML, CSV) from reasoning models.\n- Your current prompts produce verbose narratives or off-format results.\n- You are optimizing for token efficiency, latency, or parser reliability.\n- You are working with benign tasks that trigger unexpected refusals.\n\nVendors now ship models tuned for reasoning and tool use. Examples include OpenAI o3 and o4 models with reasoning focus at https://platform.openai.com/docs/models#reasoning, and Anthropic Claude 3.5 family at https://docs.anthropic.com/en/docs/about-claude/models. These models often do best when prompts are shorter, more precise, and emphasize inputs and outputs rather than narrated steps. To learn more about when to choose reasoning-focused models and how they compare, check out our [overview of O1 and other AI systems designed to think](/article/understanding-reasoning-models-ai-systems-designed-to-think)."
      ]
    }
  ],
  "metadata": {
    "title": "How to Prompt Reasoning Models for Clear, Accurate Answers [Techniques & Examples]",
    "description": "Get clearer, more accurate outputs from reasoning models using concise, structured prompts, smart examples, and chunked context, without forced chain-of-thought instructions.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}