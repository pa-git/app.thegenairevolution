{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Prototype a Locally Hosted Chatbot with Kimi K2 on Google Cloud (using Chainlit UI)\n\n**Description:** Step-by-step guide to build a self-hosted chatbot using Kimi K2 on Google Cloud: covering GPU VM setup, vLLM inference, backend & UI integration, containerization, HTTPS security, testing.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You're about to build a private, GPU-accelerated chatbot that runs entirely under your controlâ€”no third-party API calls, no data leakage, and full customization of the model and interface. This guide walks you through deploying a complete chatbot stack on a single Google Cloud VM: vLLM for fast inference, FastAPI for request handling and guardrails, and Chainlit for a streaming chat UI. By the end, you'll have a working prototype served over HTTPS, ready for internal use or further extension.\n\nThis is a focused, single-VM prototype. Out of scope: autoscaling, multi-node orchestration, and production-grade authentication (we use simple rate limiting). You'll need a Google Cloud account with GPU quota, a domain for TLS, and a Hugging Face token with access to the Kimi K2 model weights (or a substitute instruct model).\n\n## Why This Approach Works\n\n**Why Kimi K2?**  \nKimi K2 is a 7B-parameter instruct model optimized for long-context reasoning and conversational tasks. It fits comfortably on a T4 or L4 GPU (16 GB VRAM) and delivers strong performance for chat applications without requiring multi-GPU setups or quantization. If you don't have access to Kimi K2, substitute a tested alternative like Qwen2-7B-Instruct or Llama-3.1-8B-Instruct, both of which fit similar VRAM profiles and support long contexts.\n\n**Why vLLM?**  \nvLLM is a high-throughput inference server with paged attention, enabling efficient memory use and batching. It exposes an OpenAI-compatible API, so you can reuse familiar client code and swap models without rewriting application logic. For a single-user or small-team prototype, vLLM on one GPU provides low-latency responses and straightforward deployment.\n\n**Why FastAPI?**  \nFastAPI provides a lightweight, typed REST layer between the UI and vLLM. This decoupling lets you enforce guardrails (input truncation, rate limiting) and abstract model details from the frontend. It also simplifies future extensions like tool calling or multi-model routing. For tips on crafting robust prompts and ensuring consistent outputs from LLM APIs, see our article on [prompt engineering with LLM APIs](/article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-3).\n\n**Why Chainlit?**  \nChainlit is a Python framework for building chat UIs with minimal boilerplate. It handles streaming, message history rendering, and WebSocket connections out of the box, letting you focus on backend logic rather than frontend plumbing.\n\n**Why Google Cloud?**  \nGCE offers predictable GPU availability (T4, L4, A100) with per-second billing and straightforward VM provisioning. Artifact Registry integrates cleanly for container storage, and Compute Engine's firewall and networking are simple to configure for single-VM deployments.\n\n## How It Works (High-Level Overview)\n\nThe architecture is a three-tier pipeline:\n\n1. **Browser â†’ Chainlit (UI):** User sends a message via the Chainlit web interface.\n2. **Chainlit â†’ FastAPI (Backend):** Chainlit forwards the conversation history to the FastAPI backend via HTTP POST.\n3. **FastAPI â†’ vLLM (Inference):** FastAPI validates inputs, truncates history to fit context limits, applies rate limiting, and calls vLLM's OpenAI-compatible `/v1/chat/completions` endpoint.\n4. **vLLM â†’ FastAPI â†’ Chainlit â†’ Browser:** vLLM generates a response, FastAPI returns it to Chainlit, and Chainlit streams it to the user.\n\n**Data flow:**  \n- Conversation history lives in the Chainlit session and is passed to FastAPI on each request.\n- FastAPI truncates messages to fit `MAX_INPUT_CHARS` and `MAX_HISTORY_MESSAGES` before forwarding to vLLM.\n- Rate limiting is enforced per client IP in FastAPI (with caveats for reverse proxies, addressed below).\n\n**TLS termination:**  \nNginx sits in front of Chainlit and FastAPI, handling HTTPS via Let's Encrypt and proxying requests to the backend services.\n\n## Setup & Installation\n\n### Prerequisites\n\n- **Google Cloud account** with GPU quota (at least 1 T4 or L4 in your chosen region)\n- **Domain name** for TLS (e.g., `chat.yourdomain.com`)\n- **Hugging Face token** with access to Kimi K2 weights (or a substitute model)\n- **Budget estimate:** ~$0.35â€“$0.50/hour for a T4 VM (n1-standard-4 + 1 T4 GPU)\n- **Local tools:** `gcloud` CLI, Docker, and `docker compose` (v2.x)\n\n### Provision the VM\n\nCreate a VM with a T4 GPU and sufficient disk for model weights and Docker images. This example uses `us-central1-a` and an `n1-standard-4` instance with a 100 GB boot disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gcloud compute instances create k2-chatbot \\\n  --zone=us-central1-a \\\n  --machine-type=n1-standard-4 \\\n  --accelerator=type=nvidia-tesla-t4,count=1 \\\n  --image-family=ubuntu-2004-lts \\\n  --image-project=ubuntu-os-cloud \\\n  --boot-disk-size=100GB \\\n  --maintenance-policy=TERMINATE \\\n  --metadata=install-nvidia-driver=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note on GPU drivers:** The `install-nvidia-driver=True` metadata flag installs a compatible NVIDIA driver automatically. For newer GPUs like L4, you may need driver version 550 or higher. If the automatic install fails, SSH into the VM and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo /opt/deeplearning/install-driver.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then reboot and verify:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Firewall\n\nAllow HTTP and HTTPS traffic. Do not open ports 3000 or 8000 directlyâ€”these will be accessed only via Nginx."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gcloud compute firewall-rules create allow-http-https \\\n  --allow=tcp:80,tcp:443 \\\n  --source-ranges=0.0.0.0/0 \\\n  --target-tags=http-server,https-server\n\ngcloud compute instances add-tags k2-chatbot \\\n  --zone=us-central1-a \\\n  --tags=http-server,https-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Docker and Docker Compose\n\nSSH into the VM and install Docker:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gcloud compute ssh k2-chatbot --zone=us-central1-a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once connected, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo apt-get update\nsudo apt-get install -y docker.io docker-compose-plugin\nsudo usermod -aG docker $USER\nnewgrp docker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify Docker can access the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu20.04 nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up Hugging Face Token\n\nExport your Hugging Face token as an environment variable. For better secrets hygiene, create a `.env` file instead of exporting in shell history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "echo \"HF_TOKEN=your_hf_token_here\" > ~/.env\nchmod 600 ~/.env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load it in your shell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "export $(cat ~/.env | xargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Security note:** For production, use Google Secret Manager or a similar service to manage tokens.\n\n## Step-by-Step Implementation\n\n### Step 1: Create Project Structure\n\nSet up directories for the API, UI, and Docker configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mkdir -p ~/k2-chatbot/{api,ui}\ncd ~/k2-chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Write the FastAPI Backend\n\nCreate `api/main.py`. This file handles chat requests, applies rate limiting and input truncation, and calls vLLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Purpose: FastAPI backend for Kimi K2 chatbot, handling chat requests, rate limiting, and vLLM inference.\n\nimport os\nimport time\nimport logging\nfrom typing import List, Literal, Optional, Dict, Tuple\nfrom fastapi import FastAPI, Request, HTTPException\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\n\n# Configure basic logging for runtime behavior and error tracking\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s %(levelname)s %(message)s\"\n)\n\n# Load environment variables for configuration\nVLLM_BASE_URL = os.getenv(\"VLLM_BASE_URL\", \"http://vllm:8000/v1\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"EMPTY\")\nMODEL_ID = os.getenv(\"MODEL_ID\", \"your-model-id\")\nMAX_INPUT_CHARS = int(os.getenv(\"MAX_INPUT_CHARS\", \"12000\"))\nMAX_HISTORY_MESSAGES = int(os.getenv(\"MAX_HISTORY_MESSAGES\", \"24\"))\nRATE_LIMIT_RPS = float(os.getenv(\"RATE_LIMIT_RPS\", \"2.0\"))\n\n# Initialize OpenAI client for vLLM server\nclient = OpenAI(base_url=VLLM_BASE_URL, api_key=OPENAI_API_KEY)\n\n# Initialize FastAPI app\napp = FastAPI(title=\"K2 Chat API\")\n\nclass ChatMessage(BaseModel):\n    \"\"\"\n    Represents a single chat message in the conversation.\n\n    Args:\n        role (Literal[\"system\", \"user\", \"assistant\"]): The role of the message sender.\n        content (str): The message content.\n        timestamp (Optional[float]): Optional UNIX timestamp for the message.\n\n    Example:\n        {\"role\": \"user\", \"content\": \"Hello!\", \"timestamp\": 1710000000.0}\n    \"\"\"\n    role: Literal[\"system\", \"user\", \"assistant\"]\n    content: str\n    timestamp: Optional[float] = None\n\nclass ChatRequest(BaseModel):\n    \"\"\"\n    Request body for the /chat endpoint.\n\n    Args:\n        messages (List[ChatMessage]): Conversation history.\n        temperature (float): Sampling temperature for the model.\n        max_tokens (int): Maximum tokens to generate in the response.\n    \"\"\"\n    messages: List[ChatMessage] = Field(default_factory=list)\n    temperature: float = 0.2\n    max_tokens: int = 512\n\nclass ChatResponse(BaseModel):\n    \"\"\"\n    Response body for the /chat endpoint.\n\n    Args:\n        content (str): The assistant's reply.\n        tokens (Optional[int]): Number of tokens used (if available).\n        latency_ms (Optional[int]): Latency in milliseconds.\n    \"\"\"\n    content: str\n    tokens: Optional[int] = None\n    latency_ms: Optional[int] = None\n\n# In-memory leaky bucket rate limiter per client IP\nrate_state: Dict[str, Tuple[float, float]] = {}  # ip -> (last_time, tokens)\n\ndef check_rate_limit(ip: str) -> None:\n    \"\"\"\n    Enforces a simple leaky bucket rate limit per client IP.\n\n    Args:\n        ip (str): The client's IP address.\n\n    Raises:\n        HTTPException: If the client exceeds the allowed rate.\n    \"\"\"\n    now = time.time()\n    last, tokens = rate_state.get(ip, (now, RATE_LIMIT_RPS))\n    elapsed = max(0.0, now - last)\n    # Refill tokens based on elapsed time\n    tokens = min(RATE_LIMIT_RPS, tokens + elapsed * RATE_LIMIT_RPS)\n    if tokens < 1.0:\n        logging.warning(f\"Rate limit exceeded for IP {ip}\")\n        raise HTTPException(status_code=429, detail=\"Too many requests\")\n    # Update state with new token count\n    rate_state[ip] = (now, tokens - 1.0)\n    # Optional: Clean up old IPs to avoid unbounded memory growth\n    if len(rate_state) > 10000:\n        # Remove oldest entries if too many IPs (prototype trade-off)\n        for old_ip in list(rate_state.keys())[:1000]:\n            del rate_state[old_ip]\n\ndef truncate_messages(msgs: List[ChatMessage]) -> List[ChatMessage]:\n    \"\"\"\n    Truncates the message history to fit within max message count and character limits.\n\n    Args:\n        msgs (List[ChatMessage]): The full conversation history.\n\n    Returns:\n        List[ChatMessage]: The truncated message list, preserving order.\n\n    Notes:\n        - Keeps only the last MAX_HISTORY_MESSAGES.\n        - Ensures total character count does not exceed MAX_INPUT_CHARS.\n        - Truncates from the oldest messages first.\n    \"\"\"\n    # Keep only the last N messages\n    msgs = msgs[-MAX_HISTORY_MESSAGES:]\n    total = 0\n    kept = []\n    # Walk backwards to keep the most recent messages within char limit\n    for m in reversed(msgs):\n        if total + len(m.content) > MAX_INPUT_CHARS:\n            break\n        kept.append(m)\n        total += len(m.content)\n    # Reverse to restore chronological order\n    return list(reversed(kept))\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(req: Request, body: ChatRequest) -> ChatResponse:\n    \"\"\"\n    Handles chat requests from the UI, applies rate limiting and truncation, and calls vLLM.\n\n    Args:\n        req (Request): The FastAPI request object (for client IP).\n        body (ChatRequest): The chat request payload.\n\n    Returns:\n        ChatResponse: The assistant's reply, token usage, and latency.\n\n    Raises:\n        HTTPException: For input validation, rate limiting, or backend errors.\n    \"\"\"\n    ip = req.client.host\n    check_rate_limit(ip)\n    if not body.messages:\n        logging.error(\"No messages provided in request\")\n        raise HTTPException(status_code=400, detail=\"messages required\")\n    msgs = truncate_messages(body.messages)\n\n    start = time.time()\n    try:\n        # Prepare messages for OpenAI SDK (dict format)\n        openai_msgs = [m.model_dump() for m in msgs]\n        # Call vLLM via OpenAI-compatible API\n        res = client.chat.completions.create(\n            model=MODEL_ID,\n            messages=openai_msgs,\n            temperature=body.temperature,\n            max_tokens=body.max_tokens,\n        )\n        content = res.choices[0].message.content\n        usage = getattr(res, \"usage\", None)\n        tokens = usage.total_tokens if usage else None\n    except Exception as e:\n        # Log the error for debugging, but avoid leaking sensitive info\n        logging.exception(\"Error during vLLM inference\")\n        raise HTTPException(status_code=500, detail=\"Model backend error\")\n    latency_ms = int((time.time() - start) * 1000)\n    # Log successful request for monitoring\n    logging.info(f\"Chat completed for IP {ip} in {latency_ms} ms, tokens: {tokens}\")\n    return ChatResponse(content=content, tokens=tokens, latency_ms=latency_ms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why a separate backend?** This layer decouples the UI from the model server, letting you enforce guardrails (truncation, rate limiting) and abstract model details. It also simplifies future extensions like tool calling or multi-model routing.\n\nCreate `api/requirements.txt`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fastapi==0.109.0\nuvicorn[standard]==0.27.0\nopenai==1.10.0\npydantic==2.5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create `api/Dockerfile`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```dockerfile\nFROM python:3.11-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY main.py .\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\"]\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Write the Chainlit UI\n\nCreate `ui/app.py`. This file defines the chat interface and forwards messages to the FastAPI backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport chainlit as cl\nimport httpx\n\nAPI_URL = os.getenv(\"API_URL\", \"http://api:8001/chat\")\n\n@cl.on_message\nasync def main(message: cl.Message):\n    # Retrieve conversation history from Chainlit's message history\n    history = cl.user_session.get(\"history\", [])\n    history.append({\"role\": \"user\", \"content\": message.content})\n    \n    # Call FastAPI backend\n    async with httpx.AsyncClient(timeout=60.0) as client:\n        try:\n            resp = await client.post(API_URL, json={\"messages\": history})\n            resp.raise_for_status()\n            data = resp.json()\n            reply = data[\"content\"]\n        except Exception as e:\n            reply = f\"Error: {e}\"\n    \n    # Append assistant reply to history\n    history.append({\"role\": \"assistant\", \"content\": reply})\n    cl.user_session.set(\"history\", history)\n    \n    # Send reply to user\n    await cl.Message(content=reply).send()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note on message history:** This code uses `cl.user_session` to store conversation history in memory. For production, consider persisting history to a database or using Chainlit's built-in data layer.\n\nCreate `ui/requirements.txt`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chainlit==1.0.0\nhttpx==0.26.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create `ui/Dockerfile`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```dockerfile\nFROM python:3.11-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY app.py .\nCMD [\"chainlit\", \"run\", \"app.py\", \"--host\", \"0.0.0.0\", \"--port\", \"3000\"]\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Write Docker Compose Configuration\n\nCreate `docker-compose.yml` in the project root. This file orchestrates vLLM, FastAPI, and Chainlit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```yaml\nversion: \"3.8\"\n\nservices:\n  vllm:\n    image: vllm/vllm-openai:v0.3.0\n    container_name: vllm\n    environment:\n      - HF_TOKEN=${HF_TOKEN}\n    command: >\n      --model ${MODEL_ID}\n      --max-model-len 8192\n      --gpu-memory-utilization 0.9\n      --trust-remote-code\n    ports:\n      - \"127.0.0.1:8000:8000\"\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/v1/models\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n\n  api:\n    build: ./api\n    container_name: api\n    environment:\n      - VLLM_BASE_URL=http://vllm:8000/v1\n      - MODEL_ID=${MODEL_ID}\n      - MAX_INPUT_CHARS=12000\n      - MAX_HISTORY_MESSAGES=24\n      - RATE_LIMIT_RPS=2.0\n    ports:\n      - \"127.0.0.1:8001:8001\"\n    depends_on:\n      vllm:\n        condition: service_healthy\n\n  ui:\n    build: ./ui\n    container_name: ui\n    environment:\n      - API_URL=http://api:8001/chat\n    ports:\n      - \"127.0.0.1:3000:3000\"\n    depends_on:\n      - api\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**GPU access note:** This configuration uses `deploy.resources.reservations.devices` with `driver: nvidia`, which works with Docker Compose v2.x and the NVIDIA Container Toolkit. If you encounter \"no CUDA device\" errors, ensure the toolkit is installed and `nvidia-smi` works inside a test container.\n\n**Security note:** The `--trust-remote-code` flag allows the model to execute custom code during loading. This is often required for newer models but poses a security risk. Pin model revisions and review model repositories before use.\n\nCreate a `.env` file in the project root to centralize configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HF_TOKEN=your_hf_token_here\nMODEL_ID=kimi-k2-7b-instruct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adjust `MODEL_ID` to match your chosen model. If you're unsure how to evaluate which LLM best fits your hardware and application needs, our guide on [how to choose an AI model for your appâ€”speed, cost, and reliability](/article/how-to-choose-an-ai-model-for-your-app-speed-cost-reliability) offers a practical framework.\n\n### Step 5: Build and Start Services\n\nBuild the images and start the stack:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docker compose build\ndocker compose up -d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check logs to verify services are running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docker compose logs -f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wait for vLLM to download the model and load it into GPU memory. This may take 5â€“10 minutes on first run.\n\n### Step 6: Validate Locally via SSH Tunnel\n\nBefore exposing the service publicly, test it locally. On your local machine, create an SSH tunnel to the VM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gcloud compute ssh k2-chatbot --zone=us-central1-a -- -L 3000:localhost:3000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Open `http://localhost:3000` in your browser. You should see the Chainlit interface. Send a message and verify the assistant responds.\n\n### Step 7: Configure Nginx and Let's Encrypt\n\nInstall Nginx and Certbot on the VM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo apt-get install -y nginx certbot python3-certbot-nginx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create an Nginx configuration file at `/etc/nginx/sites-available/k2-chatbot`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```nginx\nserver {\n    listen 80;\n    server_name chat.yourdomain.com;\n\n    location / {\n        proxy_pass http://127.0.0.1:3000;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_read_timeout 300s;\n    }\n\n    location /api/ {\n        proxy_pass http://127.0.0.1:8001/;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_read_timeout 300s;\n    }\n}\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Enable the site and reload Nginx:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo ln -s /etc/nginx/sites-available/k2-chatbot /etc/nginx/sites-enabled/\nsudo nginx -t\nsudo systemctl reload nginx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtain a TLS certificate from Let's Encrypt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo certbot --nginx -d chat.yourdomain.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Follow the prompts. Certbot will automatically update the Nginx configuration to redirect HTTP to HTTPS.\n\n**Rate limiting caveat:** The FastAPI backend uses `req.client.host` for rate limiting, which will be the Nginx proxy IP when behind a reverse proxy. To enforce per-client limits, configure FastAPI to trust the `X-Forwarded-For` header. Add this to `api/main.py`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastapi.middleware.trustedhost import TrustedHostMiddleware\n\napp.add_middleware(TrustedHostMiddleware, allowed_hosts=[\"*\"])\n\n# Update check_rate_limit to read X-Forwarded-For\ndef get_client_ip(req: Request) -> str:\n    forwarded = req.headers.get(\"X-Forwarded-For\")\n    if forwarded:\n        return forwarded.split(\",\")[0].strip()\n    return req.client.host"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then replace `ip = req.client.host` with `ip = get_client_ip(req)` in the `chat` function.\n\n### Step 8: Test Over HTTPS\n\nOpen `https://chat.yourdomain.com` in your browser. Verify the TLS certificate is valid and the chat interface loads. Send a message and confirm the assistant responds.\n\n## Run and Validate\n\n### Test Long Context\n\nPaste a longer input (e.g., a 2000-word document) and ask the assistant to summarize it. Ensure vLLM's `--max-model-len` setting accommodates the input. If you hit truncation, adjust `MAX_INPUT_CHARS` in the `.env` file and restart the API service:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docker compose restart api"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Monitor VRAM usage to avoid out-of-memory errors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a deeper understanding of how LLMs handle memory and why they sometimes \"forget\" information as context grows, check out our article on [context rot and LLM memory limitations](/article/context-rot-why-llms-forget-as-their-memory-grows-3).\n\n### Test Rate Limiting\n\nSend multiple rapid requests from the same client. After exceeding the rate limit (default: 2 requests/second), you should receive a 429 error.\n\n### Check Logs\n\nReview logs for errors or performance issues:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docker compose logs vllm\ndocker compose logs api\ndocker compose logs ui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n\nThis prototype runs on a single VM with basic rate limiting and no persistent storage. To move toward production:\n\n- **Add authentication:** Integrate OAuth or API keys to control access.\n- **Persist conversation history:** Store messages in a database (PostgreSQL, Firestore) instead of in-memory sessions.\n- **Scale horizontally:** Use Kubernetes or Cloud Run to deploy multiple replicas behind a load balancer.\n- **Monitor and alert:** Add Prometheus metrics and Grafana dashboards to track latency, throughput, and GPU utilization.\n- **Push images to Artifact Registry:** Tag and push your built images for versioned deployments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docker tag api:latest us-docker.pkg.dev/YOUR_PROJECT/k2-repo/api:v1\ndocker push us-docker.pkg.dev/YOUR_PROJECT/k2-repo/api:v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You now have a working, GPU-accelerated chatbot running entirely under your control. You can customize the model, adjust guardrails, and extend the backend with tools or multi-model routingâ€”all without relying on third-party APIs."
      ]
    }
  ],
  "metadata": {
    "title": "How to Prototype a Locally Hosted Chatbot with Kimi K2 on Google Cloud (using Chainlit UI)",
    "description": "Step-by-step guide to build a self-hosted chatbot using Kimi K2 on Google Cloud: covering GPU VM setup, vLLM inference, backend & UI integration, containerization, HTTPS security, testing.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}